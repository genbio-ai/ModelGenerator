{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>AIDO.ModelGenerator is a software stack powering the development of an AI-driven Digital Organism by enabling researchers to adapt pretrained models and generate finetuned models for downstream tasks. To read more about AIDO.ModelGenerator's integral role in building the world's first AI-driven Digital Organism, see AIDO.</p> <p>AIDO.ModelGenerator is open-sourced as an opinionated plug-and-play research framework for cross-disciplinary teams in ML &amp; Bio.  It is designed to enable rapid and reproducible prototyping with four kinds of experiments in mind:</p> <ol> <li>Applying pre-trained foundation models to new data</li> <li>Developing new finetuning and inference tasks for foundation models</li> <li>Benchmarking foundation models and creating leaderboards</li> <li>Testing new architectures for finetuning performance</li> </ol> <p>while also scaling with hardware and integrating with larger data pipelines or research workflows.</p> <p>AIDO.ModelGenerator is built on PyTorch, HuggingFace, and Lightning, and works seamlessly with these ecosystems.</p>"},{"location":"#who-uses-aidomodelgenerator","title":"Who uses AIDO.ModelGenerator?","text":""},{"location":"#biologists","title":"\ud83e\uddec Biologists","text":"<ul> <li>Intuitive one-command CLIs for in silico experiments</li> <li>Pre-trained model zoo</li> <li>Broad data compatibility</li> <li>Pipeline-oriented workflows</li> </ul>"},{"location":"#ml-researchers","title":"\ud83e\udd16 ML Researchers","text":"<ul> <li>Reproducible-by-design experiments</li> <li>Architecture A/B testing</li> <li>Automatic hardware scaling</li> <li>Integration with PyTorch, Lightning, HuggingFace, and WandB</li> </ul>"},{"location":"#software-engineers","title":"\u2615 Software Engineers","text":"<ul> <li>Extensible and modular models, tasks, and data</li> <li>Strict typing and documentation</li> <li>Fail-fast interface design</li> <li>Continuous integration and testing</li> </ul>"},{"location":"#everyone-benefits-from","title":"\ud83e\udd1d Everyone benefits from","text":"<ul> <li>A collaborative hub and focal point for multidisciplinary work on experiments, models, software, and data</li> <li>Community-driven development</li> <li>Permissive license for academic and non-commercial use</li> </ul>"},{"location":"#projects-using-aidomodelgenerator","title":"Projects using AIDO.ModelGenerator","text":"<ul> <li>Accurate and General DNA Representations Emerge from Genome Foundation Models at Scale</li> <li>A Large-Scale Foundation Model for RNA Function and Structure Prediction</li> <li>Mixture of Experts Enable Efficient and Effective Protein Understanding and Design</li> <li>Scaling Dense Representations for Single Cell with Transcriptome-Scale Context</li> <li>Balancing Locality and Reconstruction in Protein Structure Tokenizer</li> </ul>"},{"location":"quick_start/","title":"Quick Start","text":""},{"location":"quick_start/#installation","title":"Installation","text":"<pre><code>git clone https://github.com/genbio-ai/ModelGenerator.git\ncd ModelGenerator\npip install -e .\n</code></pre> <p>Source installation is necessary to add new backbones, finetuning tasks, and data transformations, as well as use convenience configs and scripts. If you only need to run inference, reproduce published experiments, or finetune on new data, you can use</p> <pre><code>pip install modelgenerator\npip install git+https://github.com/genbio-ai/openfold.git@c4aa2fd0d920c06d3fd80b177284a22573528442\npip install git+https://github.com/NVIDIA/dllogger.git@0540a43971f4a8a16693a9de9de73c1072020769\n</code></pre>"},{"location":"quick_start/#quick-start_1","title":"Quick Start","text":""},{"location":"quick_start/#get-embeddings-from-a-pre-trained-model","title":"Get embeddings from a pre-trained model","text":"<pre><code>mgen predict --model Embed --model.backbone aido_dna_dummy \\\n  --data SequencesDataModule --data.path genbio-ai/100m-random-promoters \\\n  --data.x_col sequence --data.id_col sequence --data.test_split_size 0.0001 \\\n  --config configs/examples/save_predictions.yaml\n</code></pre>"},{"location":"quick_start/#get-token-probabilities-from-a-pre-trained-model","title":"Get token probabilities from a pre-trained model","text":"<pre><code>mgen predict --model Inference --model.backbone aido_dna_dummy \\\n  --data SequencesDataModule --data.path genbio-ai/100m-random-promoters \\\n  --data.x_col sequence --data.id_col sequence --data.test_split_size 0.0001 \\\n  --config configs/examples/save_predictions.yaml\n</code></pre>"},{"location":"quick_start/#finetune-a-model","title":"Finetune a model","text":"<pre><code>mgen fit --model ConditionalDiffusion --model.backbone aido_dna_dummy \\\n  --data ConditionalDiffusionDataModule --data.path \"genbio-ai/100m-random-promoters\"\n</code></pre>"},{"location":"quick_start/#evaluate-a-model-checkpoint","title":"Evaluate a model checkpoint","text":"<pre><code>mgen test --model ConditionalDiffusion --model.backbone aido_dna_dummy \\\n  --data ConditionalDiffusionDataModule --data.path \"genbio-ai/100m-random-promoters\" \\\n  --ckpt_path logs/lightning_logs/version_X/checkpoints/&lt;your_model&gt;.ckpt\n</code></pre>"},{"location":"quick_start/#save-predictions","title":"Save predictions","text":"<pre><code>mgen predict --model ConditionalDiffusion --model.backbone aido_dna_dummy \\\n  --data ConditionalDiffusionDataModule --data.path \"genbio-ai/100m-random-promoters\" \\\n  --ckpt_path logs/lightning_logs/version_X/checkpoints/&lt;your_model&gt;.ckpt \\\n  --config configs/examples/save_predictions.yaml\n</code></pre>"},{"location":"quick_start/#configify-your-experiment","title":"Configify your experiment","text":"<p>This command</p> <pre><code>mgen fit --model ConditionalDiffusion --model.backbone aido_dna_dummy \\\n  --data ConditionalDiffusionDataModule --data.path \"genbio-ai/100m-random-promoters\"\n</code></pre> <p>is equivalent to <code>mgen fit --config my_config.yaml</code> with</p> <pre><code># my_config.yaml\nmodel:\n  class_path: ConditionalDiffusion\n  init_args:\n    backbone: aido_dna_dummy\ndata:\n  class_path: ConditionalDiffusionDataModule\n  init_args:\n    path: \"genbio-ai/100m-random-promoters\"\n</code></pre>"},{"location":"quick_start/#use-composable-configs-to-customize-workflows","title":"Use composable configs to customize workflows","text":"<pre><code>mgen fit --model SequenceRegression --data PromoterExpressionRegression \\\n  --config configs/defaults.yaml \\\n  --config configs/examples/lora_backbone.yaml \\\n  --config configs/examples/wandb.yaml\n</code></pre> <p>We provide some useful examples in <code>configs/examples</code>. Configs use the LAST value for each attribute. Check the full configuration logged with each experiment in <code>logs/lightning_logs/your-experiment/config.yaml</code>, or if using wandb <code>logs/config.yaml</code>.</p>"},{"location":"quick_start/#use-lora-for-parameter-efficient-finetuning","title":"Use LoRA for parameter-efficient finetuning","text":"<p>This also avoids saving the full model, only the LoRA weights are saved.</p> <pre><code>mgen fit --data PromoterExpressionRegression \\\n  --model SequenceRegression --model.backbone.use_peft true \\\n  --model.backbone.lora_r 16 \\\n  --model.backbone.lora_alpha 32 \\\n  --model.backbone.lora_dropout 0.1\n</code></pre>"},{"location":"quick_start/#use-continued-pretraining-for-finetuning-domain-adaptation","title":"Use continued pretraining for finetuning domain adaptation","text":"<p>First run pretraining objective on finetuning data</p> <pre><code># https://arxiv.org/pdf/2310.02980\nmgen fit --model MLM --model.backbone aido_dna_dummy \\\n  --data MLMDataModule --data.path leannmlindsey/GUE \\\n  --data.config_name prom_core_notata\n</code></pre> <p>Then finetune using the adapted model</p> <pre><code>mgen fit --model SequenceClassification --model.strict_loading false \\\n  --data SequenceClassificationDataModule --data.path leannmlindsey/GUE \\\n  --data.config_name prom_core_notata \\\n  --ckpt_path logs/lightning_logs/version_X/checkpoints/&lt;your_adapted_model&gt;.ckpt\n</code></pre> <p>Make sure to turn off <code>strict_loading</code> to replace the adapter!</p>"},{"location":"quick_start/#use-the-headadapterdecoder-that-comes-with-the-backbone","title":"Use the head/adapter/decoder that comes with the backbone","text":"<pre><code>mgen fit --model SequenceClassification --data GUEClassification \\\n  --model.use_legacy_adapter true\n</code></pre>"},{"location":"api_reference/adapters/","title":"Adapters","text":"<p>Adapters work with <code>Backbones</code> and <code>Tasks</code> to adapt pretrained models to new objectives. They are specified with the <code>--model.adapter</code> argument in the CLI or in the <code>model.adapter</code> section of a configuration file.</p> <p>Adapters are the focal point for architecture design on top of backbones, and can be swapped with other adapters of the same type to benchmark different architectures.</p> <p>This reference overviews the available no-code adapters. If you would like to develop new adapters, see Experiment Design.</p> <pre><code># Example Adapter Configuration\nmodel:\n  class_path: modelgenerator.tasks.SequenceRegression\n  init_args:\n    adapter:\n      class_path: modelgenerator.adapters.MLPPoolAdapter\n      init_args:\n        pooling: mean_pooling\n        hidden_sizes:\n        - 512\n        bias: true\n        dropout: 0.1\n        dropout_in_middle: false\ndata:\n  ...\ntrainer:\n  ...\n</code></pre>"},{"location":"api_reference/adapters/#sequence-adapters","title":"Sequence Adapters","text":"<p>These adapters make a single prediction for the entire input.</p>"},{"location":"api_reference/adapters/#modelgenerator.adapters.MLPAdapter","title":"<code>modelgenerator.adapters.MLPAdapter</code>","text":"<p>               Bases: <code>Sequential</code>, <code>TokenAdapter</code></p> <p>Multi-layer perceptron (MLP) adapter.</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>Number of features of the input</p> required <code>out_features</code> <code>int</code> <p>Number of features of the output</p> required <code>hidden_sizes</code> <code>List[int]</code> <p>List of the hidden feature dimensions. Defaults to [].</p> <code>[]</code> <code>activation_layer</code> <code>Callable[..., Module]</code> <p>Activation function. Defaults to torch.nn.Tanh.</p> <code>Tanh</code> <code>bias</code> <code>bool</code> <p>Whether to use bias in the linear layer. Defaults to True</p> <code>True</code> <code>dropout</code> <code>float</code> <p>The probability for the dropout layer. Defaults to 0.0</p> <code>0.0</code> <code>dropout_in_middle</code> <code>bool</code> <p>Whether to use dropout in the middle layers. Defaults to True</p> <code>True</code>"},{"location":"api_reference/adapters/#modelgenerator.adapters.LinearCLSAdapter","title":"<code>modelgenerator.adapters.LinearCLSAdapter</code>","text":"<p>               Bases: <code>Module</code>, <code>SequenceAdapter</code></p> <p>Simple linear adapter for a 1D embedding</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>Number of input features</p> required <code>out_features</code> <code>int</code> <p>Number of output features</p> required"},{"location":"api_reference/adapters/#modelgenerator.adapters.LinearMeanPoolAdapter","title":"<code>modelgenerator.adapters.LinearMeanPoolAdapter</code>","text":"<p>               Bases: <code>Module</code>, <code>SequenceAdapter</code></p> <p>Mean pooling adapter for hidden_states of shape (n, seq_len, in_features)</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>Number of input features</p> required <code>out_features</code> <code>int</code> <p>Number of output features</p> required"},{"location":"api_reference/adapters/#modelgenerator.adapters.LinearMaxPoolAdapter","title":"<code>modelgenerator.adapters.LinearMaxPoolAdapter</code>","text":"<p>               Bases: <code>Module</code>, <code>SequenceAdapter</code></p> <p>Simple max pooling adapter for [batch,seq_len,dim] embeddings</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>Number of input features</p> required <code>out_features</code> <code>int</code> <p>Number of output features</p> required"},{"location":"api_reference/adapters/#modelgenerator.adapters.LinearTransformerAdapter","title":"<code>modelgenerator.adapters.LinearTransformerAdapter</code>","text":"<p>               Bases: <code>Module</code>, <code>SequenceAdapter</code></p> <p>Transformer adapter</p> <p>Note: Support cls_pooling only.</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>Hidden size</p> required <code>out_features</code> <code>int</code> <p>Number of output features</p> required"},{"location":"api_reference/adapters/#modelgenerator.adapters.ResNet2DAdapter","title":"<code>modelgenerator.adapters.ResNet2DAdapter</code>","text":"<p>               Bases: <code>Module</code>, <code>SequenceAdapter</code></p> <p>Adapter that applies ResNet2DModule to input embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Input matrix channels.</p> required <code>num_res_blocks</code> <code>int</code> <p>Number of residual blocks in the ResNet2DModule.</p> <code>2</code> <code>conv_channels</code> <code>int</code> <p>Intermediate convolution channels.</p> <code>64</code> <code>kernel_size</code> <code>int</code> <p>Kernel size for convolutions.</p> <code>3</code>"},{"location":"api_reference/adapters/#modelgenerator.adapters.ResNet1DAdapter","title":"<code>modelgenerator.adapters.ResNet1DAdapter</code>","text":"<p>               Bases: <code>Module</code>, <code>SequenceAdapter</code></p> <p>Adapter module that applies a ResNet1DModule to sequence data.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Input feature dimension.</p> required <code>channels</code> <code>int</code> <p>Number of channels for ResNet.</p> <code>256</code> <code>num_blocks</code> <code>int</code> <p>Number of residual blocks in ResNet1DModule.</p> <code>9</code> <code>dropout</code> <code>float</code> <p>Dropout rate.</p> <code>0.2</code>"},{"location":"api_reference/adapters/#token-adapters","title":"Token Adapters","text":"<p>These adapters make one prediction per token.</p>"},{"location":"api_reference/adapters/#modelgenerator.adapters.LinearAdapter","title":"<code>modelgenerator.adapters.LinearAdapter</code>","text":"<p>               Bases: <code>MLPAdapter</code></p> <p>Simple linear adapter for a 1D embedding</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>Number of input features</p> required <code>out_features</code> <code>int</code> <p>Number of output features</p> required"},{"location":"api_reference/adapters/#modelgenerator.adapters.MLPAdapter","title":"<code>modelgenerator.adapters.MLPAdapter</code>","text":"<p>               Bases: <code>Sequential</code>, <code>TokenAdapter</code></p> <p>Multi-layer perceptron (MLP) adapter.</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>Number of features of the input</p> required <code>out_features</code> <code>int</code> <p>Number of features of the output</p> required <code>hidden_sizes</code> <code>List[int]</code> <p>List of the hidden feature dimensions. Defaults to [].</p> <code>[]</code> <code>activation_layer</code> <code>Callable[..., Module]</code> <p>Activation function. Defaults to torch.nn.Tanh.</p> <code>Tanh</code> <code>bias</code> <code>bool</code> <p>Whether to use bias in the linear layer. Defaults to True</p> <code>True</code> <code>dropout</code> <code>float</code> <p>The probability for the dropout layer. Defaults to 0.0</p> <code>0.0</code> <code>dropout_in_middle</code> <code>bool</code> <p>Whether to use dropout in the middle layers. Defaults to True</p> <code>True</code>"},{"location":"api_reference/adapters/#modelgenerator.adapters.MLPAdapterWithoutOutConcat","title":"<code>modelgenerator.adapters.MLPAdapterWithoutOutConcat</code>","text":"<p>               Bases: <code>Module</code>, <code>TokenAdapter</code></p> <p>Multi-layer perceptron (MLP) adapter without outer concatenate</p> <p>This class is generally used in PairwiseTokenClassification. The following two implementations are equivalent: 1. hidden_states -&gt; outer_concat -&gt; MLPAdapter 2. hidden_states -&gt; MLPAdapterWithoutOutConcat MLPAdapterWithoutOutConcat avoids the large memory consumption of outer_concat</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>Number of features of the input</p> required <code>out_features</code> <code>int</code> <p>Number of features of the output</p> required <code>hidden_sizes</code> <code>List[int]</code> <p>List of the hidden feature dimensions. Defaults to [].</p> <code>[]</code> <code>activation_layer</code> <code>Callable[..., Module]</code> <p>Activation function. Defaults to torch.nn.Tanh.</p> <code>Tanh</code> <code>bias</code> <code>bool</code> <p>Whether to use bias in the linear layer. Defaults to True</p> <code>True</code> <code>dropout</code> <code>float</code> <p>The probability for the dropout layer. Defaults to 0.0</p> <code>0.0</code> <code>dropout_in_middle</code> <code>bool</code> <p>Whether to use dropout in the middle layers. Defaults to True</p> <code>True</code>"},{"location":"api_reference/adapters/#conditional-generation-adapters","title":"Conditional Generation Adapters","text":"<p>These adapters are used for conditional generation tasks.</p>"},{"location":"api_reference/adapters/#modelgenerator.adapters.ConditionalLMAdapter","title":"<code>modelgenerator.adapters.ConditionalLMAdapter</code>","text":"<p>               Bases: <code>Module</code>, <code>ConditionalGenerationAdapter</code></p> <p>Conditional sequence adapter</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>Number of input features</p> required <code>embed_dim</code> <code>int</code> <p>Hidden size</p> required <code>seq_len</code> <code>int</code> <p>Sequence length</p> required"},{"location":"api_reference/adapters/#fusion-adapters","title":"Fusion Adapters","text":"<p>These adapters are used for multi-modal fusion to combine multiple backbones.</p>"},{"location":"api_reference/adapters/#modelgenerator.adapters.MMFusionSeqAdapter","title":"<code>modelgenerator.adapters.MMFusionSeqAdapter</code>","text":"<p>               Bases: <code>Module</code>, <code>FusionAdapter</code></p> <p>Multimodal embeddings fusion with SequenceAdapter.</p> Note <p>Accepts 2-3 sequence embeddings as input and fuses them into a multimodal embedding for the adapter.</p> <p>Parameters:</p> Name Type Description Default <code>out_features</code> <code>int</code> <p>Number of output features.</p> required <code>input_size</code> <code>int</code> <p>number of input features for the first modality.</p> required <code>input_size_1</code> <code>int</code> <p>number of input features for the second modality.</p> required <code>input_size_2</code> <code>int</code> <p>number of input features for the third modality.</p> <code>None</code> <code>fusion</code> <code>Callable[[int, int, int], CrossAttentionFusion]</code> <p>The callable that returns a fusion module.</p> <code>CrossAttentionFusion</code> <code>adapter</code> <code>Callable[[int, int], SequenceAdapter]</code> <p>The callable that returns an adapter.</p> <code>LinearCLSAdapter</code>"},{"location":"api_reference/adapters/#modelgenerator.adapters.MMFusionTokenAdapter","title":"<code>modelgenerator.adapters.MMFusionTokenAdapter</code>","text":"<p>               Bases: <code>Module</code>, <code>FusionAdapter</code></p> <p>Multimodal embeddings fusion with TokenAdapter. Fuses embeddings into a single token embedding.</p> Note <p>Accepts 2-3 sequence embeddings as input and fuse them into a multimodal embedding for the adapter</p> <p>Parameters:</p> Name Type Description Default <code>out_features</code> <code>int</code> <p>Number of output features.</p> required <code>input_size</code> <code>int</code> <p>number of input features for the first modality.</p> required <code>input_size_1</code> <code>int</code> <p>number of input features for the second modality.</p> required <code>input_size_2</code> <code>int</code> <p>number of input features for the third modality.</p> <code>None</code> <code>fusion</code> <code>Callable[[int, int, int], ConcatFusion]</code> <p>The callable that returns a fusion module.</p> <code>ConcatFusion</code> <code>adapter</code> <code>Callable[[int, int], TokenAdapter]</code> <p>The callable that returns an adapter.</p> <code>MLPAdapter</code>"},{"location":"api_reference/backbones/","title":"Backbones","text":"<p>Backbones are pretrained foundation models. They are specified with the <code>--model.backbone</code> argument in the CLI or in the <code>model.backbone</code> section of a configuration file.</p> <p>AIDO.ModelGenerator wraps messy foundation models in a standardized interface, allowing them to be applied to finetuning and inference tasks without any code, and even fused for multi-modal tasks. Backbones are also interchangeable, making it simple to run benchmarks and create leaderboards so you can find the best model for your task.</p> <p>Many backbones come with options for parameter-efficient finetuning (PEFT) methods, low-memory checkpointing, and small-scale debugging models to assist with developing on large-scale foundation models.</p> <p>This reference overviews the available no-code backbones. If you would like to integrate new backbones, see Experiment Design.</p> <pre><code># Example Backbone Configuration\nmodel:\n  class_path: modelgenerator.tasks.SequenceRegression\n  init_args:\n    backbone:\n      class_path: modelgenerator.backbones.aido_rna_1b600m_cds\n      init_args:\n        max_length: 1024\n        use_peft: true\n        save_peft_only: true\n        lora_r: 32\n        lora_alpha: 64\n        lora_dropout: 0.1\n        lora_target_modules:\n        - query\n        - value\n        config_overwrites:\n          hidden_dropout_prob: 0.1\n          attention_probs_dropout_prob: 0.1\n        model_init_args: null\ndata:\n  ...\ntrainer:\n  ...\n</code></pre>"},{"location":"api_reference/backbones/#dna","title":"DNA","text":""},{"location":"api_reference/backbones/#modelgenerator.backbones.aido_dna_7b","title":"<code>modelgenerator.backbones.aido_dna_7b</code>","text":"<p>               Bases: <code>GenBioBERT</code></p> <p>AIDO.DNA model with 7B parameters pretrained on 10.6B nucleotides from 796 species in the NCBI RefSeq database.</p> Note <ul> <li>Mauscript: Accurate and General DNA Representations Emerge from Genome Foundation Models at Scale</li> <li>Model Card: AIDO.DNA-7B</li> <li>Weights: genbio-ai/AIDO.DNA-7B</li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>from_scratch</code> <code>bool</code> <p>Whether to create the model from scratch.</p> <code>False</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze encoder.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>32</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[list]</code> <p>LoRA target modules.</p> <code>['query', 'value']</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>genbio-ai/AIDO.DNA-7B</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.aido_dna_300m","title":"<code>modelgenerator.backbones.aido_dna_300m</code>","text":"<p>               Bases: <code>GenBioBERT</code></p> <p>AIDO.DNA model with 300M parameters pretrained on 10.6B nucleotides from 796 species in the NCBI RefSeq database.</p> Note <ul> <li>Mauscript: Accurate and General DNA Representations Emerge from Genome Foundation Models at Scale</li> <li>Model Card: AIDO.DNA-300M</li> <li>Weights: genbio-ai/AIDO.DNA-300M</li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>from_scratch</code> <code>bool</code> <p>Whether to create the model from scratch.</p> <code>False</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze encoder.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>32</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[list]</code> <p>LoRA target modules.</p> <code>['query', 'value']</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>genbio-ai/AIDO.DNA-300M</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.enformer","title":"<code>modelgenerator.backbones.enformer</code>","text":"<p>               Bases: <code>Enformer</code></p> <p>Enformer model</p> Note <ul> <li>Mauscript: Effective gene expression prediction from sequence by integrating long-range interactions</li> <li>GitHub: lucidrains/enformer-pytorch</li> <li>Model Card: EleutherAI/enformer-official-rough</li> <li>Weights: EleutherAI/enformer-official-rough</li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>from_scratch</code> <code>bool</code> <p>Whether to create the model from scratch.</p> <code>False</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>196608</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze model.</p> <code>False</code> <code>delete_crop_layer</code> <code>bool</code> <p>Whether to delete cropping layer.</p> <code>False</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <code>List[str]</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>EleutherAI/enformer-official-rough</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.borzoi","title":"<code>modelgenerator.backbones.borzoi</code>","text":"<p>               Bases: <code>Borzoi</code></p> <p>Borzoi model</p> Note <ul> <li>Mauscript: Predicting RNA-seq coverage from DNA sequence as a unifying model of gene regulation</li> <li>GitHub: johahi/borzoi</li> <li>Weights: johahi/borzoi-replicate-0</li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>from_scratch</code> <code>bool</code> <p>Whether to create the model from scratch.</p> <code>False</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>524288</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze model.</p> <code>False</code> <code>delete_crop_layer</code> <code>bool</code> <p>Whether to skip cropping layer.</p> <code>False</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <code>List[str]</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>johahi/borzoi-replicate-0</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.flashzoi","title":"<code>modelgenerator.backbones.flashzoi</code>","text":"<p>               Bases: <code>Borzoi</code></p> <p>Flashzoi model</p> Note <ul> <li>Mauscript: Flashzoi: A fast and accurate model for predicting RNA-seq coverage from DNA sequence</li> <li>GitHub: johahi/flashzoi</li> <li>Weights: johahi/flashzoi-replicate-0</li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>from_scratch</code> <code>bool</code> <p>Whether to create the model from scratch.</p> <code>False</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>524288</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze model.</p> <code>False</code> <code>delete_crop_layer</code> <code>bool</code> <p>Whether to skip cropping layer.</p> <code>False</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <code>List[str]</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>johahi/flashzoi-replicate-0</p>"},{"location":"api_reference/backbones/#rna","title":"RNA","text":""},{"location":"api_reference/backbones/#modelgenerator.backbones.aido_rna_1b600m","title":"<code>modelgenerator.backbones.aido_rna_1b600m</code>","text":"<p>               Bases: <code>GenBioBERT</code></p> <p>SOTA AIDO.RNA model with 1.6B parameters pretrained on 42M ncRNAs in the RNACentral database.</p> Note <ul> <li>Mauscript: A Large-Scale Foundation Model for RNA Function and Structure Prediction</li> <li>Model Card: AIDO.RNA-1.6B</li> <li>Weights: genbio-ai/AIDO.RNA-1.6B</li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>from_scratch</code> <code>bool</code> <p>Whether to create the model from scratch.</p> <code>False</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze encoder.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>32</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[list]</code> <p>LoRA target modules.</p> <code>['query', 'value']</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>genbio-ai/AIDO.RNA-1.6B</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.aido_rna_1b600m_cds","title":"<code>modelgenerator.backbones.aido_rna_1b600m_cds</code>","text":"<p>               Bases: <code>GenBioBERT</code></p> <p>SOTA AIDO.RNA model with 1.6B parameters adapted from <code>aido_rna_1b600m</code> by continued pretrained on 9M coding sequence RNAs from organisms in ENA.</p> Note <ul> <li>Mauscript: A Large-Scale Foundation Model for RNA Function and Structure Prediction</li> <li>Model Card: AIDO.RNA-1.6B-CDS</li> <li>Weights: genbio-ai/AIDO.RNA-1.6B-CDS</li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>from_scratch</code> <code>bool</code> <p>Whether to create the model from scratch.</p> <code>False</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze encoder.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>32</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[list]</code> <p>LoRA target modules.</p> <code>['query', 'value']</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>genbio-ai/AIDO.RNA-1.6B-CDS</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.aido_rna_650m","title":"<code>modelgenerator.backbones.aido_rna_650m</code>","text":"<p>               Bases: <code>GenBioBERT</code></p> <p>AIDO.RNA model with 650M parameters pretrained on 42M ncRNAs in the RNACentral database.</p> Note <ul> <li>Mauscript: A Large-Scale Foundation Model for RNA Function and Structure Prediction</li> <li>Model Card: AIDO.RNA-650M</li> <li>Weights: genbio-ai/AIDO.RNA-650M</li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>from_scratch</code> <code>bool</code> <p>Whether to create the model from scratch.</p> <code>False</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze encoder.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>32</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[list]</code> <p>LoRA target modules.</p> <code>['query', 'value']</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>genbio-ai/AIDO.RNA-650M</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.aido_rna_650m_cds","title":"<code>modelgenerator.backbones.aido_rna_650m_cds</code>","text":"<p>               Bases: <code>GenBioBERT</code></p> <p>AIDO.RNA model with 650M parameters adapted from <code>aido_rna_650m</code> by continued pretrained on 9M coding sequence RNAs from organisms in ENA.</p> Note <ul> <li>Mauscript: A Large-Scale Foundation Model for RNA Function and Structure Prediction</li> <li>Model Card: AIDO.RNA-650M-CDS</li> <li>Weights: genbio-ai/AIDO.RNA-650M-CDS</li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>from_scratch</code> <code>bool</code> <p>Whether to create the model from scratch.</p> <code>False</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze encoder.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>32</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[list]</code> <p>LoRA target modules.</p> <code>['query', 'value']</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>genbio-ai/AIDO.RNA-650M-CDS</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.aido_rna_300m_mars","title":"<code>modelgenerator.backbones.aido_rna_300m_mars</code>","text":"<p>               Bases: <code>GenBioBERT</code></p> <p>AIDO.RNA model with 300M parameters pretrained on 886M RNAs in the MARS dataset.</p> Note <ul> <li>Mauscript: A Large-Scale Foundation Model for RNA Function and Structure Prediction</li> <li>Model Card: AIDO.RNA-300M-MARS</li> <li>Weights: genbio-ai/AIDO.RNA-300M-MARS</li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>from_scratch</code> <code>bool</code> <p>Whether to create the model from scratch.</p> <code>False</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze encoder.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>32</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[list]</code> <p>LoRA target modules.</p> <code>['query', 'value']</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>genbio-ai/AIDO.RNA-300M-MARS</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.aido_rna_25m_mars","title":"<code>modelgenerator.backbones.aido_rna_25m_mars</code>","text":"<p>               Bases: <code>GenBioBERT</code></p> <p>AIDO.RNA model with 25M parameters pretrained on 886M RNAs in the MARS dataset.</p> Note <ul> <li>Mauscript: A Large-Scale Foundation Model for RNA Function and Structure Prediction</li> <li>Model Card: AIDO.RNA-25M-MARS</li> <li>Weights: genbio-ai/AIDO.RNA-25M-MARS</li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>from_scratch</code> <code>bool</code> <p>Whether to create the model from scratch.</p> <code>False</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze encoder.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>32</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[list]</code> <p>LoRA target modules.</p> <code>['query', 'value']</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>genbio-ai/AIDO.RNA-25M-MARS</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.aido_rna_1m_mars","title":"<code>modelgenerator.backbones.aido_rna_1m_mars</code>","text":"<p>               Bases: <code>GenBioBERT</code></p> <p>AIDO.RNA model with 1M parameters pretrained on 886M RNAs in the MARS dataset.</p> Note <ul> <li>Mauscript: A Large-Scale Foundation Model for RNA Function and Structure Prediction</li> <li>Model Card: AIDO.RNA-1M-MARS</li> <li>Weights: genbio-ai/AIDO.RNA-1M-MARS</li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>from_scratch</code> <code>bool</code> <p>Whether to create the model from scratch.</p> <code>False</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze encoder.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>32</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[list]</code> <p>LoRA target modules.</p> <code>['query', 'value']</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>genbio-ai/AIDO.RNA-1M-MARS</p>"},{"location":"api_reference/backbones/#protein","title":"Protein","text":""},{"location":"api_reference/backbones/#modelgenerator.backbones.aido_protein_16b","title":"<code>modelgenerator.backbones.aido_protein_16b</code>","text":"<p>               Bases: <code>GenBioFM</code></p> <p>AIDO.Protein model with 16B parameters pretrained on 1.2T amino acids from UniRef90 and ColabFoldDB.</p> Note <ul> <li>Mauscript: Mixture of Experts Enable Efficient and Effective Protein Understanding and Design</li> <li>Model Card: AIDO.Protein-16B</li> <li>Weights: genbio-ai/AIDO.Protein-16B</li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>from_scratch</code> <code>bool</code> <p>Whether to create the model from scratch.</p> <code>False</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze encoder.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>16</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[List[str]]</code> <p>LoRA target modules.</p> <code>['query', 'value', 'key', 'dense', 'router']</code> <code>lora_modules_to_save</code> <code>Optional[List[str]]</code> <p>LoRA modules to save.</p> <code>None</code> <code>lora_use_rslora</code> <code>bool</code> <p>Whether to use RSLora.</p> <code>False</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>genbio-ai/AIDO.Protein-16B</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.aido_protein_16b_v1","title":"<code>modelgenerator.backbones.aido_protein_16b_v1</code>","text":"<p>               Bases: <code>GenBioFM</code></p> <p>AIDO.Protein model with 16B parameters adapted from <code>aido_protein_16b</code> by continued pretrained on 100B amino acids from UniRef90.</p> Note <ul> <li>Mauscript: Mixture of Experts Enable Efficient and Effective Protein Understanding and Design</li> <li>Model Card: AIDO.Protein-16B-v1</li> <li>Weights: genbio-ai/AIDO.Protein-16B-v1</li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>from_scratch</code> <code>bool</code> <p>Whether to create the model from scratch.</p> <code>False</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze encoder.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>16</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[List[str]]</code> <p>LoRA target modules.</p> <code>['query', 'value', 'key', 'dense', 'router']</code> <code>lora_modules_to_save</code> <code>Optional[List[str]]</code> <p>LoRA modules to save.</p> <code>None</code> <code>lora_use_rslora</code> <code>bool</code> <p>Whether to use RSLora.</p> <code>False</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>genbio-ai/AIDO.Protein-16B-v1</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.esm2_15b","title":"<code>modelgenerator.backbones.esm2_15b</code>","text":"<p>               Bases: <code>ESM</code></p> <p>ESM2 15B model</p> Note <ul> <li>Mauscript: Evolutionary-scale prediction of atomic level protein structure with a language model</li> <li>GitHub: facebookresearch/esm</li> <li>Model Card: facebook/esm2_t48_15B_UR50D</li> <li>Weights: facebook/esm2_t48_15B_UR50D</li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze encoder.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>32</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[List[str]]</code> <p>LoRA target modules.</p> <code>None</code> <code>lora_modules_to_save</code> <code>Optional[List[str]]</code> <p>LoRA modules to save.</p> <code>None</code> <code>lora_use_rslora</code> <code>bool</code> <p>Whether to use RSLora.</p> <code>False</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <code>List[str]</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>facebook/esm2_t48_15B_UR50D</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.esm2_3b","title":"<code>modelgenerator.backbones.esm2_3b</code>","text":"<p>               Bases: <code>ESM</code></p> <p>ESM2 3B model</p> Note <ul> <li>Mauscript: Evolutionary-scale prediction of atomic level protein structure with a language model</li> <li>GitHub: facebookresearch/esm</li> <li>Model Card: facebook/esm2_t36_3B_UR50D</li> <li>Weights: facebook/esm2_t36_3B_UR50D</li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze encoder.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>32</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[List[str]]</code> <p>LoRA target modules.</p> <code>None</code> <code>lora_modules_to_save</code> <code>Optional[List[str]]</code> <p>LoRA modules to save.</p> <code>None</code> <code>lora_use_rslora</code> <code>bool</code> <p>Whether to use RSLora.</p> <code>False</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <code>List[str]</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>facebook/esm2_t36_3B_UR50D</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.esm2_650m","title":"<code>modelgenerator.backbones.esm2_650m</code>","text":"<p>               Bases: <code>ESM</code></p> <p>ESM2 650M model</p> Note <ul> <li>Mauscript: Evolutionary-scale prediction of atomic level protein structure with a language model</li> <li>GitHub: facebookresearch/esm</li> <li>Model Card: facebook/esm2_t33_650M_UR50D</li> <li>Weights: facebook/esm2_t33_650M_UR50D</li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze encoder.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>32</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[List[str]]</code> <p>LoRA target modules.</p> <code>None</code> <code>lora_modules_to_save</code> <code>Optional[List[str]]</code> <p>LoRA modules to save.</p> <code>None</code> <code>lora_use_rslora</code> <code>bool</code> <p>Whether to use RSLora.</p> <code>False</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <code>List[str]</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>facebook/esm2_t33_650M_UR50D</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.esm2_150m","title":"<code>modelgenerator.backbones.esm2_150m</code>","text":"<p>               Bases: <code>ESM</code></p> <p>ESM2 150M model</p> Note <ul> <li>Mauscript: Evolutionary-scale prediction of atomic level protein structure with a language model</li> <li>GitHub: facebookresearch/esm</li> <li>Model Card: facebook/esm2_t30_150M_UR50D</li> <li>Weights: facebook/esm2_t30_150M_UR50D</li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze encoder.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>32</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[List[str]]</code> <p>LoRA target modules.</p> <code>None</code> <code>lora_modules_to_save</code> <code>Optional[List[str]]</code> <p>LoRA modules to save.</p> <code>None</code> <code>lora_use_rslora</code> <code>bool</code> <p>Whether to use RSLora.</p> <code>False</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <code>List[str]</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>facebook/esm2_t30_150M_UR50D</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.esm2_35m","title":"<code>modelgenerator.backbones.esm2_35m</code>","text":"<p>               Bases: <code>ESM</code></p> <p>ESM2 35M model</p> Note <ul> <li>Mauscript: Evolutionary-scale prediction of atomic level protein structure with a language model</li> <li>GitHub: facebookresearch/esm</li> <li>Model Card: facebook/esm2_t12_35M_UR50D</li> <li>Weights: facebook/esm2_t12_35M_UR50D</li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze encoder.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>32</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[List[str]]</code> <p>LoRA target modules.</p> <code>None</code> <code>lora_modules_to_save</code> <code>Optional[List[str]]</code> <p>LoRA modules to save.</p> <code>None</code> <code>lora_use_rslora</code> <code>bool</code> <p>Whether to use RSLora.</p> <code>False</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <code>List[str]</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>facebook/esm2_t12_35M_UR50D</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.esm2_8m","title":"<code>modelgenerator.backbones.esm2_8m</code>","text":"<p>               Bases: <code>ESM</code></p> <p>ESM2 8M model</p> Note <ul> <li>Mauscript: Evolutionary-scale prediction of atomic level protein structure with a language model</li> <li>GitHub: facebookresearch/esm</li> <li>Model Card: facebook/esm2_t6_8M_UR50D</li> <li>Weights: facebook/esm2_t6_8M_UR50D</li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze encoder.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>32</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[List[str]]</code> <p>LoRA target modules.</p> <code>None</code> <code>lora_modules_to_save</code> <code>Optional[List[str]]</code> <p>LoRA modules to save.</p> <code>None</code> <code>lora_use_rslora</code> <code>bool</code> <p>Whether to use RSLora.</p> <code>False</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <code>List[str]</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>facebook/esm2_t6_8M_UR50D</p>"},{"location":"api_reference/backbones/#structure","title":"Structure","text":""},{"location":"api_reference/backbones/#modelgenerator.backbones.aido_protein2structoken_16b","title":"<code>modelgenerator.backbones.aido_protein2structoken_16b</code>","text":"<p>               Bases: <code>GenBioFM</code></p> <p>AIDO.Protein2StructureToken model with 16B parameters adapted from <code>aido_protein_16b</code> and for structure prediction with AIDO.StructureTokenizer. The model is trained on 170M sequences and structures from AlphaFold Database and 0.4M sequences and structures from PDB.</p> Note <ul> <li>Mauscripts:<ul> <li>Mixture of Experts Enable Efficient and Effective Protein Understanding and Design</li> <li>Balancing Locality and Reconstruction in Protein Structure Tokenizer</li> </ul> </li> <li>Model Card: AIDO.Protein2StructureToken-16B</li> <li>Weights: genbio-ai/AIDO.Protein2StructureToken-16B</li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>from_scratch</code> <code>bool</code> <p>Whether to create the model from scratch.</p> <code>False</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze encoder.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>16</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[List[str]]</code> <p>LoRA target modules.</p> <code>['query', 'value', 'key', 'dense', 'router']</code> <code>lora_modules_to_save</code> <code>Optional[List[str]]</code> <p>LoRA modules to save.</p> <code>None</code> <code>lora_use_rslora</code> <code>bool</code> <p>Whether to use RSLora.</p> <code>False</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>genbio-ai/AIDO.Protein2StructureToken-16B</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.aido_protein_rag_16b","title":"<code>modelgenerator.backbones.aido_protein_rag_16b</code>","text":"<p>               Bases: <code>GenBioFM</code></p> <p>AIDO.Protein-RAG model with 16B parameters adapted from <code>aido_protein_16b</code> with 180B tokens of MSA and structural context from UniRef50/UniClust30 and AlphaFold Database.</p> Note <ul> <li>Mauscripts:<ul> <li>Mixture of Experts Enable Efficient and Effective Protein Understanding and Design</li> <li>Retrieval Augmented Protein Language Models for Protein Structure Prediction</li> </ul> </li> <li>Model Card: AIDO.Protein-RAG-16B</li> <li>Weights: genbio-ai/AIDO.Protein-RAG-16B</li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>from_scratch</code> <code>bool</code> <p>Whether to create the model from scratch.</p> <code>False</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze encoder.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>16</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[List[str]]</code> <p>LoRA target modules.</p> <code>['query', 'value', 'key', 'dense', 'router']</code> <code>lora_modules_to_save</code> <code>Optional[List[str]]</code> <p>LoRA modules to save.</p> <code>None</code> <code>lora_use_rslora</code> <code>bool</code> <p>Whether to use RSLora.</p> <code>False</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>genbio-ai/AIDO.Protein-RAG-16B</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.aido_protein_rag_3b","title":"<code>modelgenerator.backbones.aido_protein_rag_3b</code>","text":"<p>               Bases: <code>GenBioFM</code></p> <p>AIDO.Protein-RAG model with 3B parameters adapted from a 3B version of AIDO.Protein 16B with 180B tokens of MSA and structural context from UniRef50/UniClust30 and AlphaFold Database.</p> Note <ul> <li>Mauscripts:<ul> <li>Mixture of Experts Enable Efficient and Effective Protein Understanding and Design</li> <li>Retrieval Augmented Protein Language Models for Protein Structure Prediction</li> </ul> </li> <li>Model Card: AIDO.Protein-RAG-3B</li> <li>Weights: genbio-ai/AIDO.Protein-RAG-3B</li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>from_scratch</code> <code>bool</code> <p>Whether to create the model from scratch.</p> <code>False</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze encoder.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>16</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[List[str]]</code> <p>LoRA target modules.</p> <code>['query', 'value', 'key', 'dense', 'router']</code> <code>lora_modules_to_save</code> <code>Optional[List[str]]</code> <p>LoRA modules to save.</p> <code>None</code> <code>lora_use_rslora</code> <code>bool</code> <p>Whether to use RSLora.</p> <code>False</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>genbio-ai/AIDO.Protein-RAG-3B</p>"},{"location":"api_reference/backbones/#cell","title":"Cell","text":""},{"location":"api_reference/backbones/#modelgenerator.backbones.aido_cell_100m","title":"<code>modelgenerator.backbones.aido_cell_100m</code>","text":"<p>               Bases: <code>GenBioCellFoundation</code></p> <p>AIDO.Cell model with 100M parameters pretrained on 50M single-cell expression profiles from diverse set of human tissues and organs.</p> Note <ul> <li>Mauscript: Scaling Dense Representations for Single Cell with Transcriptome-Scale Context</li> <li>Model Card: AIDO.Cell-100M</li> <li>Weights: genbio-ai/AIDO.Cell-100M</li> <li>Integrations:<ul> <li>CZI Virtual Cell Models</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>from_scratch</code> <code>bool</code> <p>Whether to create the model from scratch.</p> <code>False</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze encoder.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>16</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[List[str]]</code> <p>LoRA target modules.</p> <code>['query', 'value', 'key', 'dense', 'router']</code> <code>lora_modules_to_save</code> <code>Optional[List[str]]</code> <p>LoRA modules to save.</p> <code>None</code> <code>lora_use_rslora</code> <code>bool</code> <p>Whether to use RSLora.</p> <code>False</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>genbio-ai/AIDO.Cell-100M</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.aido_cell_10m","title":"<code>modelgenerator.backbones.aido_cell_10m</code>","text":"<p>               Bases: <code>GenBioCellFoundation</code></p> <p>AIDO.Cell model with 10M parameters pretrained on 50M single-cell expression profiles from diverse set of human tissues and organs.</p> Note <ul> <li>Mauscript: Scaling Dense Representations for Single Cell with Transcriptome-Scale Context</li> <li>Model Card: AIDO.Cell-10M</li> <li>Weights: genbio-ai/AIDO.Cell-10M</li> <li>Integrations:<ul> <li>CZI Virtual Cell Models</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>from_scratch</code> <code>bool</code> <p>Whether to create the model from scratch.</p> <code>False</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze encoder.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>16</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[List[str]]</code> <p>LoRA target modules.</p> <code>['query', 'value', 'key', 'dense', 'router']</code> <code>lora_modules_to_save</code> <code>Optional[List[str]]</code> <p>LoRA modules to save.</p> <code>None</code> <code>lora_use_rslora</code> <code>bool</code> <p>Whether to use RSLora.</p> <code>False</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>genbio-ai/AIDO.Cell-10M</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.aido_cell_3m","title":"<code>modelgenerator.backbones.aido_cell_3m</code>","text":"<p>               Bases: <code>GenBioCellFoundation</code></p> <p>AIDO.Cell model with 3M parameters pretrained on 50M single-cell expression profiles from diverse set of human tissues and organs.</p> Note <ul> <li>Mauscript: Scaling Dense Representations for Single Cell with Transcriptome-Scale Context</li> <li>Model Card: AIDO.Cell-3M</li> <li>Weights: genbio-ai/AIDO.Cell-3M</li> <li>Integrations:<ul> <li>CZI Virtual Cell Models</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>from_scratch</code> <code>bool</code> <p>Whether to create the model from scratch.</p> <code>False</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze encoder.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>16</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[List[str]]</code> <p>LoRA target modules.</p> <code>['query', 'value', 'key', 'dense', 'router']</code> <code>lora_modules_to_save</code> <code>Optional[List[str]]</code> <p>LoRA modules to save.</p> <code>None</code> <code>lora_use_rslora</code> <code>bool</code> <p>Whether to use RSLora.</p> <code>False</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>genbio-ai/AIDO.Cell-3M</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.scfoundation","title":"<code>modelgenerator.backbones.scfoundation</code>","text":"<p>               Bases: <code>SCFoundation</code></p> <p>scFoundation model</p> Note <ul> <li>Mauscript: Large-scale foundation model on single-cell transcriptomics</li> <li>GitHub: genbio-ai/scFoundation</li> <li>Model Card: genbio-ai/scFoundation</li> <li>Weights: genbio-ai/scFoundation</li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>num_genes</code> <code>Optional[int]</code> <p>Number of genes in the model context.</p> <code>19264</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze model.</p> <code>False</code> <code>output_type</code> <code>str</code> <p>Type of output embedding ('cell', 'gene', 'gene_batch', 'gene_expression').</p> <code>'cell'</code> <code>pool_type</code> <code>str</code> <p>Pooling type for cell embedding ('all', 'max').</p> <code>'all'</code> <code>input_type</code> <code>str</code> <p>Input data type ('singlecell', 'bulk').</p> <code>'singlecell'</code> <code>pre_normalized</code> <code>str</code> <p>Whether input is pre-normalized ('T', 'F', 'A').</p> <code>'F'</code> <code>train_last_n_layers</code> <code>int</code> <p>Number of layers to train in the encoder.</p> <code>0</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <code>List[str]</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>genbio-ai/scFoundation</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.geneformer","title":"<code>modelgenerator.backbones.geneformer</code>","text":"<p>               Bases: <code>Geneformer</code></p> <p>Geneformer model</p> Note <ul> <li>Mauscript: Transfer learning enables predictions in network biology</li> <li>Model Card: ctheodoris/Geneformer</li> <li>Weights: ctheodoris/Geneformer</li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>from_scratch</code> <code>bool</code> <p>Whether to initialize from random weights.</p> <code>False</code> <code>max_length</code> <code>int</code> <p>Maximum input sequence length.</p> <code>4096</code> <code>emb_layer</code> <code>int</code> <p>Layer to extract embeddings from.</p> <code>-2</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <code>List[str]</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>ctheodoris/Geneformer</p>"},{"location":"api_reference/backbones/#tissue","title":"Tissue","text":""},{"location":"api_reference/backbones/#modelgenerator.backbones.aido_tissue_3m","title":"<code>modelgenerator.backbones.aido_tissue_3m</code>","text":"<p>               Bases: <code>GenBioCellSpatialFoundation</code></p> <p>AIDO.Tissue model with 3M parameters adapted from <code>aido_cell_3m</code> to incorporate tissue context.</p> Note <ul> <li>Mauscript: Scaling Dense Representations for Single Cell with Transcriptome-Scale Context</li> <li>Model Card: AIDO.Tissue-3M</li> <li>Weights: genbio-ai/AIDO.Tissue-3M</li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>from_scratch</code> <code>bool</code> <p>Whether to create the model from scratch.</p> <code>False</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze encoder.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>16</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[List[str]]</code> <p>LoRA target modules.</p> <code>['query', 'value', 'key', 'dense', 'router']</code> <code>lora_modules_to_save</code> <code>Optional[List[str]]</code> <p>LoRA modules to save.</p> <code>None</code> <code>lora_use_rslora</code> <code>bool</code> <p>Whether to use RSLora.</p> <code>False</code> <code>rope2d_use_xy</code> <code>bool</code> <p>Whether to use 2D rope encoding.</p> <code>False</code> <code>sep_value</code> <code>int</code> <p>Separator value for the model.</p> <code>-10000</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <p>genbio-ai/AIDO.Tissue-3M</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.aido_tissue_60m","title":"<code>modelgenerator.backbones.aido_tissue_60m</code>","text":"<p>               Bases: <code>GenBioCellSpatialFoundation</code></p> <p>AIDO.Tissue model with 60M parameters adapted from AIDO.Cell to incorporate tissue context.</p> Note <ul> <li>Mauscript: Scaling Dense Representations for Single Cell with Transcriptome-Scale Context</li> <li>Model Card: AIDO.Tissue-60M</li> <li>Weights: genbio-ai/AIDO.Tissue-60M</li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>from_scratch</code> <code>bool</code> <p>Whether to create the model from scratch.</p> <code>False</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze encoder.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>16</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[List[str]]</code> <p>LoRA target modules.</p> <code>['query', 'value', 'key', 'dense', 'router']</code> <code>lora_modules_to_save</code> <code>Optional[List[str]]</code> <p>LoRA modules to save.</p> <code>None</code> <code>lora_use_rslora</code> <code>bool</code> <p>Whether to use RSLora.</p> <code>False</code> <code>rope2d_use_xy</code> <code>bool</code> <p>Whether to use 2D rope encoding.</p> <code>False</code> <code>sep_value</code> <code>int</code> <p>Separator value for the model.</p> <code>-10000</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>genbio-ai/AIDO.Tissue-60M</p>"},{"location":"api_reference/backbones/#integrations","title":"Integrations","text":""},{"location":"api_reference/backbones/#modelgenerator.backbones.Huggingface","title":"<code>modelgenerator.backbones.Huggingface</code>","text":"<p>               Bases: <code>HFSequenceBackbone</code></p> <p>A generic huggingface wrapper allows for using any huggingface model as backbone.</p> Note <p>Warning: This is an experimental feature, don't expect it to work with all models. Downstream task support is also extremely limited to the standard huggingface heads. Its usage often involves manual configuration of the model's head through <code>config_overwrites</code>.</p> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>model_path</code> <code>str | PathLike</code> <p>Path to the huggingface model.</p> required <code>modules_for_model_registration</code> <code>Optional[List[str]]</code> <p>List of python modules to register the model.</p> <code>None</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>16</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[List[str]]</code> <p>LoRA target modules.</p> <code>None</code> <code>lora_modules_to_save</code> <code>Optional[List[str]]</code> <p>LoRA modules to save.</p> <code>None</code> <code>lora_use_rslora</code> <code>bool</code> <p>Whether to use RSLora.</p> <code>False</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <code>List[str]</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>Path to the model weights. May be HF.</p>"},{"location":"api_reference/backbones/#debug","title":"Debug","text":""},{"location":"api_reference/backbones/#modelgenerator.backbones.Onehot","title":"<code>modelgenerator.backbones.Onehot</code>","text":"<p>               Bases: <code>HFSequenceBackbone</code></p> <p>Tokenizer-only model for one-hot encoding. Useful for baseline model testing (CNNs, linear, etc.)</p> Note <p>Models using this interface include <code>dna_onehot</code> and <code>protein_onehot</code>.</p> <p>Does not contain any parameters, and cannot be used without an adapter.</p> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>vocab_file</code> <code>str</code> <p>Path to the vocabulary file. Defaults to \"modelgenerator/huggingface_models/rnabert/vocab.txt\".</p> <code>None</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>512</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>Path to the model weights. May be HF.</p> <code>vocab_file</code> <code>str</code> <p>Path to the vocabulary file.</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.dna_onehot","title":"<code>modelgenerator.backbones.dna_onehot</code>","text":"<p>               Bases: <code>Onehot</code></p> <p>One-hot encoding for DNA sequences.  Used for benchmarking finetuning tasks without pretrained embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>vocab_file</code> <code>str</code> <p>Path to the vocabulary file. Defaults to \"modelgenerator/huggingface_models/rnabert/vocab.txt\".</p> <code>None</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>512</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>Path to the model weights. May be HF.</p> <code>vocab_file</code> <code>str</code> <p>Path to the vocabulary file <code>modelgenerator/huggingface_models/dnabert/vocab.txt</code></p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.protein_onehot","title":"<code>modelgenerator.backbones.protein_onehot</code>","text":"<p>               Bases: <code>Onehot</code></p> <p>One-hot encoding for protein sequences. Used for benchmarking finetuning tasks without pretrained embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>vocab_file</code> <code>str</code> <p>Path to the vocabulary file. Defaults to \"modelgenerator/huggingface_models/rnabert/vocab.txt\".</p> <code>None</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>512</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>Path to the model weights. May be HF.</p> <code>vocab_file</code> <code>str</code> <p>Path to the vocabulary file <code>modelgenerator/huggingface_models/fm4bio/vocab_protein.txt</code></p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.aido_dna_debug","title":"<code>modelgenerator.backbones.aido_dna_debug</code>","text":"<p>               Bases: <code>GenBioBERT</code></p> <p>A small dna/rna dense transformer model created from scratch for debugging purposes only.</p> Note <ul> <li>This model is not intended for any real-world applications and is only for testing purposes.</li> <li>It is created from scratch with a very small number of parameters and is not trained on any data.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Positional arguments passed to the parent class.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments passed to the parent class. <code>from_scratch=True</code> and <code>config_overwrites={'hidden_size': 64, 'num_hidden_layers': 2, 'num_attention_heads': 4, 'intermediate_size': 128}</code> are always overridden.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>Path to the model weights. May be HF.</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.aido_protein_debug","title":"<code>modelgenerator.backbones.aido_protein_debug</code>","text":"<p>               Bases: <code>GenBioFM</code></p> <p>A small protein dense transformer model created from scratch for debugging purposes only.</p> Note <ul> <li>This model is not intended for any real-world applications and is only for testing purposes.</li> <li>It is created from scratch with a very small number of parameters and is not trained on any data.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Positional arguments passed to the parent class.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments passed to the parent class. <code>from_scratch=True</code> and <code>config_overwrites={'hidden_size': 64, 'num_hidden_layers': 2, 'num_attention_heads': 4, 'intermediate_size': 128}</code> are always overridden.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>Path to the model weights. May be HF.</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.aido_dna_dummy","title":"<code>modelgenerator.backbones.aido_dna_dummy</code>","text":"<p>               Bases: <code>GenBioBERT</code></p> <p>A small dummy AIDO.DNA model created from scratch for debugging purposes only</p> Note <ul> <li>This model is not intended for any real-world applications and is only for testing purposes.</li> <li>It has a very small number of parameters and is not trained on any data.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>from_scratch</code> <code>bool</code> <p>Whether to create the model from scratch.</p> <code>False</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze encoder.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>32</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[list]</code> <p>LoRA target modules.</p> <code>['query', 'value']</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <p>genbio-ai/AIDO.DNA-dummy</p>"},{"location":"api_reference/backbones/#base-classes","title":"Base Classes","text":""},{"location":"api_reference/backbones/#modelgenerator.backbones.SequenceBackboneInterface","title":"<code>modelgenerator.backbones.SequenceBackboneInterface</code>","text":"<p>               Bases: <code>Module</code></p> <p>Interface class to ensure consistent implementation of essential methods for all backbones.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>The description is missing.</p> required <code>**kwargs</code> <p>The description is missing.</p> required <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <code>List[str]</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>Path to the model weights. May be HF.</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.HFSequenceBackbone","title":"<code>modelgenerator.backbones.HFSequenceBackbone</code>","text":"<p>               Bases: <code>SequenceBackboneInterface</code></p> <p>Base class for all backbone models</p> Note <p>The required possitional arguments are reserved by downstream tasks for dependency injection and cannot be changed by the user.</p> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[dict, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <code>List[str]</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>Path to the model weights. May be HF.</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.GenBioBERT","title":"<code>modelgenerator.backbones.GenBioBERT</code>","text":"<p>               Bases: <code>HFSequenceBackbone</code></p> <p>GenBioBERT model</p> Note <p>Models using this interface include <code>aido_dna_7b</code>, <code>aido_dna_300m</code>, <code>dna_dummy</code>, <code>aido_dna_debug</code>, <code>aido_rna_1b600m</code>, <code>aido_rna_1b600m_cds</code>, <code>aido_rna_1m_mars</code>, <code>aido_rna_25m_mars</code>, <code>aido_rna_300m_mars</code>, <code>aido_rna_650m</code>, <code>aido_rna_650m_cds</code>.</p> <p>FSDP auto_wrap_policy is <code>modelgenerator.distributed.fsdp.wrap.AutoWrapPolicy</code></p> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>from_scratch</code> <code>bool</code> <p>Whether to create the model from scratch.</p> <code>False</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze encoder.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>32</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[list]</code> <p>LoRA target modules.</p> <code>['query', 'value']</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>Path to the model weights. May be HF.</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.GenBioFM","title":"<code>modelgenerator.backbones.GenBioFM</code>","text":"<p>               Bases: <code>HFSequenceBackbone</code></p> <p>GenBioFM model</p> Note <p>Models using this interface include <code>aido_protein_16b</code>, <code>aido_protein_16b_v1</code>, <code>aido_protein2structoken_16b</code>, <code>aido_protein_debug</code>.</p> <p>FSDP auto_wrap_policy is <code>modelgenerator.distributed.fsdp.wrap.AutoWrapPolicy</code></p> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>from_scratch</code> <code>bool</code> <p>Whether to create the model from scratch.</p> <code>False</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze encoder.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>16</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[List[str]]</code> <p>LoRA target modules.</p> <code>['query', 'value', 'key', 'dense', 'router']</code> <code>lora_modules_to_save</code> <code>Optional[List[str]]</code> <p>LoRA modules to save.</p> <code>None</code> <code>lora_use_rslora</code> <code>bool</code> <p>Whether to use RSLora.</p> <code>False</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>Path to the model weights. May be HF.</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.GenBioCellFoundation","title":"<code>modelgenerator.backbones.GenBioCellFoundation</code>","text":"<p>               Bases: <code>HFSequenceBackbone</code></p> <p>GenBioCellFoundation model</p> Note <p>Models using this interface include <code>aido_cell_100m</code>, <code>aido_cell_10m</code>, and <code>aido_cell_3m</code>.</p> <p>FSDP auto_wrap_policy is <code>modelgenerator.distributed.fsdp.wrap.AutoWrapPolicy</code></p> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>from_scratch</code> <code>bool</code> <p>Whether to create the model from scratch.</p> <code>False</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze encoder.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>16</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[List[str]]</code> <p>LoRA target modules.</p> <code>['query', 'value', 'key', 'dense', 'router']</code> <code>lora_modules_to_save</code> <code>Optional[List[str]]</code> <p>LoRA modules to save.</p> <code>None</code> <code>lora_use_rslora</code> <code>bool</code> <p>Whether to use RSLora.</p> <code>False</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>Path to the model weights. May be HF.</p>"},{"location":"api_reference/backbones/#modelgenerator.backbones.GenBioCellSpatialFoundation","title":"<code>modelgenerator.backbones.GenBioCellSpatialFoundation</code>","text":"<p>               Bases: <code>HFSequenceBackbone</code></p> <p>GenBioCellSpatialFoundation model</p> Note <p>Models using this interface include <code>aido_tissue_60m</code> and <code>aido_tissue_3m</code>.</p> <p>FSDP auto_wrap_policy is <code>modelgenerator.distributed.fsdp.wrap.AutoWrapPolicy</code></p> <p>Parameters:</p> Name Type Description Default <code>legacy_adapter_type</code> <code>Union[LegacyAdapterType, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>default_config</code> <code>Union[DefaultConfig, None]</code> <p>Ignore. Reserved for use by <code>use_legacy_adapter</code> in Tasks.</p> required <code>from_scratch</code> <code>bool</code> <p>Whether to create the model from scratch.</p> <code>False</code> <code>max_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>None</code> <code>use_peft</code> <code>bool</code> <p>Whether to use LoRA PEFT.</p> <code>False</code> <code>frozen</code> <code>bool</code> <p>Whether to freeze encoder.</p> <code>False</code> <code>save_peft_only</code> <code>bool</code> <p>Whether to save only the PEFT weights.</p> <code>True</code> <code>lora_r</code> <code>int</code> <p>LoRA r parameter.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>LoRA alpha parameter.</p> <code>16</code> <code>lora_dropout</code> <code>float</code> <p>LoRA dropout.</p> <code>0.1</code> <code>lora_target_modules</code> <code>Optional[List[str]]</code> <p>LoRA target modules.</p> <code>['query', 'value', 'key', 'dense', 'router']</code> <code>lora_modules_to_save</code> <code>Optional[List[str]]</code> <p>LoRA modules to save.</p> <code>None</code> <code>lora_use_rslora</code> <code>bool</code> <p>Whether to use RSLora.</p> <code>False</code> <code>rope2d_use_xy</code> <code>bool</code> <p>Whether to use 2D rope encoding.</p> <code>False</code> <code>sep_value</code> <code>int</code> <p>Separator value for the model.</p> <code>-10000</code> <code>config_overwrites</code> <code>Optional[dict]</code> <p>Optional model arguments for PretrainedConfig.</p> <code>None</code> <code>model_init_args</code> <code>Optional[dict]</code> <p>Optional model arguments passed to its init method.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>Path to the model weights. May be HF.</p>"},{"location":"api_reference/callbacks/","title":"Callbacks","text":"<p>Callbacks can be used with the LightningCLI trainer to inject custom behavior into the training process. Callbacks are configured in the <code>trainer</code> section of the YAML configuration file. </p> <p>We provide a few custom callbacks for common use cases, but many more are available in the Lightning ecosystem. Check the Trainer documentation for more details.</p> <pre><code># Example Callback Configuration\ntrainer:\n  callbacks:\n  - class_path: modelgenerator.callbacks.PredictionWriter\n    dict_kwargs:\n      output_dir: my_predictions\n      filetype: tsv\n      write_cols:\n        - id\n        - prediction\n        - label\nmodel:\n  ...\ndata:\n  ...\n</code></pre>"},{"location":"api_reference/callbacks/#modelgenerator.callbacks.PredictionWriter","title":"<code>modelgenerator.callbacks.PredictionWriter</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Write batch predictions to files, and merge batch files into a single file at the end of the epoch. Note:     When saving the given data to a TSV file, any tensors in the data will have their last dimension     squeezed and converted into lists to ensure proper formatting for TSV output.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str</code> <p>Directory to save predictions.</p> required <code>filetype</code> <code>str</code> <p>Type of outputfile. Options are 'tsv' and 'pt'.</p> required <code>write_cols</code> <code>list</code> <p>The head columns of tsv file if filetype is set to 'tsv'. Defaults to None</p> <code>None</code>"},{"location":"api_reference/callbacks/#modelgenerator.callbacks.FTScheduler","title":"<code>modelgenerator.callbacks.FTScheduler</code>","text":"<p>               Bases: <code>BaseFinetuning</code></p> <p>Finetuning scheduler that gradually unfreezes layers based on a schedule</p> <p>Parameters:</p> Name Type Description Default <code>ft_schedule_path</code> <code>str</code> <p>Path to a finetuning schedule that mentions which modules to unfreeze at which epoch. See tutorial for examples.</p> required"},{"location":"api_reference/data/","title":"Data","text":"<p>Data modules specify data sources, as well as data loading and preprocessing for use with Tasks. They provide a simple interface for swapping data sources and re-using datasets for new workflows without any code changes, enabling rapid and reproducible experimentation. They are specified with the <code>--data</code> arguent in the CLI or in the <code>data</code> section of a configuration file.</p> <p>Data modules can automatically load common data sources (json, tsv, txt, HuggingFace) and uncommon ones (h5ad, TileDB). They transform, split, and sample these sources for training with <code>mgen fit</code>, evaluation with <code>mgen test/validate</code>, and inference with <code>mgen predict</code>.</p> <p>This reference overviews the available no-code data modules. If you would like to develop new datasets, see Experiment Design.</p> <pre><code>data:\n  class_path: modelgenerator.data.DMSFitnessPrediction\n  init_args:\n    path: genbio-ai/ProteinGYM-DMS\n    train_split_files:\n    - indels/B1LPA6_ECOSM_Russ_2020_indels.tsv\n    train_split_name: train\n    random_seed: 42\n    batch_size: 32\n    cv_num_folds: 5\n    cv_test_fold_id: 0\n    cv_enable_val_fold: true\n    cv_fold_id_col: fold_id\nmodel:\n  ...\ntrainer:\n  ...\n</code></pre> <p>Note: Data modules are designed for use with a specific task, indicated in the class name.</p>"},{"location":"api_reference/data/#dna","title":"DNA","text":""},{"location":"api_reference/data/#modelgenerator.data.NTClassification","title":"<code>modelgenerator.data.NTClassification</code>","text":"<p>               Bases: <code>SequenceClassificationDataModule</code></p> <p>Nucleotide Transformer benchmarks from InstaDeep.</p> Note <ul> <li>Manuscript: The Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics</li> <li>Data Card: InstaDeepAI/nucleotide_transformer_downstream_tasks</li> <li>Configs:<ul> <li><code>promoter_all</code></li> <li><code>promoter_tata</code></li> <li><code>promoter_no_tata</code></li> <li><code>enhancers</code></li> <li><code>enhancers_types</code></li> <li><code>splice_sites_all</code></li> <li><code>splice_sites_acceptor</code></li> <li><code>splice_sites_donor</code></li> <li><code>H3</code></li> <li><code>H4</code></li> <li><code>H3K9ac</code></li> <li><code>H3K14ac</code></li> <li><code>H4ac</code></li> <li><code>H3K4me1</code></li> <li><code>H3K4me2</code></li> <li><code>H3K4me3</code></li> <li><code>H3K36me3</code></li> <li><code>H3K79me3</code></li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'InstaDeepAI/nucleotide_transformer_downstream_tasks'</code> <code>config_name</code> <code>str</code> <p>The name of the HF dataset configuration. Affects how the dataset is loaded.</p> <code>'enhancers'</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.GUEClassification","title":"<code>modelgenerator.data.GUEClassification</code>","text":"<p>               Bases: <code>SequenceClassificationDataModule</code></p> <p>Genome Understanding Evaluation benchmarks for DNABERT-2 from the Liu Lab at Northwestern.</p> Note <ul> <li>Manuscript: DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome</li> <li>Data Card: leannmlindsey/GUE</li> <li>Configs:<ul> <li><code>emp_H3</code></li> <li><code>emp_H3K14ac</code></li> <li><code>emp_H3K36me3</code></li> <li><code>emp_H3K4me1</code></li> <li><code>emp_H3K4me2</code></li> <li><code>emp_H3K4me3</code></li> <li><code>emp_H3K79me3</code></li> <li><code>emp_H3K9ac</code></li> <li><code>emp_H4</code></li> <li><code>emp_H4ac</code></li> <li><code>human_tf_0</code></li> <li><code>human_tf_1</code></li> <li><code>human_tf_2</code></li> <li><code>human_tf_3</code></li> <li><code>human_tf_4</code></li> <li><code>mouse_0</code></li> <li><code>mouse_1</code></li> <li><code>mouse_2</code></li> <li><code>mouse_3</code></li> <li><code>mouse_4</code></li> <li><code>prom_300_all</code></li> <li><code>prom_300_notata</code></li> <li><code>prom_300_tata</code></li> <li><code>prom_core_all</code></li> <li><code>prom_core_notata</code></li> <li><code>prom_core_tata</code></li> <li><code>splice_reconstructed</code></li> <li><code>virus_covid</code></li> <li><code>virus_species_40</code></li> <li><code>fungi_species_20</code></li> <li><code>EPI_K562</code></li> <li><code>EPI_HeLa-S3</code></li> <li><code>EPI_NHEK</code></li> <li><code>EPI_IMR90</code></li> <li><code>EPI_HUVEC</code></li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'leannmlindsey/GUE'</code> <code>config_name</code> <code>str</code> <p>The name of the HF dataset configuration. Affects how the dataset is loaded.</p> <code>'emp_H3'</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.ClinvarRetrieve","title":"<code>modelgenerator.data.ClinvarRetrieve</code>","text":"<p>               Bases: <code>ZeroshotClassificationRetrieveDataModule</code></p> <p>ClinVar dataset for genomic variant effect prediction.</p> Note <ul> <li>Manuscript: The Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics</li> <li>Data Card: genbio-ai/Clinvar</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>None</code> <code>test_split_files</code> <code>List[str]</code> <p>Create a split called \"test\" from these files. Not used unless referenced by the name \"test\" in one of the split_name arguments. Also used for <code>mgen predict</code>.</p> <code>['ClinVar_Processed.tsv']</code> <code>reference_file</code> <code>str</code> <p>The file path to the reference file for retrieving sequences</p> <code>'hg38.ml.fa'</code> <code>method</code> <code>str</code> <p>method mode to compute metrics</p> <code>'Distance'</code> <code>window</code> <code>int</code> <p>The number of token taken on either side of the mutation site. The processed sequence length is <code>2 * window + 1</code></p> <code>512</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the parent class. <code>train_split_name=None</code>, <code>valid_split_name=None</code>, and <code>valid_split_size=0</code> are always overridden.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.PromoterExpressionRegression","title":"<code>modelgenerator.data.PromoterExpressionRegression</code>","text":"<p>               Bases: <code>SequenceRegressionDataModule</code></p> <p>Gene expression prediction from promoter sequences from the Regev Lab at the Broad Institute.</p> Note <ul> <li>Manuscript: Deciphering eukaryotic gene-regulatory logic with 100 million random promoters</li> <li>Data Card: genbio-ai/100M-random-promoters</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'genbio-ai/100M-random-promoters'</code> <code>x_col</code> <code>str</code> <p>The name of columns containing the sequences.</p> <code>'sequence'</code> <code>y_col</code> <code>str</code> <p>The name of columns containing the labels.</p> <code>'label'</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the labels.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.PromoterExpressionGeneration","title":"<code>modelgenerator.data.PromoterExpressionGeneration</code>","text":"<p>               Bases: <code>ConditionalDiffusionDataModule</code></p> <p>Promoter generation from gene expression data from the Regev Lab at the Broad Institute.</p> Note <ul> <li>Manuscript: Deciphering eukaryotic gene-regulatory logic with 100 million random promoters</li> <li>Data Card: genbio-ai/100M-random-promoters</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'genbio-ai/100M-random-promoters'</code> <code>x_col</code> <code>str</code> <p>The name of columns containing the sequences.</p> <code>'sequence'</code> <code>y_col</code> <code>str</code> <p>The name of columns containing the labels.</p> <code>'label'</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the labels.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.DependencyMappingDataModule","title":"<code>modelgenerator.data.DependencyMappingDataModule</code>","text":"<p>               Bases: <code>SequencesDataModule</code></p> <p>Data module for doing dependency mapping via in silico mutagenesis on a dataset of sequences.</p> Note <p>Each sample includes a single sequence under key 'sequences' and optionally an 'ids' to track outputs.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> required <code>vocab_file</code> <code>str</code> <p>The path to the file with the vocabulary to mutate.</p> required <code>config_name</code> <code>Optional[str]</code> <p>The name of the HF dataset configuration. Affects how the dataset is loaded.</p> <code>None</code> <code>test_split_name</code> <code>Optional[str]</code> <p>The name of the test split. Also used for <code>mgen predict</code>.</p> <code>None</code> <code>test_split_files</code> <code>Optional[str]</code> <p>Create a split called \"test\" from these files. Not used unless referenced by the name \"test\" in one of the split_name arguments. Also used for <code>mgen predict</code>.</p> <code>None</code> <code>x_col</code> <code>str</code> <p>The name of the column containing the sequences. Defaults to \"sequence\".</p> <code>'sequence'</code> <code>id_col</code> <code>str</code> <p>The name of the column containing the ids. Defaults to \"id\".</p> <code>'id'</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#rna","title":"RNA","text":""},{"location":"api_reference/data/#modelgenerator.data.TranslationEfficiency","title":"<code>modelgenerator.data.TranslationEfficiency</code>","text":"<p>               Bases: <code>SequenceRegressionDataModule</code></p> <p>Translation efficiency prediction benchmarks from the Wang Lab at Princeton.</p> Note <ul> <li>Manuscript: A 5\u2032 UTR language model for decoding untranslated regions of mRNA and function predictions</li> <li>Data Card: genbio-ai/rna-downstream-tasks</li> <li>Configs:<ul> <li><code>translation_efficiency_Muscle</code></li> <li><code>translation_efficiency_HEK</code></li> <li><code>translation_efficiency_pc3</code></li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'genbio-ai/rna-downstream-tasks'</code> <code>config_name</code> <code>str</code> <p>The name of the HF dataset configuration. Affects how the dataset is loaded.</p> <code>'translation_efficiency_Muscle'</code> <code>x_col</code> <p>The name of columns containing the sequences.</p> <code>'sequences'</code> <code>y_col</code> <p>The name of columns containing the labels.</p> <code>'labels'</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the labels.</p> <code>True</code> <code>cv_num_folds</code> <code>int</code> <p>The number of cross-validation folds, disables cv when &lt;= 1.</p> <code>10</code> <code>cv_test_fold_id</code> <code>int</code> <p>The fold id to use for cross-validation evaluation.</p> <code>0</code> <code>cv_enable_val_fold</code> <code>bool</code> <p>Whether to enable a validation fold.</p> <code>True</code> <code>cv_fold_id_col</code> <code>str</code> <p>The column name containing the fold id from a pre-split dataset. Setting to None to enable automatic splitting.</p> <code>'fold_id'</code> <code>valid_split_name</code> <code>str</code> <p>The name of the validation split.</p> <code>None</code> <code>valid_split_size</code> <code>float</code> <p>The size of the validation split. If valid_split_name is None, creates a validation split of this size from the training split.</p> <code>0</code> <code>test_split_name</code> <code>str</code> <p>The name of the test split. Also used for <code>mgen predict</code>.</p> <code>None</code> <code>test_split_size</code> <code>float</code> <p>The size of the test split. If test_split_name is None, creates a test split of this size from the training split.</p> <code>0</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.ExpressionLevel","title":"<code>modelgenerator.data.ExpressionLevel</code>","text":"<p>               Bases: <code>SequenceRegressionDataModule</code></p> <p>Expression level prediction benchmarks from the Wang Lab at Princeton.</p> Note <ul> <li>Manuscript: A 5\u2032 UTR language model for decoding untranslated regions of mRNA and function predictions</li> <li>Data Card: genbio-ai/rna-downstream-tasks</li> <li>Configs:<ul> <li><code>expression_Muscle</code></li> <li><code>expression_HEK</code></li> <li><code>expression_pc3</code></li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'genbio-ai/rna-downstream-tasks'</code> <code>config_name</code> <code>str</code> <p>The name of the HF dataset configuration. Affects how the dataset is loaded.</p> <code>'expression_Muscle'</code> <code>x_col</code> <code>str</code> <p>The name of columns containing the sequences.</p> <code>'sequences'</code> <code>y_col</code> <code>str</code> <p>The name of columns containing the labels.</p> <code>'labels'</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the labels.</p> <code>True</code> <code>cv_num_folds</code> <code>int</code> <p>The number of cross-validation folds, disables cv when &lt;= 1.</p> <code>10</code> <code>cv_test_fold_id</code> <code>int</code> <p>The fold id to use for cross-validation evaluation.</p> <code>0</code> <code>cv_enable_val_fold</code> <code>bool</code> <p>Whether to enable a validation fold.</p> <code>True</code> <code>cv_fold_id_col</code> <code>str</code> <p>The column name containing the fold id from a pre-split dataset. Setting to None to enable automatic splitting.</p> <code>'fold_id'</code> <code>valid_split_name</code> <code>str</code> <p>The name of the validation split.</p> <code>None</code> <code>valid_split_size</code> <code>float</code> <p>The size of the validation split. If valid_split_name is None, creates a validation split of this size from the training split.</p> <code>0</code> <code>test_split_name</code> <code>str</code> <p>The name of the test split. Also used for <code>mgen predict</code>.</p> <code>None</code> <code>test_split_size</code> <code>float</code> <p>The size of the test split. If test_split_name is None, creates a test split of this size from the training split.</p> <code>0</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.TranscriptAbundance","title":"<code>modelgenerator.data.TranscriptAbundance</code>","text":"<p>               Bases: <code>SequenceRegressionDataModule</code></p> <p>Transcript abundance prediction benchmarks from the Wang Lab at Princeton.</p> Note <ul> <li>Manuscript: A 5\u2032 UTR language model for decoding untranslated regions of mRNA and function predictions</li> <li>Data Card: genbio-ai/rna-downstream-tasks</li> <li>Configs:<ul> <li><code>transcript_abundance_athaliana</code></li> <li><code>transcript_abundance_dmelanogaster</code></li> <li><code>transcript_abundance_ecoli</code></li> <li><code>transcript_abundance_hsapiens</code></li> <li><code>transcript_abundance_hvolcanii</code></li> <li><code>transcript_abundance_ppastoris</code></li> <li><code>transcript_abundance_scerevisiae</code></li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'genbio-ai/rna-downstream-tasks'</code> <code>config_name</code> <code>str</code> <p>The name of the HF dataset configuration. Affects how the dataset is loaded.</p> <code>'transcript_abundance_athaliana'</code> <code>x_col</code> <code>str</code> <p>The name of columns containing the sequences.</p> <code>'sequences'</code> <code>y_col</code> <code>str</code> <p>The name of columns containing the labels.</p> <code>'labels'</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the labels.</p> <code>True</code> <code>cv_num_folds</code> <code>int</code> <p>The number of cross-validation folds, disables cv when &lt;= 1.</p> <code>5</code> <code>cv_test_fold_id</code> <code>int</code> <p>The fold id to use for cross-validation evaluation.</p> <code>0</code> <code>cv_enable_val_fold</code> <code>bool</code> <p>Whether to enable a validation fold.</p> <code>True</code> <code>cv_fold_id_col</code> <code>str</code> <p>The column name containing the fold id from a pre-split dataset. Setting to None to enable automatic splitting.</p> <code>'fold_id'</code> <code>valid_split_name</code> <code>str</code> <p>The name of the validation split.</p> <code>None</code> <code>valid_split_size</code> <code>float</code> <p>The size of the validation split. If valid_split_name is None, creates a validation split of this size from the training split.</p> <code>0</code> <code>test_split_name</code> <code>str</code> <p>The name of the test split. Also used for <code>mgen predict</code>.</p> <code>None</code> <code>test_split_size</code> <code>float</code> <p>The size of the test split. If test_split_name is None, creates a test split of this size from the training split.</p> <code>0</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.ProteinAbundance","title":"<code>modelgenerator.data.ProteinAbundance</code>","text":"<p>               Bases: <code>SequenceRegressionDataModule</code></p> <p>Protein abundance prediction benchmarks from the Wang Lab at Princeton.</p> Note <ul> <li>Manuscript: A 5\u2032 UTR language model for decoding untranslated regions of mRNA and function predictions</li> <li>Data Card: genbio-ai/rna-downstream-tasks</li> <li>Configs:<ul> <li><code>protein_abundance_athaliana</code></li> <li><code>protein_abundance_dmelanogaster</code></li> <li><code>protein_abundance_ecoli</code></li> <li><code>protein_abundance_hsapiens</code></li> <li><code>protein_abundance_scerevisiae</code></li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'genbio-ai/rna-downstream-tasks'</code> <code>config_name</code> <code>str</code> <p>The name of the HF dataset configuration. Affects how the dataset is loaded.</p> <code>'protein_abundance_athaliana'</code> <code>x_col</code> <code>str</code> <p>The name of columns containing the sequences.</p> <code>'sequences'</code> <code>y_col</code> <code>str</code> <p>The name of columns containing the labels.</p> <code>'labels'</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the labels.</p> <code>True</code> <code>cv_num_folds</code> <code>int</code> <p>The number of cross-validation folds, disables cv when &lt;= 1.</p> <code>5</code> <code>cv_test_fold_id</code> <code>int</code> <p>The fold id to use for cross-validation evaluation.</p> <code>0</code> <code>cv_enable_val_fold</code> <code>bool</code> <p>Whether to enable a validation fold.</p> <code>True</code> <code>cv_fold_id_col</code> <code>str</code> <p>The column name containing the fold id from a pre-split dataset. Setting to None to enable automatic splitting.</p> <code>'fold_id'</code> <code>valid_split_name</code> <code>str</code> <p>The name of the validation split.</p> <code>None</code> <code>valid_split_size</code> <code>float</code> <p>The size of the validation split. If valid_split_name is None, creates a validation split of this size from the training split.</p> <code>0</code> <code>test_split_name</code> <code>str</code> <p>The name of the test split. Also used for <code>mgen predict</code>.</p> <code>None</code> <code>test_split_size</code> <code>float</code> <p>The size of the test split. If test_split_name is None, creates a test split of this size from the training split.</p> <code>0</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.NcrnaFamilyClassification","title":"<code>modelgenerator.data.NcrnaFamilyClassification</code>","text":"<p>               Bases: <code>SequenceClassificationDataModule</code></p> <p>Non-coding RNA family classification benchmarks from DPTechnology.</p> Note <ul> <li>Manuscript: UNI-RNA: UNIVERSAL PRE-TRAINED MODELS REVOLUTIONIZE RNA RESEARCH</li> <li>Data Card: genbio-ai/rna-downstream-tasks</li> <li>Configs:<ul> <li><code>ncrna_family_bnoise0</code></li> <li><code>ncrna_family_bnoise200</code></li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'genbio-ai/rna-downstream-tasks'</code> <code>config_name</code> <code>str</code> <p>The name of the HF dataset configuration. Affects how the dataset is loaded.</p> <code>'ncrna_family_bnoise0'</code> <code>x_col</code> <code>str</code> <p>The name of the column containing the sequences.</p> <code>'sequences'</code> <code>y_col</code> <code>str</code> <p>The name of the column(s) containing the labels.</p> <code>'labels'</code> <code>train_split_name</code> <code>str</code> <p>The name of the training split.</p> <code>'train'</code> <code>valid_split_name</code> <code>str</code> <p>The name of the validation split.</p> <code>'validation'</code> <code>test_split_name</code> <code>str</code> <p>The name of the test split. Also used for <code>mgen predict</code>.</p> <code>'test'</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.SpliceSitePrediction","title":"<code>modelgenerator.data.SpliceSitePrediction</code>","text":"<p>               Bases: <code>SequenceClassificationDataModule</code></p> <p>Splice site prediction benchmarks from the Thompson Lab at University of Strasbourg.</p> Note <ul> <li>Manuscript: Spliceator: multi-species splice site prediction using convolutional neural networks</li> <li>Data Card: genbio-ai/rna-downstream-tasks</li> <li>Configs:<ul> <li><code>splice_site_acceptor</code></li> <li><code>splice_site_donor</code></li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'genbio-ai/rna-downstream-tasks'</code> <code>config_name</code> <code>str</code> <p>The name of the HF dataset configuration. Affects how the dataset is loaded.</p> <code>'splice_site_acceptor'</code> <code>x_col</code> <code>str</code> <p>The name of the column containing the sequences.</p> <code>'sequences'</code> <code>y_col</code> <code>str</code> <p>The name of the column(s) containing the labels.</p> <code>'labels'</code> <code>train_split_name</code> <code>str</code> <p>The name of the training split.</p> <code>'train'</code> <code>valid_split_name</code> <code>str</code> <p>The name of the validation split.</p> <code>'validation'</code> <code>test_split_name</code> <code>str</code> <p>The name of the test split. Also used for <code>mgen predict</code>.</p> <code>'test_danio'</code> <code>batch_size</code> <code>int</code> <p>The batch size.</p> <code>16</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.ModificationSitePrediction","title":"<code>modelgenerator.data.ModificationSitePrediction</code>","text":"<p>               Bases: <code>SequenceClassificationDataModule</code></p> <p>Modification site prediction benchmarks from the Meng Lab at the University of Liverpool.</p> Note <ul> <li>Manuscript: Attention-based multi-label neural networks for integrated prediction and interpretation of twelve widely occurring RNA modifications</li> <li>Data Card: genbio-ai/rna-downstream-tasks</li> <li>Configs:<ul> <li><code>modification_site</code></li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'genbio-ai/rna-downstream-tasks'</code> <code>config_name</code> <code>str</code> <p>The name of the HF dataset configuration. Affects how the dataset is loaded.</p> <code>'modification_site'</code> <code>x_col</code> <code>str</code> <p>The name of the column containing the sequences.</p> <code>'sequences'</code> <code>y_col</code> <code>List[str]</code> <p>The name of the column(s) containing the labels.</p> <code>[f'labels_{i}' for i in (range(12))]</code> <code>train_split_name</code> <code>str</code> <p>The name of the training split.</p> <code>'train'</code> <code>valid_split_name</code> <code>str</code> <p>The name of the validation split.</p> <code>'validation'</code> <code>test_split_name</code> <code>str</code> <p>The name of the test split. Also used for <code>mgen predict</code>.</p> <code>'test'</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.RNAMeanRibosomeLoadDataModule","title":"<code>modelgenerator.data.RNAMeanRibosomeLoadDataModule</code>","text":"<p>               Bases: <code>SequenceRegressionDataModule</code></p> <p>Data module for the mean ribosome load dataset.</p> Note <ul> <li>Manuscript: Human 5\u2032 UTR design and variant effect prediction from a massively parallel translation assay</li> <li>Data Card: genbio-ai/rna-downstream-tasks</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'genbio-ai/rna-downstream-tasks'</code> <code>config_name</code> <code>str</code> <p>The name of the HF dataset configuration. Affects how the dataset is loaded.</p> <code>'mean_ribosome_load'</code> <code>train_split_name</code> <code>str</code> <p>The name of the training split.</p> <code>'train'</code> <code>valid_split_name</code> <code>str</code> <p>The name of the validation split.</p> <code>'validation'</code> <code>test_split_name</code> <code>str</code> <p>The name of the test split. Also used for <code>mgen predict</code>.</p> <code>'test'</code> <code>x_col</code> <code>str</code> <p>The name of columns containing the sequences.</p> <code>'utr'</code> <code>y_col</code> <code>str</code> <p>The name of columns containing the labels.</p> <code>'rl'</code> <code>extra_cols</code> <code>List[str]</code> <p>Additional columns to include in the dataset.</p> <code>None</code> <code>extra_col_aliases</code> <code>List[str]</code> <p>The name of the columns to use as the alias for the extra columns.</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the labels.</p> <code>False</code> <code>generate_uid</code> <code>bool</code> <p>Whether to generate a unique ID for each sample.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#protein","title":"Protein","text":""},{"location":"api_reference/data/#modelgenerator.data.ContactPredictionBinary","title":"<code>modelgenerator.data.ContactPredictionBinary</code>","text":"<p>               Bases: <code>TokenClassificationDataModule</code></p> <p>Protein contact prediction benchmarks from BioMap.</p> Note <ul> <li>Manuscript: xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein</li> <li>Data Card: proteinglm/contact_prediction_binary</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'proteinglm/contact_prediction_binary'</code> <code>pairwise</code> <code>bool</code> <p>Whether the labels are pairwise.</p> <code>True</code> <code>x_col</code> <code>str</code> <p>The name of the column containing the sequences.</p> <code>'seq'</code> <code>y_col</code> <code>str</code> <p>The name of the column containing the labels.</p> <code>'label'</code> <code>batch_size</code> <code>int</code> <p>The batch size.</p> <code>1</code> <code>max_context_length</code> <code>int</code> <p>Maximum context length for the input sequences.</p> <code>12800</code> <code>msa_random_seed</code> <code>Optional[int]</code> <p>Random seed for MSA generation.</p> <code>None</code> <code>is_rag_dataset</code> <code>bool</code> <p>Whether the dataset is a RAG dataset for AIDO.Protein-RAG.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.SspQ3","title":"<code>modelgenerator.data.SspQ3</code>","text":"<p>               Bases: <code>TokenClassificationDataModule</code></p> <p>Protein secondary structure prediction benchmarks from BioMap.</p> Note <ul> <li>Manuscript: xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein</li> <li>Data Card: proteinglm/ssp_q3</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'proteinglm/ssp_q3'</code> <code>pairwise</code> <code>bool</code> <p>Whether the labels are pairwise.</p> <code>False</code> <code>x_col</code> <code>str</code> <p>The name of the column containing the sequences.</p> <code>'seq'</code> <code>y_col</code> <code>str</code> <p>The name of the column containing the labels.</p> <code>'label'</code> <code>batch_size</code> <code>int</code> <p>The batch size.</p> <code>1</code> <code>max_context_length</code> <code>int</code> <p>Maximum context length for the input sequences.</p> <code>12800</code> <code>msa_random_seed</code> <code>Optional[int]</code> <p>Random seed for MSA generation.</p> <code>None</code> <code>is_rag_dataset</code> <code>bool</code> <p>Whether the dataset is a RAG dataset for AIDO.Protein-RAG.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.FoldPrediction","title":"<code>modelgenerator.data.FoldPrediction</code>","text":"<p>               Bases: <code>SequenceClassificationDataModule</code></p> <p>Protein fold prediction benchmarks from BioMap.</p> Note <ul> <li>Manuscript: xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein</li> <li>Data Card: proteinglm/fold_prediction</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'proteinglm/fold_prediction'</code> <code>x_col</code> <code>str</code> <p>The name of the column containing the sequences.</p> <code>'seq'</code> <code>y_col</code> <code>str</code> <p>The name of the column(s) containing the labels.</p> <code>'label'</code> <code>max_context_length</code> <code>int</code> <p>Maximum context length for the input sequences.</p> <code>12800</code> <code>msa_random_seed</code> <code>Optional[int]</code> <p>Random seed for MSA generation.</p> <code>None</code> <code>is_rag_dataset</code> <code>bool</code> <p>Whether the dataset is a RAG dataset for AIDO.Protein-RAG.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.LocalizationPrediction","title":"<code>modelgenerator.data.LocalizationPrediction</code>","text":"<p>               Bases: <code>SequenceClassificationDataModule</code></p> <p>Protein localization prediction benchmarks from BioMap.</p> Note <ul> <li>Manuscript: xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein</li> <li>Data Card: proteinglm/localization_prediction</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'proteinglm/localization_prediction'</code> <code>x_col</code> <code>str</code> <p>The name of the column containing the sequences.</p> <code>'seq'</code> <code>y_col</code> <code>str</code> <p>The name of the column(s) containing the labels.</p> <code>'label'</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.MetalIonBinding","title":"<code>modelgenerator.data.MetalIonBinding</code>","text":"<p>               Bases: <code>SequenceClassificationDataModule</code></p> <p>Metal ion binding prediction benchmarks from BioMap.</p> Note <ul> <li>Manuscript: xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein</li> <li>Data Card: proteinglm/metal_ion_binding</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'proteinglm/metal_ion_binding'</code> <code>x_col</code> <code>str</code> <p>The name of the column containing the sequences.</p> <code>'seq'</code> <code>y_col</code> <code>str</code> <p>The name of the column(s) containing the labels.</p> <code>'label'</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.SolubilityPrediction","title":"<code>modelgenerator.data.SolubilityPrediction</code>","text":"<p>               Bases: <code>SequenceClassificationDataModule</code></p> <p>Protein solubility prediction benchmarks from BioMap.</p> Note <ul> <li>Manuscript: xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein</li> <li>Data Card: proteinglm/solubility_prediction</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'proteinglm/solubility_prediction'</code> <code>x_col</code> <code>str</code> <p>The name of the column containing the sequences.</p> <code>'seq'</code> <code>y_col</code> <code>str</code> <p>The name of the column(s) containing the labels.</p> <code>'label'</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.AntibioticResistance","title":"<code>modelgenerator.data.AntibioticResistance</code>","text":"<p>               Bases: <code>SequenceClassificationDataModule</code></p> <p>Antibiotic resistance prediction benchmarks from BioMap.</p> Note <ul> <li>Manuscript: xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein</li> <li>Data Card: proteinglm/antibiotic_resistance</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'proteinglm/antibiotic_resistance'</code> <code>x_col</code> <code>str</code> <p>The name of the column containing the sequences.</p> <code>'seq'</code> <code>y_col</code> <code>str</code> <p>The name of the column(s) containing the labels.</p> <code>'label'</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.CloningClf","title":"<code>modelgenerator.data.CloningClf</code>","text":"<p>               Bases: <code>SequenceClassificationDataModule</code></p> <p>Cloning classification prediction benchmarks from BioMap.</p> Note <ul> <li>Manuscript: xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein</li> <li>Data Card: proteinglm/cloning_clf</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'proteinglm/cloning_clf'</code> <code>x_col</code> <code>str</code> <p>The name of the column containing the sequences.</p> <code>'seq'</code> <code>y_col</code> <code>str</code> <p>The name of the column(s) containing the labels.</p> <code>'label'</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.MaterialProduction","title":"<code>modelgenerator.data.MaterialProduction</code>","text":"<p>               Bases: <code>SequenceClassificationDataModule</code></p> <p>Material production prediction benchmarks from BioMap.</p> Note <ul> <li>Manuscript: xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein</li> <li>Data Card: proteinglm/material_production</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'proteinglm/material_production'</code> <code>x_col</code> <code>str</code> <p>The name of the column containing the sequences.</p> <code>'seq'</code> <code>y_col</code> <code>str</code> <p>The name of the column(s) containing the labels.</p> <code>'label'</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.TcrPmhcAffinity","title":"<code>modelgenerator.data.TcrPmhcAffinity</code>","text":"<p>               Bases: <code>SequenceClassificationDataModule</code></p> <p>TCR-pMHC affinity prediction benchmarks from BioMap.</p> Note <ul> <li>Manuscript: xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein</li> <li>Data Card: proteinglm/tcr_pmhc_affinity</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'proteinglm/tcr_pmhc_affinity'</code> <code>x_col</code> <code>str</code> <p>The name of the column containing the sequences.</p> <code>'seq'</code> <code>y_col</code> <code>str</code> <p>The name of the column(s) containing the labels.</p> <code>'label'</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.PeptideHlaMhcAffinity","title":"<code>modelgenerator.data.PeptideHlaMhcAffinity</code>","text":"<p>               Bases: <code>SequenceClassificationDataModule</code></p> <p>Peptide-HLA-MHC affinity prediction benchmarks from BioMap. Note:     - Manuscript: xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein     - Data Card: proteinglm/peptide_HLA_MHC_affinity</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'proteinglm/peptide_HLA_MHC_affinity'</code> <code>x_col</code> <code>str</code> <p>The name of the column containing the sequences.</p> <code>'seq'</code> <code>y_col</code> <code>str</code> <p>The name of the column(s) containing the labels.</p> <code>'label'</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.TemperatureStability","title":"<code>modelgenerator.data.TemperatureStability</code>","text":"<p>               Bases: <code>SequenceClassificationDataModule</code></p> <p>Temperature stability prediction benchmarks from BioMap.</p> Note <ul> <li>Manuscript: xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein</li> <li>Data Card: proteinglm/temperature_stability</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'proteinglm/temperature_stability'</code> <code>x_col</code> <code>str</code> <p>The name of the column containing the sequences.</p> <code>'seq'</code> <code>y_col</code> <code>str</code> <p>The name of the column(s) containing the labels.</p> <code>'label'</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.FluorescencePrediction","title":"<code>modelgenerator.data.FluorescencePrediction</code>","text":"<p>               Bases: <code>SequenceRegressionDataModule</code></p> <p>Fluorescence prediction benchmarks from BioMap.</p> Note <ul> <li>Manuscript: xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein</li> <li>Data Card: proteinglm/fluorescence_prediction</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'proteinglm/fluorescence_prediction'</code> <code>x_col</code> <code>str</code> <p>The name of columns containing the sequences.</p> <code>'seq'</code> <code>y_col</code> <code>str</code> <p>The name of columns containing the labels.</p> <code>'label'</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the labels.</p> <code>True</code> <code>max_context_length</code> <code>int</code> <p>Maximum context length for the input sequences.</p> <code>12800</code> <code>msa_random_seed</code> <code>Optional[int]</code> <p>Random seed for MSA generation.</p> <code>None</code> <code>is_rag_dataset</code> <code>bool</code> <p>Whether the dataset is a RAG dataset for AIDO.Protein-RAG.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.FitnessPrediction","title":"<code>modelgenerator.data.FitnessPrediction</code>","text":"<p>               Bases: <code>SequenceRegressionDataModule</code></p> <p>Fitness prediction benchmarks from BioMap.</p> Note <ul> <li>Manuscript: xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein</li> <li>Data Card: proteinglm/fitness_prediction</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'proteinglm/fitness_prediction'</code> <code>x_col</code> <code>str</code> <p>The name of columns containing the sequences.</p> <code>'seq'</code> <code>y_col</code> <code>str</code> <p>The name of columns containing the labels.</p> <code>'label'</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the labels.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.StabilityPrediction","title":"<code>modelgenerator.data.StabilityPrediction</code>","text":"<p>               Bases: <code>SequenceRegressionDataModule</code></p> <p>Stability prediction benchmarks from BioMap.</p> Note <ul> <li>Manuscript: xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein</li> <li>Data Card: proteinglm/stability_prediction</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'proteinglm/stability_prediction'</code> <code>x_col</code> <code>str</code> <p>The name of columns containing the sequences.</p> <code>'seq'</code> <code>y_col</code> <code>str</code> <p>The name of columns containing the labels.</p> <code>'label'</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the labels.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.EnzymeCatalyticEfficiencyPrediction","title":"<code>modelgenerator.data.EnzymeCatalyticEfficiencyPrediction</code>","text":"<p>               Bases: <code>SequenceRegressionDataModule</code></p> <p>Enzyme catalytic efficiency prediction benchmarks from BioMap.</p> Note <ul> <li>Manuscript: xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein</li> <li>Data Card: proteinglm/enzyme_catalytic_efficiency</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'proteinglm/enzyme_catalytic_efficiency'</code> <code>x_col</code> <code>str</code> <p>The name of columns containing the sequences.</p> <code>'seq'</code> <code>y_col</code> <code>str</code> <p>The name of columns containing the labels.</p> <code>'label'</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the labels.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.OptimalTemperaturePrediction","title":"<code>modelgenerator.data.OptimalTemperaturePrediction</code>","text":"<p>               Bases: <code>SequenceRegressionDataModule</code></p> <p>Optimal temperature prediction benchmarks from BioMap.</p> Note <ul> <li>Manuscript: xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein</li> <li>Data Card: proteinglm/optimal_temperature</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'proteinglm/optimal_temperature'</code> <code>x_col</code> <code>str</code> <p>The name of columns containing the sequences.</p> <code>'seq'</code> <code>y_col</code> <code>str</code> <p>The name of columns containing the labels.</p> <code>'label'</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the labels.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.OptimalPhPrediction","title":"<code>modelgenerator.data.OptimalPhPrediction</code>","text":"<p>               Bases: <code>SequenceRegressionDataModule</code></p> <p>Optimal pH prediction benchmarks from BioMap.</p> Note <ul> <li>Manuscript: xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein</li> <li>Data Card: proteinglm/optimal_ph</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'proteinglm/optimal_ph'</code> <code>x_col</code> <code>str</code> <p>The name of columns containing the sequences.</p> <code>'seq'</code> <code>y_col</code> <code>str</code> <p>The name of columns containing the labels.</p> <code>'label'</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the labels.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.DMSFitnessPrediction","title":"<code>modelgenerator.data.DMSFitnessPrediction</code>","text":"<p>               Bases: <code>SequenceRegressionDataModule</code></p> <p>Deep mutational scanning (DMS) fitness prediction benchmarks from the Gal Lab at Oxford and the Marks Lab at Harvard.</p> Note <ul> <li>Manuscript: ProteinGym: Large-Scale Benchmarks for Protein Fitness Prediction and Design</li> <li>Data Card: genbio-ai/ProteinGYM-DMS</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'genbio-ai/ProteinGYM-DMS'</code> <code>train_split_files</code> <code>list[str]</code> <p>Create a split called \"train\" from these files. Not used unless referenced by the name \"train\" in one of the split_name arguments.</p> <code>['indels/B1LPA6_ECOSM_Russ_2020_indels.tsv']</code> <code>x_col</code> <code>str</code> <p>The name of columns containing the sequences.</p> <code>'sequences'</code> <code>y_col</code> <code>str</code> <p>The name of columns containing the labels.</p> <code>'labels'</code> <code>cv_num_folds</code> <code>int</code> <p>The number of cross-validation folds, disables cv when &lt;= 1.</p> <code>5</code> <code>cv_test_fold_id</code> <code>int</code> <p>The fold id to use for cross-validation evaluation.</p> <code>0</code> <code>cv_enable_val_fold</code> <code>bool</code> <p>Whether to enable a validation fold.</p> <code>True</code> <code>cv_replace_val_fold_as_test_fold</code> <code>bool</code> <p>Replace validation fold with test fold. Only used when cv_enable_val_fold is False.</p> <code>False</code> <code>cv_fold_id_col</code> <code>str</code> <p>The column name containing the fold id from a pre-split dataset. Setting to None to enable automatic splitting.</p> <code>'fold_id'</code> <code>cv_val_offset</code> <code>int</code> <p>The offset applied to cv_test_fold_id to determine val_fold_id.</p> <code>-1</code> <code>valid_split_name</code> <code>str</code> <p>The name of the validation split.</p> <code>None</code> <code>valid_split_size</code> <code>float</code> <p>The size of the validation split. If valid_split_name is None, creates a validation split of this size from the training split.</p> <code>0</code> <code>test_split_name</code> <code>str</code> <p>The name of the test split. Also used for <code>mgen predict</code>.</p> <code>None</code> <code>test_split_size</code> <code>float</code> <p>The size of the test split. If test_split_name is None, creates a test split of this size from the training split.</p> <code>0</code> <code>max_context_length</code> <code>int</code> <p>Maximum context length for the input sequences.</p> <code>12800</code> <code>msa_random_seed</code> <code>Optional[int]</code> <p>Random seed for MSA generation.</p> <code>None</code> <code>is_rag_dataset</code> <code>bool</code> <p>Whether the dataset is a RAG dataset for AIDO.Protein-RAG.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#structure","title":"Structure","text":""},{"location":"api_reference/data/#modelgenerator.data.ContactPredictionBinary","title":"<code>modelgenerator.data.ContactPredictionBinary</code>","text":"<p>               Bases: <code>TokenClassificationDataModule</code></p> <p>Protein contact prediction benchmarks from BioMap.</p> Note <ul> <li>Manuscript: xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein</li> <li>Data Card: proteinglm/contact_prediction_binary</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'proteinglm/contact_prediction_binary'</code> <code>pairwise</code> <code>bool</code> <p>Whether the labels are pairwise.</p> <code>True</code> <code>x_col</code> <code>str</code> <p>The name of the column containing the sequences.</p> <code>'seq'</code> <code>y_col</code> <code>str</code> <p>The name of the column containing the labels.</p> <code>'label'</code> <code>batch_size</code> <code>int</code> <p>The batch size.</p> <code>1</code> <code>max_context_length</code> <code>int</code> <p>Maximum context length for the input sequences.</p> <code>12800</code> <code>msa_random_seed</code> <code>Optional[int]</code> <p>Random seed for MSA generation.</p> <code>None</code> <code>is_rag_dataset</code> <code>bool</code> <p>Whether the dataset is a RAG dataset for AIDO.Protein-RAG.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.SspQ3","title":"<code>modelgenerator.data.SspQ3</code>","text":"<p>               Bases: <code>TokenClassificationDataModule</code></p> <p>Protein secondary structure prediction benchmarks from BioMap.</p> Note <ul> <li>Manuscript: xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein</li> <li>Data Card: proteinglm/ssp_q3</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'proteinglm/ssp_q3'</code> <code>pairwise</code> <code>bool</code> <p>Whether the labels are pairwise.</p> <code>False</code> <code>x_col</code> <code>str</code> <p>The name of the column containing the sequences.</p> <code>'seq'</code> <code>y_col</code> <code>str</code> <p>The name of the column containing the labels.</p> <code>'label'</code> <code>batch_size</code> <code>int</code> <p>The batch size.</p> <code>1</code> <code>max_context_length</code> <code>int</code> <p>Maximum context length for the input sequences.</p> <code>12800</code> <code>msa_random_seed</code> <code>Optional[int]</code> <p>Random seed for MSA generation.</p> <code>None</code> <code>is_rag_dataset</code> <code>bool</code> <p>Whether the dataset is a RAG dataset for AIDO.Protein-RAG.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.FoldPrediction","title":"<code>modelgenerator.data.FoldPrediction</code>","text":"<p>               Bases: <code>SequenceClassificationDataModule</code></p> <p>Protein fold prediction benchmarks from BioMap.</p> Note <ul> <li>Manuscript: xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein</li> <li>Data Card: proteinglm/fold_prediction</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'proteinglm/fold_prediction'</code> <code>x_col</code> <code>str</code> <p>The name of the column containing the sequences.</p> <code>'seq'</code> <code>y_col</code> <code>str</code> <p>The name of the column(s) containing the labels.</p> <code>'label'</code> <code>max_context_length</code> <code>int</code> <p>Maximum context length for the input sequences.</p> <code>12800</code> <code>msa_random_seed</code> <code>Optional[int]</code> <p>Random seed for MSA generation.</p> <code>None</code> <code>is_rag_dataset</code> <code>bool</code> <p>Whether the dataset is a RAG dataset for AIDO.Protein-RAG.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.FluorescencePrediction","title":"<code>modelgenerator.data.FluorescencePrediction</code>","text":"<p>               Bases: <code>SequenceRegressionDataModule</code></p> <p>Fluorescence prediction benchmarks from BioMap.</p> Note <ul> <li>Manuscript: xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein</li> <li>Data Card: proteinglm/fluorescence_prediction</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'proteinglm/fluorescence_prediction'</code> <code>x_col</code> <code>str</code> <p>The name of columns containing the sequences.</p> <code>'seq'</code> <code>y_col</code> <code>str</code> <p>The name of columns containing the labels.</p> <code>'label'</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the labels.</p> <code>True</code> <code>max_context_length</code> <code>int</code> <p>Maximum context length for the input sequences.</p> <code>12800</code> <code>msa_random_seed</code> <code>Optional[int]</code> <p>Random seed for MSA generation.</p> <code>None</code> <code>is_rag_dataset</code> <code>bool</code> <p>Whether the dataset is a RAG dataset for AIDO.Protein-RAG.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.DMSFitnessPrediction","title":"<code>modelgenerator.data.DMSFitnessPrediction</code>","text":"<p>               Bases: <code>SequenceRegressionDataModule</code></p> <p>Deep mutational scanning (DMS) fitness prediction benchmarks from the Gal Lab at Oxford and the Marks Lab at Harvard.</p> Note <ul> <li>Manuscript: ProteinGym: Large-Scale Benchmarks for Protein Fitness Prediction and Design</li> <li>Data Card: genbio-ai/ProteinGYM-DMS</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'genbio-ai/ProteinGYM-DMS'</code> <code>train_split_files</code> <code>list[str]</code> <p>Create a split called \"train\" from these files. Not used unless referenced by the name \"train\" in one of the split_name arguments.</p> <code>['indels/B1LPA6_ECOSM_Russ_2020_indels.tsv']</code> <code>x_col</code> <code>str</code> <p>The name of columns containing the sequences.</p> <code>'sequences'</code> <code>y_col</code> <code>str</code> <p>The name of columns containing the labels.</p> <code>'labels'</code> <code>cv_num_folds</code> <code>int</code> <p>The number of cross-validation folds, disables cv when &lt;= 1.</p> <code>5</code> <code>cv_test_fold_id</code> <code>int</code> <p>The fold id to use for cross-validation evaluation.</p> <code>0</code> <code>cv_enable_val_fold</code> <code>bool</code> <p>Whether to enable a validation fold.</p> <code>True</code> <code>cv_replace_val_fold_as_test_fold</code> <code>bool</code> <p>Replace validation fold with test fold. Only used when cv_enable_val_fold is False.</p> <code>False</code> <code>cv_fold_id_col</code> <code>str</code> <p>The column name containing the fold id from a pre-split dataset. Setting to None to enable automatic splitting.</p> <code>'fold_id'</code> <code>cv_val_offset</code> <code>int</code> <p>The offset applied to cv_test_fold_id to determine val_fold_id.</p> <code>-1</code> <code>valid_split_name</code> <code>str</code> <p>The name of the validation split.</p> <code>None</code> <code>valid_split_size</code> <code>float</code> <p>The size of the validation split. If valid_split_name is None, creates a validation split of this size from the training split.</p> <code>0</code> <code>test_split_name</code> <code>str</code> <p>The name of the test split. Also used for <code>mgen predict</code>.</p> <code>None</code> <code>test_split_size</code> <code>float</code> <p>The size of the test split. If test_split_name is None, creates a test split of this size from the training split.</p> <code>0</code> <code>max_context_length</code> <code>int</code> <p>Maximum context length for the input sequences.</p> <code>12800</code> <code>msa_random_seed</code> <code>Optional[int]</code> <p>Random seed for MSA generation.</p> <code>None</code> <code>is_rag_dataset</code> <code>bool</code> <p>Whether the dataset is a RAG dataset for AIDO.Protein-RAG.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.StructureTokenDataModule","title":"<code>modelgenerator.data.StructureTokenDataModule</code>","text":"<p>               Bases: <code>DataInterface</code>, <code>HFDatasetLoaderMixin</code></p> <p>Test only data module for structure token predictors.</p> <p>This data module is specifically designed for handling datasets uses amino acid sequences as input and structure tokens as labels.</p> Note <p>This module only supports testing and ignores training and validation splits. It assumes test split files contain sequences and optionally their structural token labels. If structural token labels are not provided, dummy labels are created.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> required <code>config_name</code> <code>Optional[str]</code> <p>The name of the HF dataset configuration. Affects how the dataset is loaded.</p> <code>None</code> <code>test_split_files</code> <code>Optional[List[str]]</code> <p>Create a split called \"test\" from these files. Not used unless referenced by the name \"test\" in one of the split_name arguments. Also used for <code>mgen predict</code>.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>The batch size.</p> <code>1</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the parent class, in which training and validation split settings are overridden so that only the test split is loaded.</p> <code>{}</code>"},{"location":"api_reference/data/#cell","title":"Cell","text":""},{"location":"api_reference/data/#modelgenerator.data.CellClassificationDataModule","title":"<code>modelgenerator.data.CellClassificationDataModule</code>","text":"<p>               Bases: <code>DataInterface</code></p> <p>Data module for cell classification.</p> Note <p>Each sample includes a feature vector (one of the rows in ) and a single class label (one of the columns in ) <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> required <code>backbone_class_path</code> <code>Optional[str]</code> <p>Class path of the backbone model.</p> <code>None</code> <code>filter_columns</code> <code>Optional[list[str]]</code> <p>The columns of  we want to use. Defaults to None, in which case all columns are used. <code>None</code> <code>rename_columns</code> <code>Optional[list[str]]</code> <p>New name of columns. Defaults to None, in which case columns are not renamed. Does nothing if filter_colums is None.</p> <code>None</code> <code>config_name</code> <code>Optional[str]</code> <p>The name of the HF dataset configuration. Affects how the dataset is loaded.</p> <code>None</code> <code>train_split_name</code> <code>Optional[str]</code> <p>The name of the training split.</p> <code>'train'</code> <code>test_split_name</code> <code>Optional[str]</code> <p>The name of the test split. Also used for <code>mgen predict</code>.</p> <code>'test'</code> <code>valid_split_name</code> <code>Optional[str]</code> <p>The name of the validation split.</p> <code>None</code> <code>train_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"train\" from these files. Not used unless referenced by the name \"train\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"test\" from these files. Not used unless referenced by the name \"test\" in one of the split_name arguments. Also used for <code>mgen predict</code>.</p> <code>None</code> <code>valid_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"valid\" from these files. Not used unless referenced by the name \"valid\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_size</code> <code>float</code> <p>The size of the test split. If test_split_name is None, creates a test split of this size from the training split.</p> <code>0.2</code> <code>valid_split_size</code> <code>float</code> <p>The size of the validation split. If valid_split_name is None, creates a validation split of this size from the training split.</p> <code>0.1</code> <code>random_seed</code> <code>int</code> <p>The random seed to use for splitting the data.</p> <code>42</code> <code>extra_reader_kwargs</code> <code>Optional[dict]</code> <p>Extra kwargs for dataset readers.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>The batch size.</p> <code>128</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data.</p> <code>True</code> <code>sampler</code> <code>Optional[Sampler]</code> <p>The sampler to use.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>The number of workers to use for data loading.</p> <code>0</code> <code>collate_fn</code> <code>Optional[callable]</code> <p>The function to use for collating data.</p> <code>None</code> <code>pin_memory</code> <code>bool</code> <p>Whether to pin memory.</p> <code>True</code> <code>persistent_workers</code> <code>bool</code> <p>Whether to use persistent workers.</p> <code>False</code> <code>cv_num_folds</code> <code>int</code> <p>The number of cross-validation folds, disables cv when &lt;= 1.</p> <code>1</code> <code>cv_test_fold_id</code> <code>int</code> <p>The fold id to use for cross-validation evaluation.</p> <code>0</code> <code>cv_enable_val_fold</code> <code>bool</code> <p>Whether to enable a validation fold.</p> <code>True</code> <code>cv_replace_val_fold_as_test_fold</code> <code>bool</code> <p>Replace validation fold with test fold. Only used when cv_enable_val_fold is False.</p> <code>False</code> <code>cv_fold_id_col</code> <code>Optional[str]</code> <p>The column name containing the fold id from a pre-split dataset. Setting to None to enable automatic splitting.</p> <code>None</code> <code>cv_val_offset</code> <code>int</code> <p>The offset applied to cv_test_fold_id to determine val_fold_id.</p> <code>1</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.CellClassificationLargeDataModule","title":"<code>modelgenerator.data.CellClassificationLargeDataModule</code>","text":"<p>               Bases: <code>DataInterface</code></p> <p>Data module for cell classification. This class handles large dataset and is implemented based on TileDB.</p> Note <p>Each sample includes a feature vector (one of the rows in ) and a single class label (one of the columns in ) <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the TileDB dataset folder</p> required <code>train_split_subfolder</code> <code>str</code> <p>Subfolder name for the training split.</p> required <code>valid_split_subfolder</code> <code>str</code> <p>Subfolder name for the validation split.</p> required <code>test_split_subfolder</code> <code>str</code> <p>Subfolder name for the test split.</p> required <code>backbone_class_path</code> <code>Optional[str]</code> <p>Class path of the backbone model.</p> <code>None</code> <code>layer_name</code> <code>str</code> <p>Name of the layer in the TileDB dataset.</p> <code>'data'</code> <code>obs_column_name</code> <code>str</code> <p>Name of the column in  to use as the label. <code>'cell_type'</code> <code>measurement_name</code> <code>str</code> <p>Name of the measurement in the TileDB dataset.</p> <code>'RNA'</code> <code>axis_query_value_filter</code> <code>Optional[str]</code> <p>Optional filter for the axis query.</p> <code>None</code> <code>prefetch_factor</code> <code>int</code> <p>Number of batches to prefetch.</p> <code>16</code> <code>config_name</code> <code>Optional[str]</code> <p>The name of the HF dataset configuration. Affects how the dataset is loaded.</p> <code>None</code> <code>train_split_name</code> <code>Optional[str]</code> <p>The name of the training split.</p> <code>'train'</code> <code>test_split_name</code> <code>Optional[str]</code> <p>The name of the test split. Also used for <code>mgen predict</code>.</p> <code>'test'</code> <code>valid_split_name</code> <code>Optional[str]</code> <p>The name of the validation split.</p> <code>None</code> <code>train_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"train\" from these files. Not used unless referenced by the name \"train\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"test\" from these files. Not used unless referenced by the name \"test\" in one of the split_name arguments. Also used for <code>mgen predict</code>.</p> <code>None</code> <code>valid_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"valid\" from these files. Not used unless referenced by the name \"valid\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_size</code> <code>float</code> <p>The size of the test split. If test_split_name is None, creates a test split of this size from the training split.</p> <code>0.2</code> <code>valid_split_size</code> <code>float</code> <p>The size of the validation split. If valid_split_name is None, creates a validation split of this size from the training split.</p> <code>0.1</code> <code>random_seed</code> <code>int</code> <p>The random seed to use for splitting the data.</p> <code>42</code> <code>extra_reader_kwargs</code> <code>Optional[dict]</code> <p>Extra kwargs for dataset readers.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>The batch size.</p> <code>128</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data.</p> <code>True</code> <code>sampler</code> <code>Optional[Sampler]</code> <p>The sampler to use.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>The number of workers to use for data loading.</p> <code>0</code> <code>collate_fn</code> <code>Optional[callable]</code> <p>The function to use for collating data.</p> <code>None</code> <code>pin_memory</code> <code>bool</code> <p>Whether to pin memory.</p> <code>True</code> <code>persistent_workers</code> <code>bool</code> <p>Whether to use persistent workers.</p> <code>False</code> <code>cv_num_folds</code> <code>int</code> <p>The number of cross-validation folds, disables cv when &lt;= 1.</p> <code>1</code> <code>cv_test_fold_id</code> <code>int</code> <p>The fold id to use for cross-validation evaluation.</p> <code>0</code> <code>cv_enable_val_fold</code> <code>bool</code> <p>Whether to enable a validation fold.</p> <code>True</code> <code>cv_replace_val_fold_as_test_fold</code> <code>bool</code> <p>Replace validation fold with test fold. Only used when cv_enable_val_fold is False.</p> <code>False</code> <code>cv_fold_id_col</code> <code>Optional[str]</code> <p>The column name containing the fold id from a pre-split dataset. Setting to None to enable automatic splitting.</p> <code>None</code> <code>cv_val_offset</code> <code>int</code> <p>The offset applied to cv_test_fold_id to determine val_fold_id.</p> <code>1</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.ClockDataModule","title":"<code>modelgenerator.data.ClockDataModule</code>","text":"<p>               Bases: <code>DataInterface</code></p> <p>Data module for transcriptomic clock tasks.</p> Note <p>Each sample includes a feature vector (one of the rows in ) and a single scalar corresponding to donor age (one of the columns in ) <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> required <code>split_column</code> <code>str</code> <p>The column of  that defines the split assignments. required <code>label_scaling</code> <code>Optional[str]</code> <p>The type of label scaling to apply.</p> <code>'z_scaling'</code> <code>backbone_class_path</code> <code>Optional[str]</code> <p>Class path of the backbone model.</p> <code>None</code> <code>filter_columns</code> <code>Optional[list[str]]</code> <p>The columns of  we want to use. Defaults to None, in which case all columns are used. <code>None</code> <code>rename_columns</code> <code>Optional[list[str]]</code> <p>New name of columns. Defaults to None, in which case columns are not renamed. Does nothing if filter_colums is None.</p> <code>None</code> <code>config_name</code> <code>Optional[str]</code> <p>The name of the HF dataset configuration. Affects how the dataset is loaded.</p> <code>None</code> <code>train_split_name</code> <code>Optional[str]</code> <p>The name of the training split.</p> <code>'train'</code> <code>test_split_name</code> <code>Optional[str]</code> <p>The name of the test split. Also used for <code>mgen predict</code>.</p> <code>'test'</code> <code>valid_split_name</code> <code>Optional[str]</code> <p>The name of the validation split.</p> <code>None</code> <code>train_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"train\" from these files. Not used unless referenced by the name \"train\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"test\" from these files. Not used unless referenced by the name \"test\" in one of the split_name arguments. Also used for <code>mgen predict</code>.</p> <code>None</code> <code>valid_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"valid\" from these files. Not used unless referenced by the name \"valid\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_size</code> <code>float</code> <p>The size of the test split. If test_split_name is None, creates a test split of this size from the training split.</p> <code>0.2</code> <code>valid_split_size</code> <code>float</code> <p>The size of the validation split. If valid_split_name is None, creates a validation split of this size from the training split.</p> <code>0.1</code> <code>random_seed</code> <code>int</code> <p>The random seed to use for splitting the data.</p> <code>42</code> <code>extra_reader_kwargs</code> <code>Optional[dict]</code> <p>Extra kwargs for dataset readers.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>The batch size.</p> <code>128</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data.</p> <code>True</code> <code>sampler</code> <code>Optional[Sampler]</code> <p>The sampler to use.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>The number of workers to use for data loading.</p> <code>0</code> <code>collate_fn</code> <code>Optional[callable]</code> <p>The function to use for collating data.</p> <code>None</code> <code>pin_memory</code> <code>bool</code> <p>Whether to pin memory.</p> <code>True</code> <code>persistent_workers</code> <code>bool</code> <p>Whether to use persistent workers.</p> <code>False</code> <code>cv_num_folds</code> <code>int</code> <p>The number of cross-validation folds, disables cv when &lt;= 1.</p> <code>1</code> <code>cv_test_fold_id</code> <code>int</code> <p>The fold id to use for cross-validation evaluation.</p> <code>0</code> <code>cv_enable_val_fold</code> <code>bool</code> <p>Whether to enable a validation fold.</p> <code>True</code> <code>cv_replace_val_fold_as_test_fold</code> <code>bool</code> <p>Replace validation fold with test fold. Only used when cv_enable_val_fold is False.</p> <code>False</code> <code>cv_fold_id_col</code> <code>Optional[str]</code> <p>The column name containing the fold id from a pre-split dataset. Setting to None to enable automatic splitting.</p> <code>None</code> <code>cv_val_offset</code> <code>int</code> <p>The offset applied to cv_test_fold_id to determine val_fold_id.</p> <code>1</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.PertClassificationDataModule","title":"<code>modelgenerator.data.PertClassificationDataModule</code>","text":"<p>               Bases: <code>DataInterface</code></p> <p>Data module for perturbation classification.</p> Note <p>Each sample includes a feature vector (one of the rows in ) and a single class label (one of the columns in ) <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> required <code>pert_column</code> <code>str</code> <p>Column of  containing perturbation labels. required <code>cell_line_column</code> <code>str</code> <p>Column of  containing cell line labels. required <code>cell_line</code> <code>str</code> <p>Name of cell line to consider.</p> required <code>split_seed</code> <code>int</code> <p>Seed for train/val/test splits.</p> <code>1234</code> <code>train_frac</code> <code>float</code> <p>Fraction of examples to assign to train set.</p> <code>0.7</code> <code>val_frac</code> <code>float</code> <p>Fraction of examples to assign to val set.</p> <code>0.15</code> <code>test_frac</code> <code>float</code> <p>Fraction of examples to assign to test set.</p> <code>0.15</code> <code>backbone_class_path</code> <code>Optional[str]</code> <p>Class path of the backbone model.</p> <code>None</code> <code>filter_columns</code> <code>Optional[list[str]]</code> <p>The columns of  we want to use. Defaults to None, in which case all columns are used. <code>None</code> <code>rename_columns</code> <code>Optional[list[str]]</code> <p>New name of columns. Defaults to None, in which case columns are not renamed. Does nothing if filter_colums is None.</p> <code>None</code> <code>config_name</code> <code>Optional[str]</code> <p>The name of the HF dataset configuration. Affects how the dataset is loaded.</p> <code>None</code> <code>train_split_name</code> <code>Optional[str]</code> <p>The name of the training split.</p> <code>'train'</code> <code>test_split_name</code> <code>Optional[str]</code> <p>The name of the test split. Also used for <code>mgen predict</code>.</p> <code>'test'</code> <code>valid_split_name</code> <code>Optional[str]</code> <p>The name of the validation split.</p> <code>None</code> <code>train_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"train\" from these files. Not used unless referenced by the name \"train\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"test\" from these files. Not used unless referenced by the name \"test\" in one of the split_name arguments. Also used for <code>mgen predict</code>.</p> <code>None</code> <code>valid_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"valid\" from these files. Not used unless referenced by the name \"valid\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_size</code> <code>float</code> <p>The size of the test split. If test_split_name is None, creates a test split of this size from the training split.</p> <code>0.2</code> <code>valid_split_size</code> <code>float</code> <p>The size of the validation split. If valid_split_name is None, creates a validation split of this size from the training split.</p> <code>0.1</code> <code>random_seed</code> <code>int</code> <p>The random seed to use for splitting the data.</p> <code>42</code> <code>extra_reader_kwargs</code> <code>Optional[dict]</code> <p>Extra kwargs for dataset readers.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>The batch size.</p> <code>128</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data.</p> <code>True</code> <code>sampler</code> <code>Optional[Sampler]</code> <p>The sampler to use.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>The number of workers to use for data loading.</p> <code>0</code> <code>collate_fn</code> <code>Optional[callable]</code> <p>The function to use for collating data.</p> <code>None</code> <code>pin_memory</code> <code>bool</code> <p>Whether to pin memory.</p> <code>True</code> <code>persistent_workers</code> <code>bool</code> <p>Whether to use persistent workers.</p> <code>False</code> <code>cv_num_folds</code> <code>int</code> <p>The number of cross-validation folds, disables cv when &lt;= 1.</p> <code>1</code> <code>cv_test_fold_id</code> <code>int</code> <p>The fold id to use for cross-validation evaluation.</p> <code>0</code> <code>cv_enable_val_fold</code> <code>bool</code> <p>Whether to enable a validation fold.</p> <code>True</code> <code>cv_replace_val_fold_as_test_fold</code> <code>bool</code> <p>Replace validation fold with test fold. Only used when cv_enable_val_fold is False.</p> <code>False</code> <code>cv_fold_id_col</code> <code>Optional[str]</code> <p>The column name containing the fold id from a pre-split dataset. Setting to None to enable automatic splitting.</p> <code>None</code> <code>cv_val_offset</code> <code>int</code> <p>The offset applied to cv_test_fold_id to determine val_fold_id.</p> <code>1</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#tissue","title":"Tissue","text":""},{"location":"api_reference/data/#modelgenerator.data.CellWithNeighborDataModule","title":"<code>modelgenerator.data.CellWithNeighborDataModule</code>","text":"<p>               Bases: <code>DataInterface</code></p> <p>Data module for cell classification with neighbors for AIDO.Tissue.</p> Note <p>Each sample includes a feature vector (one of the rows in ) and a single class label (one of the columns in ) The feature vector is concatenated with the feature vectors of its neighbors. <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> required <code>filter_columns</code> <code>Optional[List[str]]</code> <p>The columns of  we want to use. Defaults to None, in which case all columns are used. <code>None</code> <code>rename_columns</code> <code>Optional[List[str]]</code> <p>Optional list of columns to rename.</p> <code>None</code> <code>use_random_neighbor</code> <code>bool</code> <p>Whether to use random neighbors.</p> <code>False</code> <code>copy_center_as_neighbor</code> <code>bool</code> <p>Whether to copy center as a neighbor.</p> <code>False</code> <code>neighbor_num</code> <code>int</code> <p>Number of neighbors to consider.</p> <code>10</code> <code>generate_uid</code> <code>bool</code> <p>Whether to generate a unique identifier.</p> <code>False</code> <code>config_name</code> <code>Optional[str]</code> <p>The name of the HF dataset configuration. Affects how the dataset is loaded.</p> <code>None</code> <code>train_split_name</code> <code>Optional[str]</code> <p>The name of the training split.</p> <code>'train'</code> <code>test_split_name</code> <code>Optional[str]</code> <p>The name of the test split. Also used for <code>mgen predict</code>.</p> <code>'test'</code> <code>valid_split_name</code> <code>Optional[str]</code> <p>The name of the validation split.</p> <code>None</code> <code>train_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"train\" from these files. Not used unless referenced by the name \"train\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"test\" from these files. Not used unless referenced by the name \"test\" in one of the split_name arguments. Also used for <code>mgen predict</code>.</p> <code>None</code> <code>valid_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"valid\" from these files. Not used unless referenced by the name \"valid\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_size</code> <code>float</code> <p>The size of the test split. If test_split_name is None, creates a test split of this size from the training split.</p> <code>0.2</code> <code>valid_split_size</code> <code>float</code> <p>The size of the validation split. If valid_split_name is None, creates a validation split of this size from the training split.</p> <code>0.1</code> <code>random_seed</code> <code>int</code> <p>The random seed to use for splitting the data.</p> <code>42</code> <code>extra_reader_kwargs</code> <code>Optional[dict]</code> <p>Extra kwargs for dataset readers.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>The batch size.</p> <code>128</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data.</p> <code>True</code> <code>sampler</code> <code>Optional[Sampler]</code> <p>The sampler to use.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>The number of workers to use for data loading.</p> <code>0</code> <code>collate_fn</code> <code>Optional[callable]</code> <p>The function to use for collating data.</p> <code>None</code> <code>pin_memory</code> <code>bool</code> <p>Whether to pin memory.</p> <code>True</code> <code>persistent_workers</code> <code>bool</code> <p>Whether to use persistent workers.</p> <code>False</code> <code>cv_num_folds</code> <code>int</code> <p>The number of cross-validation folds, disables cv when &lt;= 1.</p> <code>1</code> <code>cv_test_fold_id</code> <code>int</code> <p>The fold id to use for cross-validation evaluation.</p> <code>0</code> <code>cv_enable_val_fold</code> <code>bool</code> <p>Whether to enable a validation fold.</p> <code>True</code> <code>cv_replace_val_fold_as_test_fold</code> <code>bool</code> <p>Replace validation fold with test fold. Only used when cv_enable_val_fold is False.</p> <code>False</code> <code>cv_fold_id_col</code> <code>Optional[str]</code> <p>The column name containing the fold id from a pre-split dataset. Setting to None to enable automatic splitting.</p> <code>None</code> <code>cv_val_offset</code> <code>int</code> <p>The offset applied to cv_test_fold_id to determine val_fold_id.</p> <code>1</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#multimodal","title":"Multimodal","text":""},{"location":"api_reference/data/#modelgenerator.data.IsoformExpression","title":"<code>modelgenerator.data.IsoformExpression</code>","text":"<p>               Bases: <code>SequenceRegressionDataModule</code></p> <p>Isoform expression prediction benchmarks from the </p> Note <ul> <li>Manuscript: Multi-modal Transfer Learning between Biological Foundation Models</li> <li>Data Card: genbio-ai/transcript_isoform_expression_prediction</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> <code>'genbio-ai/transcript_isoform_expression_prediction'</code> <code>config_name</code> <code>str</code> <p>The name of the HF dataset configuration. Affects how the dataset is loaded.</p> <code>None</code> <code>x_col</code> <code>Union[str, list]</code> <p>The name of columns containing the sequences.</p> <code>['dna_seq', 'rna_seq', 'protein_seq']</code> <code>valid_split_name</code> <p>The name of the validation split.</p> <code>'valid'</code> <code>train_split_files</code> <code>Optional[Union[str, list[str]]]</code> <p>Create a split called \"train\" from these files. Not used unless referenced by the name \"train\" in one of the split_name arguments.</p> <code>'train_*.tsv'</code> <code>test_split_files</code> <code>Optional[Union[str, list[str]]]</code> <p>Create a split called \"test\" from these files. Not used unless referenced by the name \"test\" in one of the split_name arguments. Also used for <code>mgen predict</code>.</p> <code>'test.tsv'</code> <code>valid_split_files</code> <code>Optional[Union[str, list[str]]]</code> <p>Create a split called \"valid\" from these files. Not used unless referenced by the name \"valid\" in one of the split_name arguments.</p> <code>'validation.tsv'</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the labels.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#base-classes","title":"Base Classes","text":""},{"location":"api_reference/data/#modelgenerator.data.DataInterface","title":"<code>modelgenerator.data.DataInterface</code>","text":"<p>               Bases: <code>LightningDataModule</code>, <code>KFoldMixin</code></p> <p>Base class for all data modules in this project. Handles the boilerplate of setting up data loaders.</p> Note <p>Subclasses must implement the setup method. All datasets should return a dictionary of data items. To use HF loading, add the HFDatasetLoaderMixin. For any task-specific behaviors, implement transformations using <code>torch.utils.data.Dataset</code> objects. See MLM for an example.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> required <code>config_name</code> <code>Optional[str]</code> <p>The name of the HF dataset configuration. Affects how the dataset is loaded.</p> <code>None</code> <code>train_split_name</code> <code>Optional[str]</code> <p>The name of the training split.</p> <code>'train'</code> <code>test_split_name</code> <code>Optional[str]</code> <p>The name of the test split. Also used for <code>mgen predict</code>.</p> <code>'test'</code> <code>valid_split_name</code> <code>Optional[str]</code> <p>The name of the validation split.</p> <code>None</code> <code>train_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"train\" from these files. Not used unless referenced by the name \"train\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"test\" from these files. Not used unless referenced by the name \"test\" in one of the split_name arguments. Also used for <code>mgen predict</code>.</p> <code>None</code> <code>valid_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"valid\" from these files. Not used unless referenced by the name \"valid\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_size</code> <code>float</code> <p>The size of the test split. If test_split_name is None, creates a test split of this size from the training split.</p> <code>0.2</code> <code>valid_split_size</code> <code>float</code> <p>The size of the validation split. If valid_split_name is None, creates a validation split of this size from the training split.</p> <code>0.1</code> <code>random_seed</code> <code>int</code> <p>The random seed to use for splitting the data.</p> <code>42</code> <code>extra_reader_kwargs</code> <code>Optional[dict]</code> <p>Extra kwargs for dataset readers.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>The batch size.</p> <code>128</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data.</p> <code>True</code> <code>sampler</code> <code>Optional[Sampler]</code> <p>The sampler to use.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>The number of workers to use for data loading.</p> <code>0</code> <code>collate_fn</code> <code>Optional[callable]</code> <p>The function to use for collating data.</p> <code>None</code> <code>pin_memory</code> <code>bool</code> <p>Whether to pin memory.</p> <code>True</code> <code>persistent_workers</code> <code>bool</code> <p>Whether to use persistent workers.</p> <code>False</code> <code>cv_num_folds</code> <code>int</code> <p>The number of cross-validation folds, disables cv when &lt;= 1.</p> <code>1</code> <code>cv_test_fold_id</code> <code>int</code> <p>The fold id to use for cross-validation evaluation.</p> <code>0</code> <code>cv_enable_val_fold</code> <code>bool</code> <p>Whether to enable a validation fold.</p> <code>True</code> <code>cv_replace_val_fold_as_test_fold</code> <code>bool</code> <p>Replace validation fold with test fold. Only used when cv_enable_val_fold is False.</p> <code>False</code> <code>cv_fold_id_col</code> <code>Optional[str]</code> <p>The column name containing the fold id from a pre-split dataset. Setting to None to enable automatic splitting.</p> <code>None</code> <code>cv_val_offset</code> <code>int</code> <p>The offset applied to cv_test_fold_id to determine val_fold_id.</p> <code>1</code>"},{"location":"api_reference/data/#modelgenerator.data.ColumnRetrievalDataModule","title":"<code>modelgenerator.data.ColumnRetrievalDataModule</code>","text":"<p>               Bases: <code>DataInterface</code>, <code>HFDatasetLoaderMixin</code></p> <p>Simple data module for retrieving and renaming columns from a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> required <code>config_name</code> <code>Optional[str]</code> <p>The name of the HF dataset configuration. Affects how the dataset is loaded.</p> <code>None</code> <code>in_cols</code> <code>List[str]</code> <p>The name of the columns to retrieve.</p> <code>[]</code> <code>out_cols</code> <code>Optional[List[str]]</code> <p>The name of the columns to use as the alias for the retrieved columns.</p> <code>None</code> <code>train_split_name</code> <code>Optional[str]</code> <p>The name of the training split.</p> <code>'train'</code> <code>test_split_name</code> <code>Optional[str]</code> <p>The name of the test split. Also used for <code>mgen predict</code>.</p> <code>'test'</code> <code>valid_split_name</code> <code>Optional[str]</code> <p>The name of the validation split.</p> <code>None</code> <code>train_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"train\" from these files. Not used unless referenced by the name \"train\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"test\" from these files. Not used unless referenced by the name \"test\" in one of the split_name arguments. Also used for <code>mgen predict</code>.</p> <code>None</code> <code>valid_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"valid\" from these files. Not used unless referenced by the name \"valid\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_size</code> <code>float</code> <p>The size of the test split. If test_split_name is None, creates a test split of this size from the training split.</p> <code>0.2</code> <code>valid_split_size</code> <code>float</code> <p>The size of the validation split. If valid_split_name is None, creates a validation split of this size from the training split.</p> <code>0.1</code> <code>random_seed</code> <code>int</code> <p>The random seed to use for splitting the data.</p> <code>42</code> <code>extra_reader_kwargs</code> <code>Optional[dict]</code> <p>Extra kwargs for dataset readers.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>The batch size.</p> <code>128</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data.</p> <code>True</code> <code>sampler</code> <code>Optional[Sampler]</code> <p>The sampler to use.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>The number of workers to use for data loading.</p> <code>0</code> <code>collate_fn</code> <code>Optional[callable]</code> <p>The function to use for collating data.</p> <code>None</code> <code>pin_memory</code> <code>bool</code> <p>Whether to pin memory.</p> <code>True</code> <code>persistent_workers</code> <code>bool</code> <p>Whether to use persistent workers.</p> <code>False</code> <code>cv_num_folds</code> <code>int</code> <p>The number of cross-validation folds, disables cv when &lt;= 1.</p> <code>1</code> <code>cv_test_fold_id</code> <code>int</code> <p>The fold id to use for cross-validation evaluation.</p> <code>0</code> <code>cv_enable_val_fold</code> <code>bool</code> <p>Whether to enable a validation fold.</p> <code>True</code> <code>cv_replace_val_fold_as_test_fold</code> <code>bool</code> <p>Replace validation fold with test fold. Only used when cv_enable_val_fold is False.</p> <code>False</code> <code>cv_fold_id_col</code> <code>Optional[str]</code> <p>The column name containing the fold id from a pre-split dataset. Setting to None to enable automatic splitting.</p> <code>None</code> <code>cv_val_offset</code> <code>int</code> <p>The offset applied to cv_test_fold_id to determine val_fold_id.</p> <code>1</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.SequencesDataModule","title":"<code>modelgenerator.data.SequencesDataModule</code>","text":"<p>               Bases: <code>DataInterface</code>, <code>HFDatasetLoaderMixin</code></p> <p>Data module for loading a simple dataset of sequences.</p> Note <p>Each sample includes a single sequence under key 'sequences' and optionally an 'id' to track outputs.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> required <code>config_name</code> <code>Optional[str]</code> <p>The name of the HF dataset configuration. Affects how the dataset is loaded.</p> <code>None</code> <code>test_split_name</code> <code>Optional[str]</code> <p>The name of the test split. Also used for <code>mgen predict</code>.</p> <code>None</code> <code>test_split_files</code> <code>Optional[str]</code> <p>Create a split called \"test\" from these files. Not used unless referenced by the name \"test\" in one of the split_name arguments. Also used for <code>mgen predict</code>.</p> <code>None</code> <code>x_col</code> <code>str</code> <p>The name of the column containing the sequences.</p> <code>'sequence'</code> <code>id_col</code> <code>str</code> <p>The name of the column containing the ids.</p> <code>'id'</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.SequenceClassificationDataModule","title":"<code>modelgenerator.data.SequenceClassificationDataModule</code>","text":"<p>               Bases: <code>DataInterface</code>, <code>HFDatasetLoaderMixin</code></p> <p>Data module for Hugging Face sequence classification datasets.</p> Note <p>Each sample includes a single sequence under key 'sequences' and a single class label under key 'labels'</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> required <code>config_name</code> <code>Optional[str]</code> <p>The name of the HF dataset configuration. Affects how the dataset is loaded.</p> <code>None</code> <code>x_col</code> <code>str</code> <p>The name of the column containing the sequences.</p> <code>'sequence'</code> <code>y_col</code> <code>str | List[str]</code> <p>The name of the column(s) containing the labels.</p> <code>'label'</code> <code>extra_cols</code> <code>List[str] | None</code> <p>Additional columns to include in the dataset.</p> <code>None</code> <code>extra_col_aliases</code> <code>List[str] | None</code> <p>The name of the columns to use as the alias for the extra columns.</p> <code>None</code> <code>class_filter</code> <code>int | List[int] | None</code> <p>Filter the dataset to only include samples with the specified class(es).</p> <code>None</code> <code>generate_uid</code> <code>bool</code> <p>Whether to generate a unique ID for each sample.</p> <code>False</code> <code>train_split_name</code> <code>Optional[str]</code> <p>The name of the training split.</p> <code>'train'</code> <code>test_split_name</code> <code>Optional[str]</code> <p>The name of the test split. Also used for <code>mgen predict</code>.</p> <code>'test'</code> <code>valid_split_name</code> <code>Optional[str]</code> <p>The name of the validation split.</p> <code>None</code> <code>train_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"train\" from these files. Not used unless referenced by the name \"train\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"test\" from these files. Not used unless referenced by the name \"test\" in one of the split_name arguments. Also used for <code>mgen predict</code>.</p> <code>None</code> <code>valid_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"valid\" from these files. Not used unless referenced by the name \"valid\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_size</code> <code>float</code> <p>The size of the test split. If test_split_name is None, creates a test split of this size from the training split.</p> <code>0.2</code> <code>valid_split_size</code> <code>float</code> <p>The size of the validation split. If valid_split_name is None, creates a validation split of this size from the training split.</p> <code>0.1</code> <code>random_seed</code> <code>int</code> <p>The random seed to use for splitting the data.</p> <code>42</code> <code>extra_reader_kwargs</code> <code>Optional[dict]</code> <p>Extra kwargs for dataset readers.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>The batch size.</p> <code>128</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data.</p> <code>True</code> <code>sampler</code> <code>Optional[Sampler]</code> <p>The sampler to use.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>The number of workers to use for data loading.</p> <code>0</code> <code>collate_fn</code> <code>Optional[callable]</code> <p>The function to use for collating data.</p> <code>None</code> <code>pin_memory</code> <code>bool</code> <p>Whether to pin memory.</p> <code>True</code> <code>persistent_workers</code> <code>bool</code> <p>Whether to use persistent workers.</p> <code>False</code> <code>cv_num_folds</code> <code>int</code> <p>The number of cross-validation folds, disables cv when &lt;= 1.</p> <code>1</code> <code>cv_test_fold_id</code> <code>int</code> <p>The fold id to use for cross-validation evaluation.</p> <code>0</code> <code>cv_enable_val_fold</code> <code>bool</code> <p>Whether to enable a validation fold.</p> <code>True</code> <code>cv_replace_val_fold_as_test_fold</code> <code>bool</code> <p>Replace validation fold with test fold. Only used when cv_enable_val_fold is False.</p> <code>False</code> <code>cv_fold_id_col</code> <code>Optional[str]</code> <p>The column name containing the fold id from a pre-split dataset. Setting to None to enable automatic splitting.</p> <code>None</code> <code>cv_val_offset</code> <code>int</code> <p>The offset applied to cv_test_fold_id to determine val_fold_id.</p> <code>1</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.SequenceRegressionDataModule","title":"<code>modelgenerator.data.SequenceRegressionDataModule</code>","text":"<p>               Bases: <code>DataInterface</code>, <code>HFDatasetLoaderMixin</code></p> <p>Data module for sequence regression datasets.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> required <code>config_name</code> <code>Optional[str]</code> <p>The name of the HF dataset configuration. Affects how the dataset is loaded.</p> <code>None</code> <code>x_col</code> <code>str</code> <p>The name of columns containing the sequences.</p> <code>'sequence'</code> <code>y_col</code> <code>str</code> <p>The name of columns containing the labels.</p> <code>'label'</code> <code>extra_cols</code> <code>List[str]</code> <p>Additional columns to include in the dataset.</p> <code>None</code> <code>extra_col_aliases</code> <code>List[str]</code> <p>The name of the columns to use as the alias for the extra columns.</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the labels.</p> <code>True</code> <code>generate_uid</code> <code>bool</code> <p>Whether to generate a unique ID for each sample.</p> <code>False</code> <code>train_split_name</code> <code>Optional[str]</code> <p>The name of the training split.</p> <code>'train'</code> <code>test_split_name</code> <code>Optional[str]</code> <p>The name of the test split. Also used for <code>mgen predict</code>.</p> <code>'test'</code> <code>valid_split_name</code> <code>Optional[str]</code> <p>The name of the validation split.</p> <code>None</code> <code>train_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"train\" from these files. Not used unless referenced by the name \"train\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"test\" from these files. Not used unless referenced by the name \"test\" in one of the split_name arguments. Also used for <code>mgen predict</code>.</p> <code>None</code> <code>valid_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"valid\" from these files. Not used unless referenced by the name \"valid\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_size</code> <code>float</code> <p>The size of the test split. If test_split_name is None, creates a test split of this size from the training split.</p> <code>0.2</code> <code>valid_split_size</code> <code>float</code> <p>The size of the validation split. If valid_split_name is None, creates a validation split of this size from the training split.</p> <code>0.1</code> <code>random_seed</code> <code>int</code> <p>The random seed to use for splitting the data.</p> <code>42</code> <code>extra_reader_kwargs</code> <code>Optional[dict]</code> <p>Extra kwargs for dataset readers.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>The batch size.</p> <code>128</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data.</p> <code>True</code> <code>sampler</code> <code>Optional[Sampler]</code> <p>The sampler to use.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>The number of workers to use for data loading.</p> <code>0</code> <code>collate_fn</code> <code>Optional[callable]</code> <p>The function to use for collating data.</p> <code>None</code> <code>pin_memory</code> <code>bool</code> <p>Whether to pin memory.</p> <code>True</code> <code>persistent_workers</code> <code>bool</code> <p>Whether to use persistent workers.</p> <code>False</code> <code>cv_num_folds</code> <code>int</code> <p>The number of cross-validation folds, disables cv when &lt;= 1.</p> <code>1</code> <code>cv_test_fold_id</code> <code>int</code> <p>The fold id to use for cross-validation evaluation.</p> <code>0</code> <code>cv_enable_val_fold</code> <code>bool</code> <p>Whether to enable a validation fold.</p> <code>True</code> <code>cv_replace_val_fold_as_test_fold</code> <code>bool</code> <p>Replace validation fold with test fold. Only used when cv_enable_val_fold is False.</p> <code>False</code> <code>cv_fold_id_col</code> <code>Optional[str]</code> <p>The column name containing the fold id from a pre-split dataset. Setting to None to enable automatic splitting.</p> <code>None</code> <code>cv_val_offset</code> <code>int</code> <p>The offset applied to cv_test_fold_id to determine val_fold_id.</p> <code>1</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.TokenClassificationDataModule","title":"<code>modelgenerator.data.TokenClassificationDataModule</code>","text":"<p>               Bases: <code>DataInterface</code>, <code>HFDatasetLoaderMixin</code></p> <p>Data module for Hugging Face token classification datasets.</p> Note <p>Each sample includes a single sequence under key 'sequences' and a single class sequence under key 'labels'</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> required <code>config_name</code> <code>Optional[str]</code> <p>The name of the HF dataset configuration. Affects how the dataset is loaded.</p> <code>None</code> <code>x_col</code> <code>str</code> <p>The name of the column containing the sequences.</p> <code>'sequence'</code> <code>y_col</code> <code>str</code> <p>The name of the column containing the labels.</p> <code>'label'</code> <code>extra_cols</code> <code>List[str] | None</code> <p>Additional columns to include in the dataset.</p> <code>None</code> <code>extra_col_aliases</code> <code>List[str] | None</code> <p>The name of the columns to use as the alias for the extra columns.</p> <code>None</code> <code>max_length</code> <code>Optional[int]</code> <p>The maximum length of the sequences.</p> <code>None</code> <code>truncate_extra_cols</code> <code>bool</code> <p>Whether to truncate the extra columns to the maximum length.</p> <code>False</code> <code>pairwise</code> <code>bool</code> <p>Whether the labels are pairwise.</p> <code>False</code> <code>collate_fn</code> <code>Optional[callable]</code> <p>The function to use for collating data.</p> <code>None</code> <code>generate_uid</code> <code>bool</code> <p>Whether to generate a unique ID for each sample.</p> <code>False</code> <code>train_split_name</code> <code>Optional[str]</code> <p>The name of the training split.</p> <code>'train'</code> <code>test_split_name</code> <code>Optional[str]</code> <p>The name of the test split. Also used for <code>mgen predict</code>.</p> <code>'test'</code> <code>valid_split_name</code> <code>Optional[str]</code> <p>The name of the validation split.</p> <code>None</code> <code>train_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"train\" from these files. Not used unless referenced by the name \"train\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"test\" from these files. Not used unless referenced by the name \"test\" in one of the split_name arguments. Also used for <code>mgen predict</code>.</p> <code>None</code> <code>valid_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"valid\" from these files. Not used unless referenced by the name \"valid\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_size</code> <code>float</code> <p>The size of the test split. If test_split_name is None, creates a test split of this size from the training split.</p> <code>0.2</code> <code>valid_split_size</code> <code>float</code> <p>The size of the validation split. If valid_split_name is None, creates a validation split of this size from the training split.</p> <code>0.1</code> <code>random_seed</code> <code>int</code> <p>The random seed to use for splitting the data.</p> <code>42</code> <code>extra_reader_kwargs</code> <code>Optional[dict]</code> <p>Extra kwargs for dataset readers.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>The batch size.</p> <code>128</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data.</p> <code>True</code> <code>sampler</code> <code>Optional[Sampler]</code> <p>The sampler to use.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>The number of workers to use for data loading.</p> <code>0</code> <code>pin_memory</code> <code>bool</code> <p>Whether to pin memory.</p> <code>True</code> <code>persistent_workers</code> <code>bool</code> <p>Whether to use persistent workers.</p> <code>False</code> <code>cv_num_folds</code> <code>int</code> <p>The number of cross-validation folds, disables cv when &lt;= 1.</p> <code>1</code> <code>cv_test_fold_id</code> <code>int</code> <p>The fold id to use for cross-validation evaluation.</p> <code>0</code> <code>cv_enable_val_fold</code> <code>bool</code> <p>Whether to enable a validation fold.</p> <code>True</code> <code>cv_replace_val_fold_as_test_fold</code> <code>bool</code> <p>Replace validation fold with test fold. Only used when cv_enable_val_fold is False.</p> <code>False</code> <code>cv_fold_id_col</code> <code>Optional[str]</code> <p>The column name containing the fold id from a pre-split dataset. Setting to None to enable automatic splitting.</p> <code>None</code> <code>cv_val_offset</code> <code>int</code> <p>The offset applied to cv_test_fold_id to determine val_fold_id.</p> <code>1</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.DiffusionDataModule","title":"<code>modelgenerator.data.DiffusionDataModule</code>","text":"<p>               Bases: <code>DataInterface</code>, <code>HFDatasetLoaderMixin</code></p> <p>Data module for datasets with discrete diffusion-based noising and loss weights from MDLM.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> required <code>config_name</code> <code>Optional[str]</code> <p>The name of the HF dataset configuration. Affects how the dataset is loaded.</p> <code>None</code> <code>x_col</code> <code>str</code> <p>The column with the data to train on.</p> <code>'sequence'</code> <code>extra_cols</code> <code>List[str] | None</code> <p>Additional columns to include in the dataset.</p> <code>None</code> <code>extra_col_aliases</code> <code>List[str] | None</code> <p>The name of the columns to use as the alias for the extra columns.</p> <code>None</code> <code>timesteps_per_sample</code> <code>int</code> <p>The number of timesteps per sample.</p> <code>10</code> <code>randomize_targets</code> <code>bool</code> <p>Whether to randomize the target sequences for each timestep (experimental efficiency boost).</p> <code>False</code> <code>batch_size</code> <code>int</code> <p>The batch size.</p> <code>10</code> <code>train_split_name</code> <code>Optional[str]</code> <p>The name of the training split.</p> <code>'train'</code> <code>test_split_name</code> <code>Optional[str]</code> <p>The name of the test split. Also used for <code>mgen predict</code>.</p> <code>'test'</code> <code>valid_split_name</code> <code>Optional[str]</code> <p>The name of the validation split.</p> <code>None</code> <code>train_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"train\" from these files. Not used unless referenced by the name \"train\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"test\" from these files. Not used unless referenced by the name \"test\" in one of the split_name arguments. Also used for <code>mgen predict</code>.</p> <code>None</code> <code>valid_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"valid\" from these files. Not used unless referenced by the name \"valid\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_size</code> <code>float</code> <p>The size of the test split. If test_split_name is None, creates a test split of this size from the training split.</p> <code>0.2</code> <code>valid_split_size</code> <code>float</code> <p>The size of the validation split. If valid_split_name is None, creates a validation split of this size from the training split.</p> <code>0.1</code> <code>random_seed</code> <code>int</code> <p>The random seed to use for splitting the data.</p> <code>42</code> <code>extra_reader_kwargs</code> <code>Optional[dict]</code> <p>Extra kwargs for dataset readers.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data.</p> <code>True</code> <code>sampler</code> <code>Optional[Sampler]</code> <p>The sampler to use.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>The number of workers to use for data loading.</p> <code>0</code> <code>collate_fn</code> <code>Optional[callable]</code> <p>The function to use for collating data.</p> <code>None</code> <code>pin_memory</code> <code>bool</code> <p>Whether to pin memory.</p> <code>True</code> <code>persistent_workers</code> <code>bool</code> <p>Whether to use persistent workers.</p> <code>False</code> <code>cv_num_folds</code> <code>int</code> <p>The number of cross-validation folds, disables cv when &lt;= 1.</p> <code>1</code> <code>cv_test_fold_id</code> <code>int</code> <p>The fold id to use for cross-validation evaluation.</p> <code>0</code> <code>cv_enable_val_fold</code> <code>bool</code> <p>Whether to enable a validation fold.</p> <code>True</code> <code>cv_replace_val_fold_as_test_fold</code> <code>bool</code> <p>Replace validation fold with test fold. Only used when cv_enable_val_fold is False.</p> <code>False</code> <code>cv_fold_id_col</code> <code>Optional[str]</code> <p>The column name containing the fold id from a pre-split dataset. Setting to None to enable automatic splitting.</p> <code>None</code> <code>cv_val_offset</code> <code>int</code> <p>The offset applied to cv_test_fold_id to determine val_fold_id.</p> <code>1</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code> Notes <p>Each sample includes timesteps_per_sample sequences at different noise levels Each sample's target sequences are under 'target_sequences', the input sequences are under 'sequences', and posterior weights are under 'posterior_weights'</p>"},{"location":"api_reference/data/#modelgenerator.data.ClassDiffusionDataModule","title":"<code>modelgenerator.data.ClassDiffusionDataModule</code>","text":"<p>               Bases: <code>SequenceClassificationDataModule</code></p> <p>Data module for conditional (or class-filtered) diffusion, and applying discrete diffusion noising.</p> Note <p>Each sample includes timesteps_per_sample sequences at different noise levels Each sample's target sequences are under 'target_seqs', the input sequences are under 'input_seqs', and posterior weights are under 'posterior_weights'</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> required <code>config_name</code> <code>Optional[str]</code> <p>The name of the HF dataset configuration. Affects how the dataset is loaded.</p> <code>None</code> <code>x_col</code> <code>str</code> <p>The name of the column containing the sequences.</p> <code>'sequence'</code> <code>y_col</code> <code>str | List[str]</code> <p>The name of the column(s) containing the labels.</p> <code>'label'</code> <code>timesteps_per_sample</code> <code>int</code> <p>The number of timesteps per sample.</p> <code>10</code> <code>randomize_targets</code> <code>bool</code> <p>Whether to randomize the target sequences for each timestep (experimental efficiency boost).</p> <code>False</code> <code>batch_size</code> <code>int</code> <p>The batch size.</p> <code>10</code> <code>extra_cols</code> <code>List[str] | None</code> <p>Additional columns to include in the dataset.</p> <code>None</code> <code>extra_col_aliases</code> <code>List[str] | None</code> <p>The name of the columns to use as the alias for the extra columns.</p> <code>None</code> <code>class_filter</code> <code>int | List[int] | None</code> <p>Filter the dataset to only include samples with the specified class(es).</p> <code>None</code> <code>generate_uid</code> <code>bool</code> <p>Whether to generate a unique ID for each sample.</p> <code>False</code> <code>train_split_name</code> <code>Optional[str]</code> <p>The name of the training split.</p> <code>'train'</code> <code>test_split_name</code> <code>Optional[str]</code> <p>The name of the test split. Also used for <code>mgen predict</code>.</p> <code>'test'</code> <code>valid_split_name</code> <code>Optional[str]</code> <p>The name of the validation split.</p> <code>None</code> <code>train_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"train\" from these files. Not used unless referenced by the name \"train\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"test\" from these files. Not used unless referenced by the name \"test\" in one of the split_name arguments. Also used for <code>mgen predict</code>.</p> <code>None</code> <code>valid_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"valid\" from these files. Not used unless referenced by the name \"valid\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_size</code> <code>float</code> <p>The size of the test split. If test_split_name is None, creates a test split of this size from the training split.</p> <code>0.2</code> <code>valid_split_size</code> <code>float</code> <p>The size of the validation split. If valid_split_name is None, creates a validation split of this size from the training split.</p> <code>0.1</code> <code>random_seed</code> <code>int</code> <p>The random seed to use for splitting the data.</p> <code>42</code> <code>extra_reader_kwargs</code> <code>Optional[dict]</code> <p>Extra kwargs for dataset readers.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data.</p> <code>True</code> <code>sampler</code> <code>Optional[Sampler]</code> <p>The sampler to use.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>The number of workers to use for data loading.</p> <code>0</code> <code>collate_fn</code> <code>Optional[callable]</code> <p>The function to use for collating data.</p> <code>None</code> <code>pin_memory</code> <code>bool</code> <p>Whether to pin memory.</p> <code>True</code> <code>persistent_workers</code> <code>bool</code> <p>Whether to use persistent workers.</p> <code>False</code> <code>cv_num_folds</code> <code>int</code> <p>The number of cross-validation folds, disables cv when &lt;= 1.</p> <code>1</code> <code>cv_test_fold_id</code> <code>int</code> <p>The fold id to use for cross-validation evaluation.</p> <code>0</code> <code>cv_enable_val_fold</code> <code>bool</code> <p>Whether to enable a validation fold.</p> <code>True</code> <code>cv_replace_val_fold_as_test_fold</code> <code>bool</code> <p>Replace validation fold with test fold. Only used when cv_enable_val_fold is False.</p> <code>False</code> <code>cv_fold_id_col</code> <code>Optional[str]</code> <p>The column name containing the fold id from a pre-split dataset. Setting to None to enable automatic splitting.</p> <code>None</code> <code>cv_val_offset</code> <code>int</code> <p>The offset applied to cv_test_fold_id to determine val_fold_id.</p> <code>1</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.ConditionalDiffusionDataModule","title":"<code>modelgenerator.data.ConditionalDiffusionDataModule</code>","text":"<p>               Bases: <code>SequenceRegressionDataModule</code></p> <p>Data module for conditional diffusion with a continuous condition, and applying discrete diffusion noising.</p> Note <p>Each sample includes timesteps_per_sample sequences at different noise levels Each sample's target sequences are under 'target_seqs', the input sequences are under 'input_seqs', and posterior weights are under 'posterior_weights'</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> required <code>config_name</code> <code>Optional[str]</code> <p>The name of the HF dataset configuration. Affects how the dataset is loaded.</p> <code>None</code> <code>x_col</code> <code>str</code> <p>The name of columns containing the sequences.</p> <code>'sequence'</code> <code>y_col</code> <code>str</code> <p>The name of columns containing the labels.</p> <code>'label'</code> <code>extra_cols</code> <code>List[str]</code> <p>Additional columns to include in the dataset.</p> <code>None</code> <code>extra_col_aliases</code> <code>List[str]</code> <p>The name of the columns to use as the alias for the extra columns.</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the labels.</p> <code>True</code> <code>generate_uid</code> <code>bool</code> <p>Whether to generate a unique ID for each sample.</p> <code>False</code> <code>timesteps_per_sample</code> <code>int</code> <p>The number of timesteps per sample.</p> <code>10</code> <code>randomize_targets</code> <code>bool</code> <p>Whether to randomize the target sequences for each timestep (experimental efficiency boost).</p> <code>False</code> <code>batch_size</code> <code>int</code> <p>The batch size.</p> <code>10</code> <code>train_split_name</code> <code>Optional[str]</code> <p>The name of the training split.</p> <code>'train'</code> <code>test_split_name</code> <code>Optional[str]</code> <p>The name of the test split. Also used for <code>mgen predict</code>.</p> <code>'test'</code> <code>valid_split_name</code> <code>Optional[str]</code> <p>The name of the validation split.</p> <code>None</code> <code>train_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"train\" from these files. Not used unless referenced by the name \"train\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"test\" from these files. Not used unless referenced by the name \"test\" in one of the split_name arguments. Also used for <code>mgen predict</code>.</p> <code>None</code> <code>valid_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"valid\" from these files. Not used unless referenced by the name \"valid\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_size</code> <code>float</code> <p>The size of the test split. If test_split_name is None, creates a test split of this size from the training split.</p> <code>0.2</code> <code>valid_split_size</code> <code>float</code> <p>The size of the validation split. If valid_split_name is None, creates a validation split of this size from the training split.</p> <code>0.1</code> <code>random_seed</code> <code>int</code> <p>The random seed to use for splitting the data.</p> <code>42</code> <code>extra_reader_kwargs</code> <code>Optional[dict]</code> <p>Extra kwargs for dataset readers.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data.</p> <code>True</code> <code>sampler</code> <code>Optional[Sampler]</code> <p>The sampler to use.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>The number of workers to use for data loading.</p> <code>0</code> <code>collate_fn</code> <code>Optional[callable]</code> <p>The function to use for collating data.</p> <code>None</code> <code>pin_memory</code> <code>bool</code> <p>Whether to pin memory.</p> <code>True</code> <code>persistent_workers</code> <code>bool</code> <p>Whether to use persistent workers.</p> <code>False</code> <code>cv_num_folds</code> <code>int</code> <p>The number of cross-validation folds, disables cv when &lt;= 1.</p> <code>1</code> <code>cv_test_fold_id</code> <code>int</code> <p>The fold id to use for cross-validation evaluation.</p> <code>0</code> <code>cv_enable_val_fold</code> <code>bool</code> <p>Whether to enable a validation fold.</p> <code>True</code> <code>cv_replace_val_fold_as_test_fold</code> <code>bool</code> <p>Replace validation fold with test fold. Only used when cv_enable_val_fold is False.</p> <code>False</code> <code>cv_fold_id_col</code> <code>Optional[str]</code> <p>The column name containing the fold id from a pre-split dataset. Setting to None to enable automatic splitting.</p> <code>None</code> <code>cv_val_offset</code> <code>int</code> <p>The offset applied to cv_test_fold_id to determine val_fold_id.</p> <code>1</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/data/#modelgenerator.data.MLMDataModule","title":"<code>modelgenerator.data.MLMDataModule</code>","text":"<p>               Bases: <code>SequenceClassificationDataModule</code></p> <p>Data module for continuing pretraining on a masked language modeling task.</p> Note <p>Each sample includes a single sequence under key 'sequences' and a single target sequence under key 'target_sequences'</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> required <code>config_name</code> <code>Optional[str]</code> <p>The name of the HF dataset configuration. Affects how the dataset is loaded.</p> <code>None</code> <code>masking_rate</code> <code>float</code> <p>The masking rate. Defaults to 0.15.</p> <code>0.15</code> <code>x_col</code> <code>str</code> <p>The name of the column containing the sequences.</p> <code>'sequence'</code> <code>y_col</code> <code>str | List[str]</code> <p>The name of the column(s) containing the labels.</p> <code>'label'</code> <code>extra_cols</code> <code>List[str] | None</code> <p>Additional columns to include in the dataset.</p> <code>None</code> <code>extra_col_aliases</code> <code>List[str] | None</code> <p>The name of the columns to use as the alias for the extra columns.</p> <code>None</code> <code>class_filter</code> <code>int | List[int] | None</code> <p>Filter the dataset to only include samples with the specified class(es).</p> <code>None</code> <code>generate_uid</code> <code>bool</code> <p>Whether to generate a unique ID for each sample.</p> <code>False</code> <code>train_split_name</code> <code>Optional[str]</code> <p>The name of the training split.</p> <code>'train'</code> <code>test_split_name</code> <code>Optional[str]</code> <p>The name of the test split. Also used for <code>mgen predict</code>.</p> <code>'test'</code> <code>valid_split_name</code> <code>Optional[str]</code> <p>The name of the validation split.</p> <code>None</code> <code>train_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"train\" from these files. Not used unless referenced by the name \"train\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"test\" from these files. Not used unless referenced by the name \"test\" in one of the split_name arguments. Also used for <code>mgen predict</code>.</p> <code>None</code> <code>valid_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"valid\" from these files. Not used unless referenced by the name \"valid\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_size</code> <code>float</code> <p>The size of the test split. If test_split_name is None, creates a test split of this size from the training split.</p> <code>0.2</code> <code>valid_split_size</code> <code>float</code> <p>The size of the validation split. If valid_split_name is None, creates a validation split of this size from the training split.</p> <code>0.1</code> <code>random_seed</code> <code>int</code> <p>The random seed to use for splitting the data.</p> <code>42</code> <code>extra_reader_kwargs</code> <code>Optional[dict]</code> <p>Extra kwargs for dataset readers.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>The batch size.</p> <code>128</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data.</p> <code>True</code> <code>sampler</code> <code>Optional[Sampler]</code> <p>The sampler to use.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>The number of workers to use for data loading.</p> <code>0</code> <code>collate_fn</code> <code>Optional[callable]</code> <p>The function to use for collating data.</p> <code>None</code> <code>pin_memory</code> <code>bool</code> <p>Whether to pin memory.</p> <code>True</code> <code>persistent_workers</code> <code>bool</code> <p>Whether to use persistent workers.</p> <code>False</code> <code>cv_num_folds</code> <code>int</code> <p>The number of cross-validation folds, disables cv when &lt;= 1.</p> <code>1</code> <code>cv_test_fold_id</code> <code>int</code> <p>The fold id to use for cross-validation evaluation.</p> <code>0</code> <code>cv_enable_val_fold</code> <code>bool</code> <p>Whether to enable a validation fold.</p> <code>True</code> <code>cv_replace_val_fold_as_test_fold</code> <code>bool</code> <p>Replace validation fold with test fold. Only used when cv_enable_val_fold is False.</p> <code>False</code> <code>cv_fold_id_col</code> <code>Optional[str]</code> <p>The column name containing the fold id from a pre-split dataset. Setting to None to enable automatic splitting.</p> <code>None</code> <code>cv_val_offset</code> <code>int</code> <p>The offset applied to cv_test_fold_id to determine val_fold_id.</p> <code>1</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code>"},{"location":"api_reference/tasks/","title":"Tasks","text":"<p>Tasks define data and model usage. They provide a simple interface for swapping backbones, adapters, and data without any code changes, enabling rapid and reproducible experimentation. They are specified with the <code>--model</code> argument in the CLI or in the <code>model</code> section of a configuration file.</p> <p>Tasks automatically configure backbones and adapters for training with <code>mgen fit</code>, evaluation with <code>mgen test/validate</code>, and inference with <code>mgen predict</code>. They cover a range of use-cases for information extraction, domain adaptation, supervised prediction, generative modeling, and zero-shot applications.</p> <p>This reference overviews the available no-code tasks for finetuning and inference. If you would like to develop new tasks, see Experiment Design.</p> <pre><code># Example Task Configuration\nmodel:\n  class_path: SequenceClassification\n  init_args:\n    backbone:\n      class_path: aido_dna_7b\n      init_args:\n        use_peft: true\n        lora_r: 16\n        lora_alpha: 32\n        lora_dropout: 0.1  \n    adapter:\n      class_path: modelgenerator.adapters.MLPPoolAdapter\n      init_args:\n        pooling: mean_pooling\n        hidden_sizes: \n        - 512\n        - 256\n        bias: true\n        dropout: 0.1\n        dropout_in_middle: false\n    optimizer:\n      class_path: torch.optim.AdamW\n      init_args:\n        lr: 1e-4\n    lr_scheduler:\n      class_path: torch.optim.lr_scheduler.StepLR\n      init_args:\n        step_size: 1\n        gamma: 0.1\ndata:\n  ...\ntrainer:\n  ...\n</code></pre> <p>Note: Adapters and Backbones are typed as <code>Callables</code>, since some args are reserved to be automatically configured within the task. As a general rule, positional arguments are reserved while keyword arguments are free to use. For example, the backbone, adapter, optimizer, and lr_scheduler can be configured as</p>"},{"location":"api_reference/tasks/#extract","title":"Extract","text":""},{"location":"api_reference/tasks/#modelgenerator.tasks.Embed","title":"<code>modelgenerator.tasks.Embed</code>","text":"<p>               Bases: <code>TaskInterface</code></p> <p>Task for getting embeddings from a pretrained backbone. This task is used only for inference.</p> Note <p>Must be used with PredictionWriter. Embeddings are stored under \"predictions\".</p> <p>Parameters:</p> Name Type Description Default <code>backbone</code> <code>BackboneCallable</code> <p>A pretrained backbone from the modelgenerator library.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the parent class. <code>use_legacy_adapter=False</code> is always overridden.</p> <code>{}</code>"},{"location":"api_reference/tasks/#modelgenerator.tasks.Inference","title":"<code>modelgenerator.tasks.Inference</code>","text":"<p>               Bases: <code>MLM</code></p> <p>Task for performing inference with a pretrained backbone end-to-end, including the backbone's original adapter.</p> Note <p>Must be used with PredictionWriter. Model outputs are stored under \"predictions\".</p> <p>Parameters:</p> Name Type Description Default <code>backbone</code> <code>BackboneCallable</code> <p>A pretrained backbone from the modelgenerator library.</p> required <code>use_legacy_adapter</code> <code>bool</code> <p>Whether to use the adapter from the backbone (HF head support). Warning: This is not supported for all tasks and will be depreciated in the future.</p> <code>True</code> <code>**kwargs</code> <p>Additional arguments passed to the parent class.</p> <code>{}</code>"},{"location":"api_reference/tasks/#adapt","title":"Adapt","text":""},{"location":"api_reference/tasks/#modelgenerator.tasks.MLM","title":"<code>modelgenerator.tasks.MLM</code>","text":"<p>               Bases: <code>TaskInterface</code></p> <p>Task for performing masked language modeling (MLM) with a pretrained backbone. Can be used to train from scratch or for domain adaptation. Uses the MLMDataModule. Evaluates in terms of reconstruction accuracy on all tokens and cross-entropy loss.</p> <p>Parameters:</p> Name Type Description Default <code>backbone</code> <code>BackboneCallable</code> <p>A pretrained backbone from the modelgenerator library.</p> required <code>use_legacy_adapter</code> <code>bool</code> <p>Whether to use the adapter from the backbone (HF head support). Warning: This is not supported for all tasks and will be depreciated in the future.</p> <code>True</code> <code>**kwargs</code> <p>Additional arguments passed to the parent class.</p> <code>{}</code>"},{"location":"api_reference/tasks/#modelgenerator.tasks.ConditionalMLM","title":"<code>modelgenerator.tasks.ConditionalMLM</code>","text":"<p>               Bases: <code>TaskInterface</code></p> <p>Task for masked language modeling with extra condition inputs. Evaluates in terms of reconstruction accuracy on all tokens and cross-entropy loss.</p> <p>Parameters:</p> Name Type Description Default <code>backbone</code> <code>BackboneCallable</code> <p>A pretrained backbone from the modelgenerator library.</p> required <code>adapter</code> <code>Optional[Callable[[int, int, int, Module], ConditionalGenerationAdapter]]</code> <p>A ConditionalGenerationAdapter for the model.</p> <code>ConditionalLMAdapter</code> <code>use_legacy_adapter</code> <code>bool</code> <p>Whether to use the pre-trained legacy adapter within the conditional decoder.</p> <code>True</code> <code>condition_dim</code> <code>int</code> <p>The dimension of the condition.</p> <code>1</code> <code>**kwargs</code> <p>Additional arguments passed to the parent class.</p> <code>{}</code>"},{"location":"api_reference/tasks/#predict","title":"Predict","text":""},{"location":"api_reference/tasks/#modelgenerator.tasks.SequenceClassification","title":"<code>modelgenerator.tasks.SequenceClassification</code>","text":"<p>               Bases: <code>TaskInterface</code></p> <p>Task for fine-tuning a sequence model for classification. Evaluates in terms of accuracy, F1 score, Matthews correlation coefficient (MCC), and AUROC.</p> Note <p>Supports binary, multiclass, and binary multi-label classification tasks.</p> <p>Parameters:</p> Name Type Description Default <code>backbone</code> <code>BackboneCallable</code> <p>A pretrained backbone from the modelgenerator library.</p> required <code>adapter</code> <code>Optional[Callable[[int, int], SequenceAdapter]]</code> <p>A SequenceAdapter for the model.</p> <code>LinearCLSAdapter</code> <code>n_classes</code> <code>int</code> <p>The number of classes in the classification task.</p> <code>2</code> <code>multilabel</code> <code>bool</code> <p>Indicate whether multiple labels can be positive, turning this into a multi-way binary classification task.</p> <code>False</code> <code>**kwargs</code> <p>Additional arguments passed to the parent class.</p> <code>{}</code>"},{"location":"api_reference/tasks/#modelgenerator.tasks.TokenClassification","title":"<code>modelgenerator.tasks.TokenClassification</code>","text":"<p>               Bases: <code>SequenceClassification</code></p> <p>Task for fine-tuning a model for token-wise classification. Evaluates in terms of accuracy, F1 score, Matthews correlation coefficient (MCC), and AUROC.</p> <p>Parameters:</p> Name Type Description Default <code>backbone</code> <code>BackboneCallable</code> <p>A pretrained backbone from the modelgenerator library.</p> required <code>adapter</code> <code>Optional[Callable[[int, int], TokenAdapter]]</code> <p>A TokenAdapter for the model.</p> <code>LinearAdapter</code> <code>n_classes</code> <code>int</code> <p>The number of classes in the classification task.</p> <code>2</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class. <code>multilabel=False</code> is always overridden.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>legacy_adapter_type</code> <code>LegacyAdapterType</code> <p>The LegacyAdapterType.TOKEN_CLS legacy adapter.</p>"},{"location":"api_reference/tasks/#modelgenerator.tasks.PairwiseTokenClassification","title":"<code>modelgenerator.tasks.PairwiseTokenClassification</code>","text":"<p>               Bases: <code>SequenceClassification</code></p> <p>Task for fine-tuning a model for pairwise token classification. Evaluates in terms of accuracy, F1 score, Matthews correlation coefficient (MCC), AUROC, and top-k accuracy for k=2,5,10.</p> <p>Parameters:</p> Name Type Description Default <code>backbone</code> <code>BackboneCallable</code> <p>A pretrained backbone from the modelgenerator library.</p> required <code>adapter</code> <code>Optional[Callable[[int, int], TokenAdapter]]</code> <p>A TokenAdapter for the model.</p> <code>LinearAdapter</code> <code>adapter_dim_multiplier</code> <code>int</code> <p>The multiplier for the adapter dimension. Defaults to 2.</p> <code>2</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class. <code>n_classes=2</code> and <code>multilabel=False</code> are always overridden.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>legacy_adapter_type</code> <code>LegacyAdapterType</code> <p>The LegacyAdapterType.TOKEN_CLS legacy adapter.</p>"},{"location":"api_reference/tasks/#modelgenerator.tasks.MMSequenceRegression","title":"<code>modelgenerator.tasks.MMSequenceRegression</code>","text":"<p>               Bases: <code>TaskInterface</code></p> <p>Task for fine-tuning multiple models on single-/multi-task regression. Evaluates in terms of mean absolute error, mean squared error, R2 score, Pearson correlation, and Spearman correlation.</p> Note <p>Supports any combination of DNA, RNA and protein backbones</p> <p>Parameters:</p> Name Type Description Default <code>backbone</code> <code>BackboneCallable</code> <p>A pretrained backbone from the modelgenerator library.</p> required <code>backbone1</code> <code>BackboneCallable</code> <p>A second pretrained backbone from the modelgenerator library.</p> required <code>backbone2</code> <code>Optional[BackboneCallable]</code> <p>An optional third pretrained backbone from the modelgenerator library.</p> <code>None</code> <code>backbone_order</code> <code>list</code> <p>A list of data columns in order of the backbones. Defaults to [\"dna_seq\", \"rna_seq\", \"protein_seq\"].</p> <code>['dna_seq', 'rna_seq']</code> <code>adapter</code> <code>Optional[Callable[[int, int, int, int], FusionAdapter]]</code> <p>A callable that returns a FusionAdapter.</p> <code>MMFusionTokenAdapter</code> <code>num_outputs</code> <code>int</code> <p>The number of outputs for the regression task.</p> <code>1</code> <code>loss_func</code> <code>Callable[..., Module]</code> <p>A callable that returns a loss function.</p> <code>MSELoss</code> <code>**kwargs</code> <p>Additional arguments passed to the parent class.</p> <code>{}</code>"},{"location":"api_reference/tasks/#generate","title":"Generate","text":""},{"location":"api_reference/tasks/#modelgenerator.tasks.Diffusion","title":"<code>modelgenerator.tasks.Diffusion</code>","text":"<p>               Bases: <code>TaskInterface</code></p> <p>Masked Diffusion Language Modeling training and generation on sequences. Evaluates in terms of reconstruction accuracy on masked tokens and cross-entropy loss.</p> <p>Parameters:</p> Name Type Description Default <code>backbone</code> <code>BackboneCallable</code> <p>A pretrained backbone from the modelgenerator library.</p> required <code>adapter</code> <code>Optional[Callable[[int, int], TokenAdapter]]</code> <p>A TokenAdapter for the model.</p> <code>None</code> <code>use_legacy_adapter</code> <code>bool</code> <p>Whether to use the adapter from the backbone (HF head support). Warning: This is not supported for all tasks and will be depreciated in the future.</p> <code>True</code> <code>sample_seq</code> <code>bool</code> <p>Whether to sample tokens during denoising, instead of always using the most likely token.</p> <code>False</code> <code>num_denoise_steps</code> <code>int</code> <p>Granularity of the denoising process. Less steps makes fewer forward passes and denoises more aggressively.</p> <code>4</code> <code>sampling_temperature</code> <code>float</code> <p>The temperature for sampling tokens, if sample_seq is True.</p> <code>1.0</code> <code>normalize_posterior_weights</code> <code>bool</code> <p>Whether to normalize posterior weights. Experimental feature to help training stability.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Print while denoising (warning: fun to watch).</p> <code>False</code> <code>**kwargs</code> <p>Additional arguments passed to the parent class.</p> <code>{}</code>"},{"location":"api_reference/tasks/#modelgenerator.tasks.ConditionalDiffusion","title":"<code>modelgenerator.tasks.ConditionalDiffusion</code>","text":"<p>               Bases: <code>Diffusion</code></p> <p>Task for masked diffusion language modeling with extra condition inputs. Evaluates in terms of reconstruction accuracy on masked tokens and cross-entropy loss.</p> <p>Parameters:</p> Name Type Description Default <code>backbone</code> <code>BackboneCallable</code> <p>A pretrained backbone from the modelgenerator library.</p> required <code>adapter</code> <code>Optional[Callable[[int, int, int, Module], ConditionalGenerationAdapter]]</code> <p>A ConditionalGenerationAdapter for the model.</p> <code>ConditionalLMAdapter</code> <code>use_legacy_adapter</code> <code>bool</code> <p>Whether to use the pre-trained legacy adapter within the conditional decoder.</p> <code>True</code> <code>condition_dim</code> <code>int</code> <p>The dimension of the condition.</p> <code>1</code> <code>**kwargs</code> <p>Additional arguments passed to the parent class.</p> <code>{}</code>"},{"location":"api_reference/tasks/#zero-shot","title":"Zero-Shot","text":""},{"location":"api_reference/tasks/#modelgenerator.tasks.ZeroshotPredictionDiff","title":"<code>modelgenerator.tasks.ZeroshotPredictionDiff</code>","text":"<p>               Bases: <code>TaskInterface</code></p> <p>Task for zero-shot prediction with a languange model that produces token logits.    Computes the log-likelihood difference between probability of ref and alt at the mutated position.    Evaluates in terms of AUROC and AUPRC.</p> <p>Parameters:</p> Name Type Description Default <code>backbone</code> <code>BackboneCallable</code> <p>A pretrained backbone from the modelgenerator library.</p> required <code>**kwargs</code> <p>Additional keyword arguments for the parent class. <code>use_legacy_adapter=True</code> is always overridden.</p> <code>{}</code>"},{"location":"api_reference/tasks/#modelgenerator.tasks.ZeroshotPredictionDistance","title":"<code>modelgenerator.tasks.ZeroshotPredictionDistance</code>","text":"<p>               Bases: <code>TaskInterface</code></p> <p>Task for zero-shot prediction with a model that produces embeddings.    Computes the L1 and L2 distance between the reference and mutated sequence embeddings.    Evaluates in terms of AUROC and AUPRC of the embedding distance.</p> <p>Parameters:</p> Name Type Description Default <code>backbone</code> <code>BackboneCallable</code> <p>A pretrained backbone from the modelgenerator library.</p> required <code>all_hidden_states</code> <code>bool</code> <p>Whether to run the test on all available hidden layers. Defaults to False, only using the last layer.</p> <code>False</code> <code>shared_ref</code> <code>bool</code> <p>Whether to use a shared reference sequence to accelerate zero-shot computation. Uses a separate reference sequence for each mutated sequence by default.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class. <code>use_legacy_adapter=True</code> is always overridden.</p> <code>{}</code>"},{"location":"api_reference/trainer/","title":"Trainer","text":"<p>AIDO.ModelGenerator uses the LightningCLI for configuring runs with the PyTorch Lightning Trainer.  The entrypoint for the CLI is <code>mgen</code>, which can be used with the <code>fit</code>, <code>test</code>, <code>validate</code>, and <code>predict</code> commands and the <code>--model</code>, <code>--data</code>, and <code>--trainer</code> arguments and their sub-arguments.</p> <pre><code>mgen fit --model ConditionalDiffusion --model.backbone aido_dna_300m \\\n  --data ConditionalDiffusionDataModule --data.path \"genbio-ai/100m-random-promoters\" \\\n  --trainer.max_epochs 1 --trainer.accelerator auto --trainer.devices auto\n</code></pre> <p>For detailed information about the LightningCLI, see the LightningCLI documentation.</p> <pre><code># Example Trainer Configuration\ntrainer:\n  accelerator: auto\n  strategy: lightning.pytorch.strategies.DDPStrategy\n  devices: auto\n  num_nodes: 1\n  precision: bf16-mixed\n  logger: null\n  callbacks:\n  - class_path: lightning.pytorch.callbacks.ModelCheckpoint\n    init_args:\n      filename: best_val:{step}-{val_loss:.3f}-{train_loss:.3f}\n      monitor: val_loss\n      save_top_k: 1\n  fast_dev_run: false\n  max_epochs: 100\n  limit_val_batches: null\n  val_check_interval: null\n  check_val_every_n_epoch: 1\n  log_every_n_steps: 50\n  accumulate_grad_batches: 1\n  gradient_clip_val: 1\n  gradient_clip_algorithm: null\n  detect_anomaly: false\n  default_root_dir: logs\nmodel:\n  ...\ndata:\n  ...\n</code></pre>"},{"location":"experiment_design/","title":"Experiment Design","text":"<p>AIDO.ModelGenerator is designed to enable rapid and reproducible prototyping with four kinds of experiments in mind:</p> <ol> <li>Applying pre-trained foundation models to new data</li> <li>Developing new finetuning and inference tasks for foundation models</li> <li>Benchmarking foundation models and creating leaderboards</li> <li>Testing new architectures for finetuning performance</li> </ol> <p>while also scaling with hardware and integrating with larger data pipelines or research workflows.</p> <p>This section is a pocket guide on developing each of these types of experiments, outlining key interfaces and the minimal code required to get new experiments, models, or data up and running.</p>"},{"location":"experiment_design/#experiment-types","title":"Experiment Types","text":"<p>AIDO.ModelGenerator interfaces hide boilerplate and standardize training, evaluation, and prediction to enable a few common development goals.  If you want to</p> <ol> <li>Use a new dataset for finetuning or inference.<ol> <li>Load your data: If you want to apply an existing model or task to train, evaluate, or predict with your data (locally or from Hugging Face), this usually requires no code.</li> <li>Or add a dataset: If you have unusual data types, require more than simple loading, or want to develop a new loading pattern for a new task, implement a <code>DataInterface</code>, 2 methods usually &lt;10 lines.</li> </ol> </li> <li>Benchmark foundation models against each other and create leaderboards for your use-case.<ol> <li>Add a backbone: Implement a <code>BackboneInterface</code>, mostly one-liners.</li> </ol> </li> <li>Develop new finetuning or inference tasks that make use of foundation models (e.g. inverse folding, multi-modal fusion, diffusion).<ol> <li>Add a task: Implement a <code>TaskInterface</code>, 5 methods usually &lt;10 lines each</li> </ol> </li> <li>Test new architectures for finetuning performance.<ol> <li>Add an adapter: Implement a <code>nn.Module</code> for use with a finetuning task, 2 methods.</li> </ol> </li> </ol>"},{"location":"experiment_design/#codebase-structure","title":"Codebase Structure","text":"<p>AIDO.ModelGenerator is built on PyTorch Lightning for training and testing, Huggingface for model and data management, and LightningCLI for experiment configuration and organization.</p> <p>Most development will focus on implementing a simple interface in <code>backbones</code> <code>adapters</code> <code>tasks</code> or <code>data</code></p> <pre><code>pyproject.toml  # Installation and packaging\nDockerfile      # Containerization\nconfigs/        # Useful config files\nexperiments/    # Configs and scripts to reproduce manuscripts\ndocs/           # Website\nmodelgenerator/\n    # Main codebase\n*   backbones/  # Backbone models\n*   adapters/   # Finetuning adapter heads\n*   tasks/      # Model finetuning and usage\n*   data/       # Data loading and formatting\n    main.py     # mgen CLI entrypoint\n    # Supplementary files\n    callbacks.py      # Useful callbacks for saving inferences, etc\n    lr_schedulers.py  # Custom LR schedulers\n    metrics.py        # Custom metrics\n    # Task-specific submodules\n    rna_ss/                 # RNA secondary structure prediction\n    structure_tokenizer/    # Protein structure tokenization\n    rna_inv_fold/           # RNA inverse folding\n    prot_inv_fold/          # Protein inverse folding\n</code></pre>"},{"location":"experiment_design/backbones/","title":"Adding Backbones","text":"<p>Backbones are pre-trained foundation models. </p> <p>Foundation models are essential to modern ML but are often difficult to work with. Design decisions made during pre-training (tokenization, architecture, io format) cannot be changed. At best, this results in many reimplementations for benchmarking or finetuning tasks, and a high risk of buggy code. At worst, these decisions can lock in users and exclude certain tasks and use-cases.</p> <p>AIDO.ModelGenerator eliminates the need for reimplementation and makes backbones task-agnostic: wrap your backbone in a standard interface, and reuse it across all inference and finetuning tasks. It also makes compatibility transparent: if a backbone fits the required interface, it can be used for any data-appropriate task.</p> <p>Note: Backbones for 1D sequence modeling are univerally supported. Other types of backbones included in AIDO.ModelGenerator (e.g. structure, image) are not yet universally supported, but will be in the future.</p> <p>Available Backbones: </p> <ul> <li>DNA: <code>aido_dna_7b</code>, <code>aido_dna_300m</code>, <code>aido_dna_dummy</code>, <code>aido_dna_debug</code>, <code>dna_onehot</code></li> <li>RNA: <code>aido_rna_1b600m</code>, <code>aido_rna_1b600m_cds</code>, <code>aido_rna_650m</code>, <code>aido_rna_650m_cds</code>, <code>aido_rna_300m_mars</code>, <code>aido_rna_25m_mars</code>, <code>aido_rna_1m_mars</code>, <code>aido_dna_dummy</code>, <code>aido_dna_debug</code>, <code>dna_onehot</code></li> <li>Protein: <code>aido_protein_16b</code>, <code>aido_protein_16b_v1</code>, <code>aido_protein2structoken_16b</code>, <code>aido_protein_debug</code>, <code>protein_onehot</code>, <code>aido_protein_rag_16b</code>, <code>aido_protein_rag_3b</code></li> <li>Cell (gene expression): <code>aido_cell_100m</code>, <code>aido_cell_10m</code>, <code>aido_cell_3m</code></li> <li>OneHot: dummy model, only tokenizes, useful for non-FM baselines and quick tests</li> </ul> <p>At their core, backbones are PyTorch <code>nn.Module</code> objects with a few extra interfaces.  To implement a new backbone, subclass a backbone interface and implement the required methods.</p>"},{"location":"experiment_design/backbones/#modelgenerator.backbones.SequenceBackboneInterface","title":"<code>modelgenerator.backbones.SequenceBackboneInterface</code>","text":"<p>               Bases: <code>Module</code></p> <p>Interface class to ensure consistent implementation of essential methods for all backbones.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>The description is missing.</p> required <code>**kwargs</code> <p>The description is missing.</p> required <p>Attributes:</p> Name Type Description <code>fsdp_wrap_modules</code> <code>List[str]</code> <p>List of module paths to wrap when using distributed training with FSDP.</p> <code>model_path</code> <code>str</code> <p>Path to the model weights. May be HF.</p> Source code in <code>modelgenerator/backbones/base.py</code> <pre><code>class SequenceBackboneInterface(nn.Module, metaclass=GoogleDocstringInheritanceInitMeta):\n    \"\"\"Interface class to ensure consistent implementation of essential methods for all backbones.\n\n    Attributes:\n        fsdp_wrap_modules: List of module paths to wrap when using distributed training with FSDP.\n        model_path (str): Path to the model weights. May be HF.\n    \"\"\"\n\n    # import paths of modules to wrap when using FSDP\n    fsdp_wrap_modules: List[str] = []\n    model_path: str = \"\"\n\n    def forward(\n        self, input_ids: Tensor, attention_mask: Tensor, all_hidden_states: bool = False\n    ) -&gt; Union[Tensor, List[Tensor]]:\n        \"\"\"Defines the forward pass for the model.\n\n        Args:\n            input_ids (Tensor): Token IDs (n, seq_len).\n            attention_mask (Tensor): Attention mask (n, seq_len).\n            all_hidden_states (bool, optional): Whether to return all hidden states. Defaults to False.\n\n        Returns:\n            Union[Tensor, List[Tensor]]: Model output, typically the last hidden state or logits.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_decoder(self) -&gt; nn.Module:\n        \"\"\"Returns the decoder module for the model, if applicable.\n\n        Returns:\n            nn.Module: The decoder module.\n        \"\"\"\n        raise NotImplementedError\n\n    def tokenize(\n        self,\n        sequences: List[str],\n        padding: bool = True,\n        add_special_tokens: bool = True,\n        **kwargs,\n    ) -&gt; Tuple[Tensor, Tensor, Tensor]:\n        \"\"\"Tokenizes input sequences into input IDs and attention masks.\n\n        Args:\n            sequences (List[str]): List of input sequences.\n            padding (bool, optional): Whether to pad sequences. Defaults to True.\n            add_special_tokens (bool, optional): Whether to add special tokens. Defaults to True.\n\n        Returns:\n            dict: A dictionary containing input_ids.\n        \"\"\"\n        raise NotImplementedError\n\n    def decode_tokens(self, tokenized_sequences: Tensor) -&gt; List[str]:\n        \"\"\"Decodes tokenized sequences back to text.\n\n        Args:\n            tokenized_sequences (Tensor): Tokenized sequences.\n\n        Returns:\n            List[str]: Decoded text sequences.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_token_id(self, token: str) -&gt; int:\n        \"\"\"Gets the ID of a specific token.\n\n        Args:\n            token (str): The token to look up.\n\n        Returns:\n            int: Token ID.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_max_context(self) -&gt; int:\n        \"\"\"Gets the maximum context length of the model.\n\n        Returns:\n            int: Maximum context length.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_embedding_size(self) -&gt; int:\n        \"\"\"Gets the embedding size of the model.\n\n        Returns:\n            int: Embedding size.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_vocab_size(self) -&gt; int:\n        \"\"\"Gets the vocabulary size of the model.\n\n        Returns:\n            int: Vocabulary size.\n        \"\"\"\n        raise NotImplementedError\n\n    def on_save_checkpoint(self, checkpoint: dict):\n        \"\"\"Handles checkpoint saving logic for the model.\n\n        Args:\n            checkpoint (dict): The checkpoint dictionary.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_num_layer(self) -&gt; int:\n        \"\"\"Gets the number of layers in the model.\n\n        Returns:\n            int: Number of layers.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"experiment_design/data/","title":"Adding Data Loaders","text":"<p>AIDO.ModelGenerator uses Lightning DataModules for dataset management and loading. We also provide a few tools to make data management more convenient, and work with common file types out-of-the-box.</p> <p>AIDO.ModelGenerator provides a <code>DataInterface</code> class that hides boilerplate, along with a <code>HFDatasetLoaderMixin</code> that combines Lightning DataModule structure and HuggingFace Datasets convenience together to quickly load data from HuggingFace or common file formats (e.g. tsv, csv, json, etc). More convenient mixins and example usage are outlined below.</p> <p>Many common tasks and data loaders are already implemented in AIDO.ModelGenerator, and only require setting new paths to run with new data.  See the Data API Reference for all types of available data modules.</p>"},{"location":"experiment_design/data/#modelgenerator.data.DataInterface","title":"<code>modelgenerator.data.DataInterface</code>","text":"<p>               Bases: <code>LightningDataModule</code>, <code>KFoldMixin</code></p> <p>Base class for all data modules in this project. Handles the boilerplate of setting up data loaders.</p> Note <p>Subclasses must implement the setup method. All datasets should return a dictionary of data items. To use HF loading, add the HFDatasetLoaderMixin. For any task-specific behaviors, implement transformations using <code>torch.utils.data.Dataset</code> objects. See MLM for an example.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> required <code>config_name</code> <code>Optional[str]</code> <p>The name of the HF dataset configuration. Affects how the dataset is loaded.</p> <code>None</code> <code>train_split_name</code> <code>Optional[str]</code> <p>The name of the training split.</p> <code>'train'</code> <code>test_split_name</code> <code>Optional[str]</code> <p>The name of the test split. Also used for <code>mgen predict</code>.</p> <code>'test'</code> <code>valid_split_name</code> <code>Optional[str]</code> <p>The name of the validation split.</p> <code>None</code> <code>train_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"train\" from these files. Not used unless referenced by the name \"train\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"test\" from these files. Not used unless referenced by the name \"test\" in one of the split_name arguments. Also used for <code>mgen predict</code>.</p> <code>None</code> <code>valid_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"valid\" from these files. Not used unless referenced by the name \"valid\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_size</code> <code>float</code> <p>The size of the test split. If test_split_name is None, creates a test split of this size from the training split.</p> <code>0.2</code> <code>valid_split_size</code> <code>float</code> <p>The size of the validation split. If valid_split_name is None, creates a validation split of this size from the training split.</p> <code>0.1</code> <code>random_seed</code> <code>int</code> <p>The random seed to use for splitting the data.</p> <code>42</code> <code>extra_reader_kwargs</code> <code>Optional[dict]</code> <p>Extra kwargs for dataset readers.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>The batch size.</p> <code>128</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data.</p> <code>True</code> <code>sampler</code> <code>Optional[Sampler]</code> <p>The sampler to use.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>The number of workers to use for data loading.</p> <code>0</code> <code>collate_fn</code> <code>Optional[callable]</code> <p>The function to use for collating data.</p> <code>None</code> <code>pin_memory</code> <code>bool</code> <p>Whether to pin memory.</p> <code>True</code> <code>persistent_workers</code> <code>bool</code> <p>Whether to use persistent workers.</p> <code>False</code> <code>cv_num_folds</code> <code>int</code> <p>The number of cross-validation folds, disables cv when &lt;= 1.</p> <code>1</code> <code>cv_test_fold_id</code> <code>int</code> <p>The fold id to use for cross-validation evaluation.</p> <code>0</code> <code>cv_enable_val_fold</code> <code>bool</code> <p>Whether to enable a validation fold.</p> <code>True</code> <code>cv_replace_val_fold_as_test_fold</code> <code>bool</code> <p>Replace validation fold with test fold. Only used when cv_enable_val_fold is False.</p> <code>False</code> <code>cv_fold_id_col</code> <code>Optional[str]</code> <p>The column name containing the fold id from a pre-split dataset. Setting to None to enable automatic splitting.</p> <code>None</code> <code>cv_val_offset</code> <code>int</code> <p>The offset applied to cv_test_fold_id to determine val_fold_id.</p> <code>1</code> Source code in <code>modelgenerator/data/base.py</code> <pre><code>class DataInterface(pl.LightningDataModule, KFoldMixin, metaclass=GoogleDocstringInheritanceInitMeta):\n    \"\"\"Base class for all data modules in this project. Handles the boilerplate of setting up data loaders.\n\n    Note:\n        Subclasses must implement the setup method.\n        All datasets should return a dictionary of data items.\n        To use HF loading, add the HFDatasetLoaderMixin.\n        For any task-specific behaviors, implement transformations using `torch.utils.data.Dataset` objects.\n        See [MLM](./#modelgenerator.data.MLMDataModule) for an example.\n\n    Args:\n        path: Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier\n        config_name: The name of the HF dataset configuration.\n            Affects how the dataset is loaded.\n        train_split_name: The name of the training split.\n        test_split_name: The name of the test split. Also used for `mgen predict`.\n        valid_split_name: The name of the validation split.\n        train_split_files: Create a split called \"train\" from these files.\n            Not used unless referenced by the name \"train\" in one of the split_name arguments.\n        test_split_files: Create a split called \"test\" from these files.\n            Not used unless referenced by the name \"test\" in one of the split_name arguments.\n            Also used for `mgen predict`.\n        valid_split_files: Create a split called \"valid\" from these files.\n            Not used unless referenced by the name \"valid\" in one of the split_name arguments.\n        test_split_size: The size of the test split.\n           If test_split_name is None, creates a test split of this size from the training split.\n        valid_split_size: The size of the validation split.\n           If valid_split_name is None, creates a validation split of this size from the training split.\n        random_seed: The random seed to use for splitting the data.\n        extra_reader_kwargs: Extra kwargs for dataset readers.\n        batch_size: The batch size.\n        shuffle: Whether to shuffle the data.\n        sampler: The sampler to use.\n        num_workers: The number of workers to use for data loading.\n        collate_fn: The function to use for collating data.\n        pin_memory: Whether to pin memory.\n        persistent_workers: Whether to use persistent workers.\n        cv_num_folds: The number of cross-validation folds, disables cv when &lt;= 1.\n        cv_test_fold_id: The fold id to use for cross-validation evaluation.\n        cv_enable_val_fold: Whether to enable a validation fold.\n        cv_replace_val_fold_as_test_fold: Replace validation fold with test fold. Only used when cv_enable_val_fold is False.\n        cv_fold_id_col: The column name containing the fold id from a pre-split dataset. Setting to None to enable automatic splitting.\n        cv_val_offset: The offset applied to cv_test_fold_id to determine val_fold_id.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        config_name: Optional[str] = None,\n        train_split_name: Optional[str] = \"train\",\n        test_split_name: Optional[str] = \"test\",\n        valid_split_name: Optional[str] = None,\n        train_split_files: Optional[Union[str, List[str]]] = None,\n        test_split_files: Optional[Union[str, List[str]]] = None,\n        valid_split_files: Optional[Union[str, List[str]]] = None,\n        test_split_size: float = 0.2,\n        valid_split_size: float = 0.1,\n        random_seed: int = 42,\n        extra_reader_kwargs: Optional[dict] = None,\n        batch_size: int = 128,\n        shuffle: bool = True,\n        sampler: Optional[torch.utils.data.Sampler] = None,\n        num_workers: int = 0,\n        collate_fn: Optional[callable] = None,\n        pin_memory: bool = True,\n        persistent_workers: bool = False,\n        # TODO: cv params will be deprecated and will be handled by trainer directly\n        cv_num_folds: int = 1,\n        cv_test_fold_id: int = 0,\n        cv_enable_val_fold: bool = True,\n        cv_replace_val_fold_as_test_fold: bool = False,\n        cv_fold_id_col: Optional[str] = None,\n        cv_val_offset: int = 1,\n    ):\n        super().__init__()\n        if os.path.isfile(path):\n            raise ValueError(\n                \"Path must be a directory or a Huggingface dataset repo. \"\n                \"If you want to pass only one file, set the path to the directory \"\n                \"containing the file and set `*_split_files` to `[filename]`.\"\n            )\n        self.path = path\n        self.config_name = config_name\n        self.train_split_name = train_split_name\n        self.test_split_name = test_split_name\n        self.valid_split_name = valid_split_name\n        self.train_split_files = train_split_files\n        self.test_split_files = test_split_files\n        self.valid_split_files = valid_split_files\n        self.test_split_size = test_split_size\n        self.valid_split_size = valid_split_size\n        self.random_seed = random_seed\n        self.extra_reader_kwargs = extra_reader_kwargs or {}\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.sampler = sampler\n        self.num_workers = num_workers\n        self.collate_fn = collate_fn\n        self.pin_memory = pin_memory\n        self.persistent_workers = persistent_workers\n        self.cv_num_folds = cv_num_folds\n        self.cv_test_fold_id = cv_test_fold_id\n        self.cv_enable_val_fold = cv_enable_val_fold\n        self.cv_replace_val_fold_as_test_fold = cv_replace_val_fold_as_test_fold\n        self.cv_fold_id_col = cv_fold_id_col\n        self.cv_val_offset = cv_val_offset\n\n    def setup(self, stage: Optional[str] = None) -&gt; None:\n        \"\"\"Set up the data module. This method should be overridden by subclasses.\n\n        Args:\n            stage (Optional[str], optional): training, validation, or test if these need to be setup separately. Defaults to None.\n        \"\"\"\n        self.train_dataset = None\n        self.val_dataset = None\n        self.test_dataset = None\n\n    def train_dataloader(self) -&gt; DataLoader:\n        \"\"\"Get the training data loader\n\n        Returns:\n            DataLoader: The training data loader\n        \"\"\"\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            shuffle=self.shuffle,\n            sampler=self.sampler,\n            num_workers=self.num_workers,\n            collate_fn=self.collate_fn,\n            pin_memory=self.pin_memory,\n            persistent_workers=self.persistent_workers,\n        )\n\n    def val_dataloader(self) -&gt; DataLoader:\n        \"\"\"Get the validation data loader\n\n        Returns:\n            DataLoader: The validation data loader\n        \"\"\"\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            sampler=self.sampler,\n            num_workers=self.num_workers,\n            collate_fn=self.collate_fn,\n            pin_memory=self.pin_memory,\n            persistent_workers=self.persistent_workers,\n        )\n\n    def test_dataloader(self) -&gt; DataLoader:\n        \"\"\"Get the test data loader\n\n        Returns:\n            DataLoader: The test data loader\n        \"\"\"\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            sampler=self.sampler,\n            num_workers=self.num_workers,\n            collate_fn=self.collate_fn,\n            pin_memory=self.pin_memory,\n            persistent_workers=self.persistent_workers,\n        )\n\n    def predict_dataloader(self) -&gt; DataLoader:\n        \"\"\"Get the dataloader for predictions for the test set\n\n        Returns:\n            DataLoader: The predict data loader\n        \"\"\"\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            sampler=self.sampler,\n            num_workers=self.num_workers,\n            collate_fn=self.collate_fn,\n            pin_memory=self.pin_memory,\n            persistent_workers=self.persistent_workers,\n        )\n</code></pre>"},{"location":"experiment_design/data/#modelgenerator.data.DataInterface.setup","title":"<code>setup(stage=None)</code>","text":"<p>Set up the data module. This method should be overridden by subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>Optional[str]</code> <p>training, validation, or test if these need to be setup separately. Defaults to None.</p> <code>None</code> Source code in <code>modelgenerator/data/base.py</code> <pre><code>def setup(self, stage: Optional[str] = None) -&gt; None:\n    \"\"\"Set up the data module. This method should be overridden by subclasses.\n\n    Args:\n        stage (Optional[str], optional): training, validation, or test if these need to be setup separately. Defaults to None.\n    \"\"\"\n    self.train_dataset = None\n    self.val_dataset = None\n    self.test_dataset = None\n</code></pre>"},{"location":"experiment_design/data/#useful-mixins","title":"Useful Mixins","text":""},{"location":"experiment_design/data/#modelgenerator.data.HFDatasetLoaderMixin","title":"<code>modelgenerator.data.HFDatasetLoaderMixin</code>","text":"<p>Provides methods for loading datasets using the Huggingface datasets library.</p> Source code in <code>modelgenerator/data/base.py</code> <pre><code>class HFDatasetLoaderMixin:\n    \"\"\"Provides methods for loading datasets using the Huggingface datasets library.\"\"\"\n\n    def load_dataset(self, **kwargs) -&gt; Tuple[datasets.Dataset]:\n        split_names = [\n            self.train_split_name,\n            self.valid_split_name,\n            self.test_split_name,\n        ]\n        data_files = {}\n        if self.train_split_files:\n            data_files[\"train\"] = self.train_split_files\n        if self.valid_split_files:\n            data_files[\"valid\"] = self.valid_split_files\n        if self.test_split_files:\n            data_files[\"test\"] = self.test_split_files\n        splits = ()\n        for split_name in split_names:\n            if split_name is None:\n                splits += (None,)\n            else:\n                try:\n                    ds = load_dataset(\n                        self.path,\n                        data_files=None if not data_files else data_files,\n                        name=self.config_name,\n                        streaming=False,\n                        split=split_name,\n                        **kwargs,\n                    )\n                except ValueError as e:\n                    rank_zero_warn(\n                        f\"Could not load split='{split_name}': {e}. Setting to None. You may ignore if you are not using split '{split_name}'.\"\n                    )\n                    ds = None\n                splits += (ds,)\n        return splits\n\n    def load_and_split_dataset(self, **kwargs) -&gt; Tuple[datasets.Dataset]:\n        train_dataset, valid_dataset, test_dataset = self.load_dataset(**kwargs)\n        if train_dataset is not None:\n            if test_dataset is None and self.test_split_size &gt; 0:\n                rank_zero_info(\n                    f\"&gt; Randomly split {self.test_split_size} of train for testing, Random seed: {self.random_seed}\"\n                )\n                train_test_split = train_dataset.train_test_split(\n                    test_size=self.test_split_size, seed=self.random_seed\n                )\n                train_dataset = train_test_split[\"train\"]\n                test_dataset = train_test_split[\"test\"]\n            if valid_dataset is None and self.valid_split_size &gt; 0:\n                rank_zero_info(\n                    f\"&gt; Randomly split {self.valid_split_size} of train for validation. Random seed: {self.random_seed}\"\n                )\n                train_test_split = train_dataset.train_test_split(\n                    test_size=self.valid_split_size, seed=self.random_seed\n                )\n                train_dataset = train_test_split[\"train\"]\n                valid_dataset = train_test_split[\"test\"]\n        first_non_empty = train_dataset or valid_dataset or test_dataset\n        if first_non_empty is None:\n            raise ValueError(\"All splits are empty\")\n        # return empty datasets instead of None for easier handling\n        if train_dataset is None:\n            train_dataset = datasets.Dataset.from_dict(\n                {k: [] for k in first_non_empty.column_names}\n            )\n        if valid_dataset is None:\n            valid_dataset = datasets.Dataset.from_dict(\n                {k: [] for k in first_non_empty.column_names}\n            )\n        if test_dataset is None:\n            test_dataset = datasets.Dataset.from_dict(\n                {k: [] for k in first_non_empty.column_names}\n            )\n        return train_dataset, valid_dataset, test_dataset\n</code></pre>"},{"location":"experiment_design/data/#modelgenerator.data.KFoldMixin","title":"<code>modelgenerator.data.KFoldMixin</code>","text":"<p>Provides methods for splitting datasets into k-folds for cross-validation</p> Source code in <code>modelgenerator/data/base.py</code> <pre><code>class KFoldMixin:\n    \"\"\"Provides methods for splitting datasets into k-folds for cross-validation\"\"\"\n\n    def __init__(self):\n        self.cv_splits = None\n\n    def get_split_by_fold_id(\n        self, train_dataset, val_dataset, test_dataset, fold_id, val_idx_offset=1\n    ):\n        \"\"\"Split the dataset into training, validation, and test sets based on the fold id for test\"\"\"\n        if self.cv_num_folds &lt;= 1:\n            return train_dataset, val_dataset, test_dataset\n        if len(val_dataset) or len(test_dataset):\n            rank_zero_warn(\n                \"Redundant val or test splits are not expected and will be ignored during training. \"\n                \"Disable this warning by setting {test,val}_split_size=0 and {test,val}_split_name=None\"\n            )\n        if self.cv_fold_id_col is not None:\n            splits = self.read_kfold_split(train_dataset)\n        else:\n            splits = self.generate_kfold_split(len(train_dataset), self.cv_num_folds)\n        test_idx = splits[fold_id]\n        val_idx = (\n            splits[(fold_id + val_idx_offset) % self.cv_num_folds]\n            if self.cv_enable_val_fold\n            else []\n        )\n        if not self.cv_enable_val_fold and self.cv_replace_val_fold_as_test_fold:\n            val_idx = test_idx\n        train_idx = list(set(range(len(train_dataset))) - set(test_idx) - set(val_idx))\n        return (\n            train_dataset.select(train_idx),\n            train_dataset.select(val_idx),\n            train_dataset.select(test_idx),\n        )\n\n    def generate_kfold_split(\n        self, num_samples: int, num_folds: int, shuffle: bool = True\n    ):\n        \"\"\"Randomly split n_samples into n_splits folds and return list of fold_idx\n\n        Args:\n            num_samples (int): Number of samples in the data.\n            num_folds (Optional[int]): Number of folds for cross validation, must be &gt; 1. Defaults to 10.\n            shuffle (Optional[bool]): Whether to shuffle the data before splitting into batches. Defaults to True.\n\n        Returns:\n            list of list containing indices for each fold\n        \"\"\"\n        if self.cv_splits is not None:\n            return self.cv_splits\n        idx = np.arange(num_samples)\n        if shuffle:\n            np.random.seed(self.random_seed)\n            np.random.shuffle(idx)\n        fold_samples = num_samples // num_folds\n        kfold_split_idx = []\n        for k in range(num_folds - 1):\n            kfold_split_idx.append(\n                idx[k * fold_samples : (k + 1) * fold_samples].tolist()\n            )\n        kfold_split_idx.append(idx[(k + 1) * fold_samples :].tolist())\n        self.cv_splits = kfold_split_idx\n        return kfold_split_idx\n\n    def read_kfold_split(self, dataset: datasets.Dataset):\n        \"\"\"Read the fold ids from a pre-split dataset and return list of fold_idx\"\"\"\n        fold_ids = sorted(dataset.unique(self.cv_fold_id_col))\n        if list(range(self.cv_num_folds)) != fold_ids:\n            raise ValueError(f\"Fold ids {fold_ids} do not match the expected range\")\n        kfold_split_idx = []\n        for fold_id in fold_ids:\n            kfold_split_idx.append(\n                np.where(np.array(dataset[self.cv_fold_id_col], dtype=int) == fold_id)[0].tolist()\n            )\n        self.cv_splits = kfold_split_idx\n        return kfold_split_idx\n</code></pre>"},{"location":"experiment_design/data/#implementing-a-datamodule","title":"Implementing a DataModule","text":"<p>To transform datasets for task-specific behaviors (e.g. masking for masked language modeling), use <code>torch.utils.data.Dataset</code> objects to implement the transformation. Below is an example.</p>"},{"location":"experiment_design/data/#modelgenerator.data.MLMDataModule","title":"<code>modelgenerator.data.MLMDataModule</code>","text":"<p>               Bases: <code>SequenceClassificationDataModule</code></p> <p>Data module for continuing pretraining on a masked language modeling task.</p> Note <p>Each sample includes a single sequence under key 'sequences' and a single target sequence under key 'target_sequences'</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset, can be (1) a local path to a data folder or (2) a Huggingface dataset identifier</p> required <code>config_name</code> <code>Optional[str]</code> <p>The name of the HF dataset configuration. Affects how the dataset is loaded.</p> <code>None</code> <code>masking_rate</code> <code>float</code> <p>The masking rate. Defaults to 0.15.</p> <code>0.15</code> <code>x_col</code> <code>str</code> <p>The name of the column containing the sequences.</p> <code>'sequence'</code> <code>y_col</code> <code>str | List[str]</code> <p>The name of the column(s) containing the labels.</p> <code>'label'</code> <code>extra_cols</code> <code>List[str] | None</code> <p>Additional columns to include in the dataset.</p> <code>None</code> <code>extra_col_aliases</code> <code>List[str] | None</code> <p>The name of the columns to use as the alias for the extra columns.</p> <code>None</code> <code>class_filter</code> <code>int | List[int] | None</code> <p>Filter the dataset to only include samples with the specified class(es).</p> <code>None</code> <code>generate_uid</code> <code>bool</code> <p>Whether to generate a unique ID for each sample.</p> <code>False</code> <code>train_split_name</code> <code>Optional[str]</code> <p>The name of the training split.</p> <code>'train'</code> <code>test_split_name</code> <code>Optional[str]</code> <p>The name of the test split. Also used for <code>mgen predict</code>.</p> <code>'test'</code> <code>valid_split_name</code> <code>Optional[str]</code> <p>The name of the validation split.</p> <code>None</code> <code>train_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"train\" from these files. Not used unless referenced by the name \"train\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"test\" from these files. Not used unless referenced by the name \"test\" in one of the split_name arguments. Also used for <code>mgen predict</code>.</p> <code>None</code> <code>valid_split_files</code> <code>Optional[Union[str, List[str]]]</code> <p>Create a split called \"valid\" from these files. Not used unless referenced by the name \"valid\" in one of the split_name arguments.</p> <code>None</code> <code>test_split_size</code> <code>float</code> <p>The size of the test split. If test_split_name is None, creates a test split of this size from the training split.</p> <code>0.2</code> <code>valid_split_size</code> <code>float</code> <p>The size of the validation split. If valid_split_name is None, creates a validation split of this size from the training split.</p> <code>0.1</code> <code>random_seed</code> <code>int</code> <p>The random seed to use for splitting the data.</p> <code>42</code> <code>extra_reader_kwargs</code> <code>Optional[dict]</code> <p>Extra kwargs for dataset readers.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>The batch size.</p> <code>128</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data.</p> <code>True</code> <code>sampler</code> <code>Optional[Sampler]</code> <p>The sampler to use.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>The number of workers to use for data loading.</p> <code>0</code> <code>collate_fn</code> <code>Optional[callable]</code> <p>The function to use for collating data.</p> <code>None</code> <code>pin_memory</code> <code>bool</code> <p>Whether to pin memory.</p> <code>True</code> <code>persistent_workers</code> <code>bool</code> <p>Whether to use persistent workers.</p> <code>False</code> <code>cv_num_folds</code> <code>int</code> <p>The number of cross-validation folds, disables cv when &lt;= 1.</p> <code>1</code> <code>cv_test_fold_id</code> <code>int</code> <p>The fold id to use for cross-validation evaluation.</p> <code>0</code> <code>cv_enable_val_fold</code> <code>bool</code> <p>Whether to enable a validation fold.</p> <code>True</code> <code>cv_replace_val_fold_as_test_fold</code> <code>bool</code> <p>Replace validation fold with test fold. Only used when cv_enable_val_fold is False.</p> <code>False</code> <code>cv_fold_id_col</code> <code>Optional[str]</code> <p>The column name containing the fold id from a pre-split dataset. Setting to None to enable automatic splitting.</p> <code>None</code> <code>cv_val_offset</code> <code>int</code> <p>The offset applied to cv_test_fold_id to determine val_fold_id.</p> <code>1</code> <code>**kwargs</code> <p>Additional keyword arguments for the parent class.</p> <code>{}</code> Source code in <code>modelgenerator/data/data.py</code> <pre><code>class MLMDataModule(SequenceClassificationDataModule):\n    \"\"\"Data module for continuing pretraining on a masked language modeling task.\n\n    Note:\n        Each sample includes a single sequence under key 'sequences' and a single target sequence under key 'target_sequences'\n\n    Args:\n        masking_rate (float, optional): The masking rate. Defaults to 0.15.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        config_name: Optional[str] = None,\n        masking_rate: float = 0.15,\n        x_col: str = \"sequence\",\n        y_col: str | List[str] = \"label\",\n        extra_cols: List[str] | None = None,\n        extra_col_aliases: List[str] | None = None,\n        class_filter: int | List[int] | None = None,\n        generate_uid: bool = False,\n        train_split_name: Optional[str] = \"train\",\n        test_split_name: Optional[str] = \"test\",\n        valid_split_name: Optional[str] = None,\n        train_split_files: Optional[Union[str, List[str]]] = None,\n        test_split_files: Optional[Union[str, List[str]]] = None,\n        valid_split_files: Optional[Union[str, List[str]]] = None,\n        test_split_size: float = 0.2,\n        valid_split_size: float = 0.1,\n        random_seed: int = 42,\n        extra_reader_kwargs: Optional[dict] = None,\n        batch_size: int = 128,\n        shuffle: bool = True,\n        sampler: Optional[torch.utils.data.Sampler] = None,\n        num_workers: int = 0,\n        collate_fn: Optional[callable] = None,\n        pin_memory: bool = True,\n        persistent_workers: bool = False,\n        cv_num_folds: int = 1,\n        cv_test_fold_id: int = 0,\n        cv_enable_val_fold: bool = True,\n        cv_replace_val_fold_as_test_fold: bool = False,\n        cv_fold_id_col: Optional[str] = None,\n        cv_val_offset: int = 1,\n        **kwargs,\n    ):\n        super().__init__(\n            path=path,\n            config_name=config_name,\n            x_col=x_col,\n            y_col=y_col,\n            extra_cols=extra_cols,\n            extra_col_aliases=extra_col_aliases,\n            class_filter=class_filter,\n            generate_uid=generate_uid,\n            train_split_name=train_split_name,\n            test_split_name=test_split_name,\n            valid_split_name=valid_split_name,\n            train_split_files=train_split_files,\n            test_split_files=test_split_files,\n            valid_split_files=valid_split_files,\n            test_split_size=test_split_size,\n            valid_split_size=valid_split_size,\n            random_seed=random_seed,\n            extra_reader_kwargs=extra_reader_kwargs,\n            batch_size=batch_size,\n            shuffle=shuffle,\n            sampler=sampler,\n            num_workers=num_workers,\n            collate_fn=collate_fn,\n            pin_memory=pin_memory,\n            persistent_workers=persistent_workers,\n            cv_num_folds=cv_num_folds,\n            cv_test_fold_id=cv_test_fold_id,\n            cv_enable_val_fold=cv_enable_val_fold,\n            cv_replace_val_fold_as_test_fold=cv_replace_val_fold_as_test_fold,\n            cv_fold_id_col=cv_fold_id_col,\n            cv_val_offset=cv_val_offset,\n            **kwargs,\n        )\n        self.masking_rate = masking_rate\n\n    def setup(self, stage: Optional[str] = None):\n        super().setup(stage)\n        self.train_dataset = MLMDataset(\n            dataset=self.train_dataset,\n            masking_rate=self.masking_rate,\n        )\n        self.val_dataset = MLMDataset(\n            dataset=self.val_dataset,\n            masking_rate=self.masking_rate,\n        )\n        self.test_dataset = MLMDataset(\n            dataset=self.test_dataset,\n            masking_rate=self.masking_rate,\n        )\n</code></pre>"},{"location":"experiment_design/data/#modelgenerator.data.MLMDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Set up the data module by loading the whole datasets and splitting them into training, validation, and test sets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>Optional[str]</code> <p>training, validation, or test if these need to be setup separately. Defaults to None.</p> <code>None</code> Source code in <code>modelgenerator/data/data.py</code> <pre><code>def setup(self, stage: Optional[str] = None):\n    super().setup(stage)\n    self.train_dataset = MLMDataset(\n        dataset=self.train_dataset,\n        masking_rate=self.masking_rate,\n    )\n    self.val_dataset = MLMDataset(\n        dataset=self.val_dataset,\n        masking_rate=self.masking_rate,\n    )\n    self.test_dataset = MLMDataset(\n        dataset=self.test_dataset,\n        masking_rate=self.masking_rate,\n    )\n</code></pre>"},{"location":"experiment_design/data/#modelgenerator.data.MLMDataset","title":"<code>modelgenerator.data.MLMDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Masked Language Modeling Dataset</p> Note <p>Each sample includes a single sequence under key 'sequences' and a single target sequence under key 'target_sequences'</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset to mask</p> required <code>masking_rate</code> <code>float</code> <p>The masking rate</p> required Source code in <code>modelgenerator/data/data.py</code> <pre><code>class MLMDataset(Dataset):\n    \"\"\"Masked Language Modeling Dataset\n\n    Note:\n        Each sample includes a single sequence under key 'sequences' and a single target sequence under key 'target_sequences'\n\n    Args:\n        dataset (Dataset): The dataset to mask\n        masking_rate (float): The masking rate\n    \"\"\"\n\n    def __init__(self, dataset, masking_rate):\n        self.dataset = dataset\n        self.masking_rate = masking_rate\n\n    def get_masked_sample(self, seq_target, masking_rate):\n        \"\"\"\n        Mask a sequence with a given masking rate\n        \"\"\"\n        num_mask_tokens = max(1, int(len(seq_target) * masking_rate))\n        perm = torch.randperm(len(seq_target))\n        input_mask_indices = perm[:num_mask_tokens]\n        # Mask the input sequence\n        seq_input = replace_characters_at_indices(\n            s=seq_target, indices=input_mask_indices, replacement_char=\"[MASK]\"\n        )\n        return seq_input\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        data_dict = self.dataset[idx]\n        seq_target = data_dict[\"sequences\"]\n        seq_input = self.get_masked_sample(seq_target, self.masking_rate)\n        data_dict.update({\"sequences\": seq_input, \"target_sequences\": seq_target})\n        # prepend __empty__ to help pytorch lightning infer batch size\n        return {\"__empty__\": 0, **data_dict}\n</code></pre>"},{"location":"experiment_design/datafiles/","title":"Overriding Data Files","text":"<p>Applying a task or pre-trained model to train, evaluate, or predict with your data (locally or from Hugging Face) is straightforward. There are two requirements:</p> <ol> <li>You must know which Task and DataModule you are using. If you are using a finetuned model checkpoint, find this in the associated config.yaml.</li> <li>Your data must be in a format that the Hugging Face <code>load_dataset</code> can read (e.g. tsv, csv, json, etc).</li> </ol> <p>If these are satisified, congrats! You can immediately use your data with AIDO.ModelGenerator by overriding the data paths and files in the config.</p>"},{"location":"experiment_design/datafiles/#overriding-data-paths","title":"Overriding Data Paths","text":"<p>Suppose a model was trained with data from Hugging Face but now we want to predict or retrain with a local dataset.</p> <pre><code>my_dataset/\n  # Columns: `id`, `dna_sequence`, `expression`\n  my_train.tsv\n  my_test.tsv\n\n</code></pre> <p>The config.yaml <code>data</code> section might look like this:</p> <pre><code>data:\n  class_path: PromoterExpressionRegression\n  init_args:\n    path: genbio-ai/100m-random-promoters\n    train_split_files: train.tsv\n    test_split_files: test.tsv\n    x_col: sequence\n    y_col: label\n    val_split_size: 0.1\n</code></pre> <p>To re-run with your data, just override the necessary args</p> <pre><code>data:\n  class_path: PromoterExpressionRegression\n  init_args:\n    path: my_dataset\n    train_split_files: my_train.tsv\n    test_split_files: my_test.tsv\n    x_col: dna_sequence\n    y_col: expression\n    val_split_size: 0.1\n</code></pre> <p>If some of the overrides aren't immediately obvious from the config, you can go to the Data API Reference to find documentation for usage. Many datasets are convenience datasets, and aren't explicitly documented, so you'll need to look for their parent class in the Data API Reference.</p> <p>For example, a quick look at the codebase shows that <code>PromoterExpressionRegression</code> is a convenience subclass of <code>SequenceRegressionDataModule</code>. The above overrides are equivalent to</p> <pre><code>data:\n  class_path: SequenceRegressionDataModule\n  init_args:\n    path: my_dataset\n    train_split_files: my_train.tsv\n    test_split_files: my_test.tsv\n    x_col: dna_sequence\n    y_col: expression\n    val_split_size: 0.1\n</code></pre>"},{"location":"experiment_design/tasks/","title":"Adding Tasks","text":"<p>Tasks are use-cases for pre-trained foundation models.</p> <p>Pre-trained foundation models (FMs, backbones) improve performance across a wide range of ML tasks. However, tasks utilize FMs in very different ways, often requiring a unique reimplementation or adaptation for every backbone-task pair, a process that is time-consuming and error-prone. For FM-enabled research and development to be practical, modularity and reusability are essential.</p> <p>AIDO.ModelGenerator <code>tasks</code> enable rapid prototyping and experimentation through hot-swappable <code>backbone</code> and <code>adapter</code> components, which make use of standard interfaces. All of this is made possible by the PyTorch Lightning framework, which provides the LightningModule interface for hardware-agnostic training, evaluation, and prediction, as well as configified experiment management and extensive CLI support.</p> <p>Available Tasks:  Inference, MLM, SequenceClassification, TokenClassification, PairwiseTokenClassification, Diffusion, ConditionalDiffusion, SequenceRegression, Embed</p> <p>Note: Adapters and Backbones are typed as <code>Callables</code>, since some args are reserved to automatically configure the adapter with the backbone. Create an <code>AdapterCallable</code> signature for a task to specify which arguments are configurable, and which are reserved. </p>"},{"location":"experiment_design/tasks/#adding-adapters","title":"Adding Adapters","text":"<p>Adapters serve as a linker between a backbone's output and a task's objective function. </p> <p>They are simple <code>nn.Module</code> objects that use the backbone interface to configure their weights and forward pass. Their construction is handled within the task's <code>configure_model</code> method. Each task only tolerates a specific adapter type, which all adapters for that task must subclass. See the <code>SequenceAdapter</code> type and implemented <code>LinearCLSAdapter</code> for <code>SequenceRegression</code> as an example below.</p>"},{"location":"experiment_design/tasks/#modelgenerator.tasks.TaskInterface","title":"<code>modelgenerator.tasks.TaskInterface</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>Interface class to ensure consistent implementation of essential methods for all tasks.</p> Note <p>Tasks will usually take a backbone and adapter as arguments, but these are not strictly required. See SequenceRegression task for an succinct example implementation. Handles the boilerplate of setting up training, validation, and testing steps, as well as the optimizer and learning rate scheduler. Subclasses must implement the init, configure_model, collate, forward, and evaluate methods.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>OptimizerCallable</code> <p>The optimizer to use for training.</p> <code>AdamW</code> <code>lr_scheduler</code> <code>Optional[LRSchedulerCallable]</code> <p>The learning rate scheduler to use for training.</p> <code>None</code> <code>batch_size</code> <code>Optional[int]</code> <p>The batch size to use for training.</p> <code>None</code> <code>use_legacy_adapter</code> <code>bool</code> <p>Whether to use the adapter from the backbone (HF head support). Warning: This is not supported for all tasks and will be depreciated in the future.</p> <code>False</code> <code>strict_loading</code> <code>bool</code> <p>Whether to strictly load the model from the checkpoint. If False, replaces missing weights with pretrained weights. Should be enabled when loading a checkpoint from a run with <code>use_peft</code> and <code>save_peft_only</code>.</p> <code>True</code> <code>reset_optimizer_states</code> <code>bool</code> <p>Whether to reset the optimizer states. Set it to True if you want to replace the adapter (e.g. for continued pretraining).</p> <code>False</code> <code>**kwargs</code> <p>Additional arguments passed to the parent class.</p> <code>{}</code> Source code in <code>modelgenerator/tasks/base.py</code> <pre><code>class TaskInterface(pl.LightningModule, metaclass=GoogleDocstringInheritanceInitMeta):\n    \"\"\"Interface class to ensure consistent implementation of essential methods for all tasks.\n\n    Note:\n        Tasks will usually take a backbone and adapter as arguments, but these are not strictly required.\n        See [SequenceRegression](./#modelgenerator.tasks.SequenceRegression) task for an succinct example implementation.\n        Handles the boilerplate of setting up training, validation, and testing steps,\n        as well as the optimizer and learning rate scheduler. Subclasses must implement\n        the __init__, configure_model, collate, forward, and evaluate methods.\n\n    Args:\n        use_legacy_adapter: Whether to use the adapter from the backbone (HF head support). \n            **Warning**: This is not supported for all tasks and will be depreciated in the future.\n        strict_loading: Whether to strictly load the model from the checkpoint. \n            If False, replaces missing weights with pretrained weights. \n            Should be enabled when loading a checkpoint from a run with `use_peft` and `save_peft_only`.\n        batch_size: The batch size to use for training.\n        optimizer: The optimizer to use for training.\n        reset_optimizer_states: Whether to reset the optimizer states.\n            Set it to True if you want to replace the adapter (e.g. for continued pretraining).\n        lr_scheduler: The learning rate scheduler to use for training.\n        **kwargs: Additional arguments passed to the parent class.\n    \"\"\"\n\n    def __init__(\n        self,\n        optimizer: OptimizerCallable = torch.optim.AdamW,\n        lr_scheduler: Optional[LRSchedulerCallable] = None,\n        batch_size: Optional[int] = None,\n        use_legacy_adapter: bool = False,\n        strict_loading: bool = True,\n        reset_optimizer_states: bool = False,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        # NOTE: A very explicit way of preventing unwanted hparams from being\n        # saved due to inheritance. All subclasses should include the\n        # following condition under super().__init__().\n        # Converting it to a reusable method could work but it would rely\n        # on the implementation detail of save_hyperparameters() walking up\n        # the call stack, which can change at any time.\n        if self.__class__ is TaskInterface:\n            self.save_hyperparameters()\n        self.optimizer = optimizer\n        self.lr_scheduler = lr_scheduler\n        self.batch_size = batch_size\n        self.use_legacy_adapter = use_legacy_adapter\n        self.metrics = nn.ModuleDict(\n            {\n                \"train_metrics\": nn.ModuleDict(),\n                \"val_metrics\": nn.ModuleDict(),\n                \"test_metrics\": nn.ModuleDict(),\n            }\n        )\n        self.metrics_to_pbar: Set[str] = {}\n        self.strict_loading = strict_loading\n        self.reset_optimizer_states = reset_optimizer_states\n\n    def configure_model(self) -&gt; None:\n        \"\"\"Configures the model for training and interence. Subclasses must implement this method.\"\"\"\n        raise NotImplementedError\n\n    def transform(\n        self, batch: dict[str, Union[list, Tensor]], batch_idx: int\n    ) -&gt; dict[str, Union[list, Tensor]]:\n        \"\"\"Collates and tokenizes a batch of data into a format that can be passed to the forward and evaluate methods. Subclasses must implement this method.\n\n        Note:\n            Tokenization is handled here using the backbone interface.\n            Tensor typing and device moving should be handled here.\n\n        Args:\n            batch (dict[str, Union[list, Tensor]]): A batch of data from the DataLoader\n            batch_idx (int): The index of the current batch in the DataLoader\n\n        Returns:\n            dict[str, Union[list, Tensor]]: The collated batch\n        \"\"\"\n        raise NotImplementedError\n\n    def forward(self, collated_batch: dict[str, Union[list, Tensor]]) -&gt; Tensor:\n        \"\"\"Runs a forward pass of the model on the collated batch of data. Subclasses must implement this method.\n\n        Args:\n            collated_batch (dict[str, Union[list, Tensor]]): The collated batch of data from collate.\n\n        Returns:\n            Tensor: The model predictions\n        \"\"\"\n        raise NotImplementedError\n\n    def evaluate(\n        self,\n        preds: Tensor,\n        collated_batch: dict[str, Union[list, Tensor]],\n        stage: Optional[Literal[\"train\", \"val\", \"test\"]] = None,\n        loss_only: bool = False,\n    ) -&gt; dict[str, Union[Tensor, float]]:\n        \"\"\"Calculate loss and update metrics states. Subclasses must implement this method.\n\n        Args:\n            preds (Tensor): The model predictions from forward.\n            collated_batch (dict[str, Union[list, Tensor]]): The collated batch of data from collate.\n            stage (str, optional): The stage of training (train, val, test). Defaults to None.\n            loss_only (bool, optional): If true, only update loss metric. Defaults to False.\n\n        Returns:\n            dict[str, Union[Tensor, float]]: The loss and any additional metrics.\n        \"\"\"\n        raise NotImplementedError\n\n    def configure_optimizers(self):\n        \"\"\"Configures the optimizer and learning rate scheduler for training.\n\n        Returns:\n            list: A list of optimizers and learning rate schedulers\n        \"\"\"\n        config = {\n            \"optimizer\": self.optimizer(\n                filter(lambda p: p.requires_grad, self.parameters())\n            )\n        }\n        if self.lr_scheduler is not None:\n            scheduler = self.lr_scheduler(config[\"optimizer\"])\n            if isinstance(scheduler, LazyLRScheduler):\n                scheduler.initialize(self.trainer)\n            config[\"lr_scheduler\"] = {\n                \"scheduler\": scheduler,\n                \"interval\": \"step\",\n                \"monitor\": \"train_loss\",  # Only used for torch.optim.lr_scheduler.ReduceLROnPlateau\n            }\n        return config\n\n    def on_save_checkpoint(self, checkpoint: dict):\n        if hasattr(self.backbone, \"on_save_checkpoint\"):\n            self.backbone.on_save_checkpoint(checkpoint)\n\n    def on_load_checkpoint(self, checkpoint: dict):\n        if self.reset_optimizer_states:\n            checkpoint[\"optimizer_states\"] = {}\n            checkpoint[\"lr_schedulers\"] = {}\n\n    def training_step(\n        self, batch: dict[str, Union[list, Tensor]], batch_idx: Optional[int] = None\n    ) -&gt; Tensor:\n        \"\"\"Runs a training step on a batch of data. Calls collate, forward, and evaluate methods in order.\n\n        Args:\n            batch (dict[str, Union[list, Tensor]]): A batch of data from the DataLoader\n            batch_idx (int, optional): The index of the current batch in the DataLoader\n\n        Returns:\n            Tensor: The loss from the training step\n        \"\"\"\n        collated_batch = self.transform(batch, batch_idx)\n        preds = self.forward(collated_batch)\n        outputs = self.evaluate(preds, collated_batch, \"train\", loss_only=False)\n        self.log_loss_and_metrics(outputs[\"loss\"], \"train\")\n        return outputs\n\n    def validation_step(\n        self, batch: dict[str, Union[list, Tensor]], batch_idx: Optional[int] = None\n    ) -&gt; Tensor:\n        \"\"\"Runs a validation step on a batch of data. Calls collate, forward, and evaluate methods in order.\n\n        Args:\n            batch (dict[str, Union[list, Tensor]]): A batch of data from the DataLoader\n            batch_idx (int, optional): The index of the current batch in the DataLoader\n\n        Returns:\n            Tensor: The loss from the validation step\n        \"\"\"\n        collated_batch = self.transform(batch, batch_idx)\n        preds = self.forward(collated_batch)\n        outputs = self.evaluate(preds, collated_batch, \"val\", loss_only=False)\n        self.log_loss_and_metrics(outputs[\"loss\"], \"val\")\n        return {\"predictions\": preds, **batch, **collated_batch, **outputs}\n\n    def test_step(\n        self, batch: dict[str, Union[list, Tensor]], batch_idx: Optional[int] = None\n    ) -&gt; Tensor:\n        \"\"\"Runs a test step on a batch of data. Calls collate, forward, and evaluate methods in order.\n\n        Args:\n            batch (dict[str, Union[list, Tensor]]): A batch of data from the DataLoader\n            batch_idx (int, optional): The index of the current batch in the DataLoader\n\n        Returns:\n            Tensor: The loss from the test step\n        \"\"\"\n        collated_batch = self.transform(batch, batch_idx)\n        preds = self.forward(collated_batch)\n        outputs = self.evaluate(preds, collated_batch, \"test\", loss_only=False)\n        self.log_loss_and_metrics(outputs[\"loss\"], \"test\")\n        return {\"predictions\": preds, **batch, **collated_batch, **outputs}\n\n    def predict_step(\n        self, batch: dict[str, Union[list, Tensor]], batch_idx: Optional[int] = None\n    ) -&gt; dict[str, Union[list, Tensor]]:\n        \"\"\"Infers predictions from a batch of data. Calls collate and forward methods in order.\n\n        Args:\n            batch (dict[str, Union[list, Tensor]]): A batch of data from the DataLoader\n            batch_idx (int, optional): The index of the current batch in the DataLoader\n\n        Returns:\n            dict[str, Union[list, Tensor]]: The predictions from the model along with the collated batch.\n        \"\"\"\n        collated_batch = self.transform(batch, batch_idx)\n        preds = self.forward(collated_batch)\n        return {\"predictions\": preds, **batch, **collated_batch}\n\n    def get_metrics_by_stage(\n        self, stage: Literal[\"train\", \"val\", \"test\"]\n    ) -&gt; nn.ModuleDict:\n        \"\"\"Returns the metrics dict for a given stage.\n\n        Args:\n            stage (str): The stage of training (train, val, test)\n\n        Returns:\n            nn.ModuleDict: The metrics for the given stage\n        \"\"\"\n        try:\n            return self.metrics[f\"{stage}_metrics\"]\n        except KeyError:\n            raise ValueError(\n                f\"Stage must be one of 'train', 'val', or 'test'. Got {stage}\"\n            )\n\n    def log_loss_and_metrics(\n        self, loss: Tensor, stage: Literal[\"train\", \"val\", \"test\"]\n    ) -&gt; None:\n        \"\"\"Logs the loss and metrics for a given stage.\n\n        Args:\n            loss (Tensor): The loss from the training, validation, or testing step\n            stage (str): The stage of training (train, val, test)\n        \"\"\"\n        self.log(f\"{stage}_loss\", loss, prog_bar=True, sync_dist=stage != \"train\")\n        for k, v in self.metrics[f\"{stage}_metrics\"].items():\n            self.log(f\"{stage}_{k}\", v, prog_bar=k in self.metrics_to_pbar)\n\n    def call_or_update_metric(\n        self, stage: Literal[\"train\", \"val\", \"test\"], metric: tm.Metric, *args, **kwargs\n    ):\n        if stage == \"train\":\n            # in addition to .update(), metric.__call__ also .compute() the metric\n            # for the current batch. However, .compute() may fail if data is insufficient.\n            try:\n                metric(*args, **kwargs)\n            except ValueError:\n                metric.update(*args, **kwargs)\n        else:\n            # update only since per step metrics are not logged in val and test stages\n            metric.update(*args, **kwargs)\n\n    @classmethod\n    def from_config(cls, config: dict) -&gt; \"TaskInterface\":\n        \"\"\"Creates a task model from a configuration dictionary\n\n        Args:\n            config (Dict[str, Any]): Configuration dictionary\n\n        Returns:\n            TaskInterface: Task model\n        \"\"\"\n        parser = ArgumentParser()\n        parser.add_class_arguments(cls, \"model\")\n        init = parser.instantiate_classes(parser.parse_object(config))\n        init.model.configure_model()\n        return init.model\n</code></pre>"},{"location":"experiment_design/tasks/#modelgenerator.tasks.TaskInterface.configure_model","title":"<code>configure_model()</code>","text":"<p>Configures the model for training and interence. Subclasses must implement this method.</p> Source code in <code>modelgenerator/tasks/base.py</code> <pre><code>def configure_model(self) -&gt; None:\n    \"\"\"Configures the model for training and interence. Subclasses must implement this method.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"experiment_design/tasks/#modelgenerator.tasks.TaskInterface.forward","title":"<code>forward(collated_batch)</code>","text":"<p>Runs a forward pass of the model on the collated batch of data. Subclasses must implement this method.</p> <p>Parameters:</p> Name Type Description Default <code>collated_batch</code> <code>dict[str, Union[list, Tensor]]</code> <p>The collated batch of data from collate.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The model predictions</p> Source code in <code>modelgenerator/tasks/base.py</code> <pre><code>def forward(self, collated_batch: dict[str, Union[list, Tensor]]) -&gt; Tensor:\n    \"\"\"Runs a forward pass of the model on the collated batch of data. Subclasses must implement this method.\n\n    Args:\n        collated_batch (dict[str, Union[list, Tensor]]): The collated batch of data from collate.\n\n    Returns:\n        Tensor: The model predictions\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"experiment_design/tasks/#modelgenerator.tasks.TaskInterface.evaluate","title":"<code>evaluate(preds, collated_batch, stage=None, loss_only=False)</code>","text":"<p>Calculate loss and update metrics states. Subclasses must implement this method.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>Tensor</code> <p>The model predictions from forward.</p> required <code>collated_batch</code> <code>dict[str, Union[list, Tensor]]</code> <p>The collated batch of data from collate.</p> required <code>stage</code> <code>str</code> <p>The stage of training (train, val, test). Defaults to None.</p> <code>None</code> <code>loss_only</code> <code>bool</code> <p>If true, only update loss metric. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Union[Tensor, float]]</code> <p>dict[str, Union[Tensor, float]]: The loss and any additional metrics.</p> Source code in <code>modelgenerator/tasks/base.py</code> <pre><code>def evaluate(\n    self,\n    preds: Tensor,\n    collated_batch: dict[str, Union[list, Tensor]],\n    stage: Optional[Literal[\"train\", \"val\", \"test\"]] = None,\n    loss_only: bool = False,\n) -&gt; dict[str, Union[Tensor, float]]:\n    \"\"\"Calculate loss and update metrics states. Subclasses must implement this method.\n\n    Args:\n        preds (Tensor): The model predictions from forward.\n        collated_batch (dict[str, Union[list, Tensor]]): The collated batch of data from collate.\n        stage (str, optional): The stage of training (train, val, test). Defaults to None.\n        loss_only (bool, optional): If true, only update loss metric. Defaults to False.\n\n    Returns:\n        dict[str, Union[Tensor, float]]: The loss and any additional metrics.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"experiment_design/tasks/#examples","title":"Examples","text":""},{"location":"experiment_design/tasks/#modelgenerator.tasks.SequenceRegression","title":"<code>modelgenerator.tasks.SequenceRegression</code>","text":"<p>               Bases: <code>TaskInterface</code></p> <p>Task for fine-tuning a sequence model for single-/multi-task regression. Evaluates in terms of mean absolute error, mean squared error, R2 score, Pearson correlation, and Spearman correlation.</p> <p>Parameters:</p> Name Type Description Default <code>backbone</code> <code>BackboneCallable</code> <p>A pretrained backbone from the modelgenerator library.</p> required <code>adapter</code> <code>Optional[Callable[[int, int], SequenceAdapter]]</code> <p>A SequenceAdapter for the model.</p> <code>LinearCLSAdapter</code> <code>num_outputs</code> <code>int</code> <p>The number of outputs for the regression task.</p> <code>1</code> <code>loss_func</code> <code>Callable[..., Module]</code> <p>The loss function to use for training.</p> <code>MSELoss</code> <code>log_grad_norm_step</code> <code>int</code> <p>The step interval for logging gradient norms.</p> <code>0</code> <code>**kwargs</code> <p>Additional arguments passed to the parent class.</p> <code>{}</code> Source code in <code>modelgenerator/tasks/tasks.py</code> <pre><code>class SequenceRegression(TaskInterface):\n    \"\"\"Task for fine-tuning a sequence model for single-/multi-task regression.\n    Evaluates in terms of mean absolute error, mean squared error, R2 score, Pearson correlation, and Spearman correlation.\n\n    Args:\n        backbone: A pretrained backbone from the modelgenerator library.\n        adapter: A SequenceAdapter for the model.\n        num_outputs: The number of outputs for the regression task.\n        loss_func: The loss function to use for training.\n        log_grad_norm_step: The step interval for logging gradient norms.\n    \"\"\"\n\n    def __init__(\n        self,\n        backbone: BackboneCallable,\n        adapter: Optional[Callable[[int, int], SequenceAdapter]] = LinearCLSAdapter,\n        num_outputs: int = 1,\n        loss_func: Callable[..., torch.nn.Module] = torch.nn.MSELoss,\n        log_grad_norm_step: int = 0,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        if self.__class__ is SequenceRegression:\n            self.save_hyperparameters()\n        self.backbone_fn = backbone\n        self.adapter_fn = adapter\n        self.num_outputs = num_outputs\n        self.backbone = None\n        self.adapter = None\n        self.loss = loss_func()\n        self.log_grad_norm_step = log_grad_norm_step\n        for stage in [\"train\", \"val\", \"test\"]:\n            self.metrics[f\"{stage}_metrics\"] = nn.ModuleDict(\n                {\n                    \"pearson\": PearsonCorrCoef(\n                        num_outputs=num_outputs, multioutput=\"uniform_average\"\n                    ),\n                    \"spearman\": SpearmanCorrCoef(\n                        num_outputs=num_outputs, multioutput=\"uniform_average\"\n                    ),\n                    \"mae\": MeanAbsoluteError(\n                        num_outputs=num_outputs, multioutput=\"uniform_average\"\n                    ),\n                    \"r2\": tm.R2Score(multioutput=\"uniform_average\"),\n                    \"mse\": MeanSquaredError(\n                        num_outputs=num_outputs, multioutput=\"uniform_average\"\n                    ),\n                }\n            )\n            if stage == \"test\" and self.num_outputs &gt; 1:\n                # calculate scores for each task\n                label_wise_spearman = nn.ModuleDict(\n                        {\n                            \"spearman_\" + str(i): SpearmanCorrCoef(num_outputs=1)\n                            for i in range(self.num_outputs)\n                        }\n                    )\n                label_wise_pearson = nn.ModuleDict(\n                        {\n                            \"pearson_\" + str(i): PearsonCorrCoef(num_outputs=1)\n                            for i in range(self.num_outputs)\n                        }\n                    )\n                label_wise_r2 = nn.ModuleDict(\n                        {\n                            \"r2_\" + str(i): tm.R2Score()\n                            for i in range(self.num_outputs)\n                        }\n                    )\n                label_wise_mse = nn.ModuleDict(\n                        {\n                            \"mse_\" + str(i): MeanSquaredError(num_outputs=1)\n                            for i in range(self.num_outputs)\n                        }\n                    )\n                label_wise_mae = nn.ModuleDict(\n                        {\n                            \"mae_\" + str(i): MeanAbsoluteError(num_outputs=1)\n                            for i in range(self.num_outputs)\n                        }\n                    )\n                self.metrics[f\"{stage}_metrics\"].update(label_wise_spearman)\n                self.metrics[f\"{stage}_metrics\"].update(label_wise_pearson)\n                self.metrics[f\"{stage}_metrics\"].update(label_wise_r2)\n                self.metrics[f\"{stage}_metrics\"].update(label_wise_mse)\n                self.metrics[f\"{stage}_metrics\"].update(label_wise_mae)\n        self.metrics_to_pbar = set(self.metrics[\"train_metrics\"].keys())\n\n    def configure_model(self) -&gt; None:\n        if self.backbone is not None:\n            return\n        if self.use_legacy_adapter:\n            self.backbone = self.backbone_fn(\n                LegacyAdapterType.SEQ_CLS,\n                DefaultConfig(\n                    config_overwrites={\n                        \"problem_type\": \"regression\",\n                        \"num_labels\": self.num_outputs,\n                    }\n                ),\n            )\n            self.adapter = self.backbone.get_decoder()\n        else:\n            self.backbone = self.backbone_fn(None, None)\n            self.adapter = self.adapter_fn(\n                self.backbone.get_embedding_size(), self.num_outputs\n            )\n\n    def transform(\n        self, batch: dict[str, Union[list, Tensor]], batch_idx: Optional[int] = None\n    ) -&gt; dict[str, Union[list, Tensor]]:\n        \"\"\"Collates a batch of data into a format that can be passed to the forward and evaluate methods.\n\n        Args:\n            batch (dict[str, Union[list, Tensor]]): A batch of data containing sequences and labels\n            batch_idx (int, optional): The index of the current batch in the DataLoader\n\n        Returns:\n            dict[str, Union[list, Tensor]]: The collated batch containing sequences, input_ids, attention_mask, and labels\n        \"\"\"\n\n        sequences = batch.pop(\"sequences\")\n        tokenized_result = self.backbone.tokenize(sequences, **batch)\n        input_ids = tokenized_result.pop(\"input_ids\", None)\n        attention_mask = tokenized_result.pop(\"attention_mask\", None)\n        special_tokens_mask = tokenized_result.pop(\"special_tokens_mask\", None)\n\n        input_ids = torch.tensor(input_ids, dtype=torch.long).to(self.device)\n        if attention_mask is not None:\n            attention_mask = torch.tensor(attention_mask, dtype=torch.long).to(self.device)\n        labels = None\n        if batch.get(\"labels\") is not None:\n            labels = batch[\"labels\"].to(self.device, dtype=self.dtype)\n        return {\n            \"sequences\": sequences,\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"special_tokens_mask\": special_tokens_mask,\n            \"labels\": labels,\n            **tokenized_result,\n        }\n\n    def forward(self, collated_batch: dict[str, Union[list, Tensor]]) -&gt; Tensor:\n        \"\"\"Runs a forward pass of the model.\n\n        Args:\n            collated_batch (dict[str, Union[list, Tensor]]): A collated batch of data containing input_ids and attention_mask.\n\n        Returns:\n            Tensor: The regression predictions\n        \"\"\"\n\n        hidden_states = self.backbone(**collated_batch)  # (bs, seq_len, dim)\n        preds = self.adapter(hidden_states, collated_batch[\"attention_mask\"])\n        return preds\n\n    def evaluate(\n        self,\n        preds: Tensor,\n        collated_batch: dict[str, Union[list, Tensor]],\n        stage: Optional[Literal[\"train\", \"val\", \"test\"]] = None,\n        loss_only: bool = False,\n    ) -&gt; dict[str, Union[Tensor, float]]:\n        \"\"\"Evaluates the model predictions against the ground truth labels.\n\n        Args:\n            logits (Tensor): The model predictions\n            collated_batch (dict[str, Union[list, Tensor]]): The collated batch of data containing labels\n            loss_only (bool, optional): Whether to only return the loss. Defaults to False.\n\n        Returns:\n            dict[str, Union[Tensor, float]]: A dictionary of metrics containing loss and mse\n        \"\"\"\n\n        labels = collated_batch[\"labels\"]\n        loss = self.loss(preds, labels)\n        if loss_only:\n            return {\"loss\": loss}\n        metrics = self.get_metrics_by_stage(stage)\n\n        if self.num_outputs &gt; 1 and stage == \"test\":\n            for name, metric in metrics.items():\n                if len(name.split(\"_\")) == 1:\n                    self.call_or_update_metric(stage, metric, preds, labels)\n                else:\n                    i = int(name.split(\"_\")[-1])\n                    self.call_or_update_metric(stage, metric, preds[:, i], labels[:, i])\n        else:\n            for metric in metrics.values():\n                self.call_or_update_metric(stage, metric, preds, labels)\n\n        return {\"loss\": loss}\n\n    def log_grad_norm(self, optimizer):\n        \"\"\"\n        Log the total total_norm, adaptor_param_norm and adaptor_grad_norm.\n\n        Refer to\n        https://github.com/Lightning-AI/pytorch-lightning/blob/master/src/lightning/pytorch/plugins/precision/precision.py\n        https://github.com/Lightning-AI/pytorch-lightning/blob/master/src/lightning/pytorch/core/module.py\n        for the calculation of the gradient norm\n        \"\"\"\n        parameters = self.trainer.precision_plugin.main_params(optimizer)\n        parameters = list(parameters)\n        if len(parameters) &gt; 0:\n            assert all([p.requires_grad for p in parameters])\n            if all([p.grad is not None for p in parameters]):\n                total_norm = vector_norm(\n                    torch.stack([vector_norm(p.grad, ord=2) for p in parameters]), ord=2\n                )\n                adaptor_param_norm = vector_norm(\n                    torch.stack(\n                        [vector_norm(p, ord=2) for p in self.adapter.parameters()]\n                    ),\n                    ord=2,\n                )\n                adaptor_grad_norm = vector_norm(\n                    torch.stack(\n                        [vector_norm(p.grad, ord=2) for p in self.adapter.parameters()]\n                    ),\n                    ord=2,\n                )\n\n                self.log(\"total_norm\", total_norm, rank_zero_only=True)\n                self.log(\"adaptor_param_norm\", adaptor_param_norm, rank_zero_only=True)\n                self.log(\"adaptor_grad_norm\", adaptor_grad_norm, rank_zero_only=True)\n\n    def on_before_optimizer_step(self, optimizer):\n        \"\"\"\n        Log gradient norm of adaptor's parameters\n        \"\"\"\n        if (\n            self.log_grad_norm_step &gt; 0\n            and self.trainer.global_step % self.log_grad_norm_step == 0\n        ):\n            self.log_grad_norm(optimizer)\n</code></pre>"},{"location":"experiment_design/tasks/#modelgenerator.tasks.SequenceRegression.configure_model","title":"<code>configure_model()</code>","text":"<p>Configures the model for training and interence. Subclasses must implement this method.</p> Source code in <code>modelgenerator/tasks/tasks.py</code> <pre><code>def configure_model(self) -&gt; None:\n    if self.backbone is not None:\n        return\n    if self.use_legacy_adapter:\n        self.backbone = self.backbone_fn(\n            LegacyAdapterType.SEQ_CLS,\n            DefaultConfig(\n                config_overwrites={\n                    \"problem_type\": \"regression\",\n                    \"num_labels\": self.num_outputs,\n                }\n            ),\n        )\n        self.adapter = self.backbone.get_decoder()\n    else:\n        self.backbone = self.backbone_fn(None, None)\n        self.adapter = self.adapter_fn(\n            self.backbone.get_embedding_size(), self.num_outputs\n        )\n</code></pre>"},{"location":"experiment_design/tasks/#modelgenerator.tasks.SequenceRegression.forward","title":"<code>forward(collated_batch)</code>","text":"<p>Runs a forward pass of the model.</p> <p>Parameters:</p> Name Type Description Default <code>collated_batch</code> <code>dict[str, Union[list, Tensor]]</code> <p>A collated batch of data containing input_ids and attention_mask.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The regression predictions</p> Source code in <code>modelgenerator/tasks/tasks.py</code> <pre><code>def forward(self, collated_batch: dict[str, Union[list, Tensor]]) -&gt; Tensor:\n    \"\"\"Runs a forward pass of the model.\n\n    Args:\n        collated_batch (dict[str, Union[list, Tensor]]): A collated batch of data containing input_ids and attention_mask.\n\n    Returns:\n        Tensor: The regression predictions\n    \"\"\"\n\n    hidden_states = self.backbone(**collated_batch)  # (bs, seq_len, dim)\n    preds = self.adapter(hidden_states, collated_batch[\"attention_mask\"])\n    return preds\n</code></pre>"},{"location":"experiment_design/tasks/#modelgenerator.tasks.SequenceRegression.evaluate","title":"<code>evaluate(preds, collated_batch, stage=None, loss_only=False)</code>","text":"<p>Evaluates the model predictions against the ground truth labels.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>Tensor</code> <p>The model predictions from forward.</p> required <code>collated_batch</code> <code>dict[str, Union[list, Tensor]]</code> <p>The collated batch of data containing labels</p> required <code>stage</code> <code>str</code> <p>The stage of training (train, val, test). Defaults to None.</p> <code>None</code> <code>loss_only</code> <code>bool</code> <p>Whether to only return the loss. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Union[Tensor, float]]</code> <p>dict[str, Union[Tensor, float]]: A dictionary of metrics containing loss and mse</p> Source code in <code>modelgenerator/tasks/tasks.py</code> <pre><code>def evaluate(\n    self,\n    preds: Tensor,\n    collated_batch: dict[str, Union[list, Tensor]],\n    stage: Optional[Literal[\"train\", \"val\", \"test\"]] = None,\n    loss_only: bool = False,\n) -&gt; dict[str, Union[Tensor, float]]:\n    \"\"\"Evaluates the model predictions against the ground truth labels.\n\n    Args:\n        logits (Tensor): The model predictions\n        collated_batch (dict[str, Union[list, Tensor]]): The collated batch of data containing labels\n        loss_only (bool, optional): Whether to only return the loss. Defaults to False.\n\n    Returns:\n        dict[str, Union[Tensor, float]]: A dictionary of metrics containing loss and mse\n    \"\"\"\n\n    labels = collated_batch[\"labels\"]\n    loss = self.loss(preds, labels)\n    if loss_only:\n        return {\"loss\": loss}\n    metrics = self.get_metrics_by_stage(stage)\n\n    if self.num_outputs &gt; 1 and stage == \"test\":\n        for name, metric in metrics.items():\n            if len(name.split(\"_\")) == 1:\n                self.call_or_update_metric(stage, metric, preds, labels)\n            else:\n                i = int(name.split(\"_\")[-1])\n                self.call_or_update_metric(stage, metric, preds[:, i], labels[:, i])\n    else:\n        for metric in metrics.values():\n            self.call_or_update_metric(stage, metric, preds, labels)\n\n    return {\"loss\": loss}\n</code></pre>"},{"location":"experiment_design/tasks/#modelgenerator.adapters.SequenceAdapter","title":"<code>modelgenerator.adapters.SequenceAdapter</code>","text":"<p>Base class only for type hinting purposes. Used for Callable[[int, int] SequenceAdapter] types.</p> Source code in <code>modelgenerator/adapters/base.py</code> <pre><code>class SequenceAdapter:\n    \"\"\"Base class only for type hinting purposes. Used for Callable[[int, int] SequenceAdapter] types.\"\"\"\n\n    pass\n</code></pre>"},{"location":"experiment_design/tasks/#modelgenerator.adapters.LinearCLSAdapter","title":"<code>modelgenerator.adapters.LinearCLSAdapter</code>","text":"<p>               Bases: <code>Module</code>, <code>SequenceAdapter</code></p> <p>Simple linear adapter for a 1D embedding</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>Number of input features</p> required <code>out_features</code> <code>int</code> <p>Number of output features</p> required Source code in <code>modelgenerator/adapters/adapters.py</code> <pre><code>class LinearCLSAdapter(nn.Module, SequenceAdapter):\n    \"\"\"Simple linear adapter for a 1D embedding\n\n    Args:\n        in_features (int): Number of input features\n        out_features (int): Number of output features\n    \"\"\"\n\n    def __init__(self, in_features: int, out_features: int):\n        super().__init__()\n        self.linear = nn.Linear(in_features, out_features)\n\n    def forward(self, hidden_states: Tensor, attention_mask: Tensor = None) -&gt; Tensor:\n        \"\"\"Forward pass\n\n        Args:\n            hidden_states (torch.Tensor): of shape (n, seq_len, in_features)\n            attention_mask (torch.Tensor): of shape (n, seq_len)\n\n        Returns:\n            torch.Tensor: predictions (n, out_features)\n        \"\"\"\n        output = self.linear(hidden_states[:, 0])\n        return output\n</code></pre>"},{"location":"tutorials/dependency_mapping/","title":"Dependency Mapping","text":"<p>Dependency mapping is an in silico mutagenesis technique that identifies co-conserved elements in a sequence. AIDO.ModelGenerator implements the procedure proposed by Tomaz da Silva et al. We use this to mine functional genomic elements in the AIDO.DNA paper with the AIDO.DNA-7B and AIDO.DNA-300M models. This task uses the pre-trained models directly, and does not require finetuning.</p> <p>To reproduce the dependency mapping results from the AIDO.DNA paper, run the following from the ModelGenerator root directory:</p> <pre><code># Inference\nmgen predict --config experiments/AIDO.DNA/dependency_mapping/config.yaml\n\n# Plotting\npython experiments/AIDO.DNA/dependency_mapping/plot_dependency_maps.py \\\n    -i depmap_predictions \\\n    -o depmap_plots \\\n    -v experiments/AIDO.DNA/dependency_mapping/DNA.txt \\\n    -t modelgenerator/huggingface_models/rnabert/vocab.txt \n</code></pre> <p>To create new dependency maps,</p> <ol> <li>Gather your sequences into a .tsv file with an <code>id</code> and <code>sequence</code> column.</li> <li>Run <code>mgen predict --config config.yaml</code> where</li> </ol> <pre><code>model:\n  class_path: Inference\n  init_args: \n    backbone: &lt;you-choose&gt;\ndata:\n  class_path: DependencyMappingDataModule\n  init_args:\n    path: &lt;path/to/your/seq/dir&gt;  # Note: this errors for ., use ../dependency_mapping if necessary\n    test_split_files: \n      - &lt;my_sequences.tsv&gt;\n    vocab_file: &lt;vocab&gt;.txt  # E.g. experiments/AIDO.DNA/dependency_mapping/DNA.txt\ntrainer:\n  callbacks:\n  - class_path: modelgenerator.callbacks.PredictionWriter\n    dict_kwargs:\n      output_dir: predictions\n      filetype: pt\n</code></pre> <ol> <li>Run the plotting tool</li> </ol> <pre><code>python experiments/AIDO.DNA/dependency_mapping/plot_dependency_maps.py \\\n    -i &lt;prediction_dir&gt; \\\n    -o &lt;output_dir&gt; \\\n    -v &lt;vocab.txt&gt; \\\n    -t &lt;tokenizer_vocab.txt&gt;  \n</code></pre> <p>The output will be files of the name <code>&lt;id&gt;.png</code> in the output directory, with heatmaps of dependencies and logos with sequence information content.</p>"},{"location":"tutorials/finetuning_scheduler/","title":"Finetuning scheduler","text":"<p>For some of our experiments, we leverage the gradual unfreezing finetuning scheduler, adapted from RiNALMo's code repository.</p>"},{"location":"tutorials/finetuning_scheduler/#creating-schedule","title":"Creating <code>schedule</code>","text":"<p>To use a FT scheduler, first we have to create a schedule and saving as a <code>.yaml</code> file. An example schedule is shown below:</p> <pre><code>0:\n    - adapter.*\n3:\n    - backbone.encoder.encoder.ln.*\n    - backbone.encoder.encoder.layer.32.*\n</code></pre> <p>In this example, when the model is setup, all the layers are first frozen. Then before the <code>0-th</code>-th epoch starts, all the parameters in the <code>adapter</code> module are unfrozen, and they remain unfrozen (trainable) for the rest of the training run. Similarly, before the <code>3-rd</code> epoch starts, parameters in the <code>backbone.encoder.encoder.ln</code> module (i.e., the last layer norm module of the backbone's encoder) is unfrozen, and they remain unfrozen until the training ends. Here can add any other layer or module if we want to unfreeze it before the starting of some specific epoch.</p>"},{"location":"tutorials/finetuning_scheduler/#using-schedule-when-finetuning-with-modelgenerator","title":"Using <code>schedule</code> when finetuning with ModelGenerator","text":"<p>In order to use this schedule for finetuning, we can simply to set this as CLI argument for <code>--trainer.callbacks.ft_schedule_path</code> when calling <code>mget fit</code>. </p> <p>Following is an example of finetuning the AIDO.RNA-1.6B model for RNA secondary structure prediction, with a scheduler named <code>layers_0_32.yaml</code>. (NOTE: Please refer to the correspoding experiment folder for details of this experiment):</p> <pre><code>cd experiments/AIDO.RNA/rna_secondary_structure_prediction\nMGEN_DATA_DIR=~/mgen_data\nDATASET_NAME=bpRNA\nCKPT_SAVE_DIR=logs/rna_ss/${DATASET_NAME}\nmgen fit --config rna_ss_prediction.yaml \\\n    --data.path ${MGEN_DATA_DIR}/modelgenerator/datasets/rna_ss_data/ \\\n    --data.dataset ${DATASET_NAME} \\\n    --trainer.default_root_dir ${CKPT_SAVE_DIR} \\\n    --trainer.callbacks.ft_schedule_path ft_schedules/layers_0_32.yaml \\\n    --trainer.devices 0,1,2,3\n</code></pre>"},{"location":"tutorials/kfold_cross_validation/","title":"K-fold cross validation","text":"<p>Datasets implementing the <code>DataInterface</code> with the <code>KFoldMixin</code> support semi-automatic k-fold crossvalidation for uncertainty estimation. </p> <p>We use translation efficiency prediction as an example task to demonstrate how to do a k-fold cross validation in ModelGenerator. The logic is to split the dataset into k-fold, and call each fold as a test set iteratively.</p>"},{"location":"tutorials/kfold_cross_validation/#data-configs","title":"Data configs","text":"<p>For cross validation task, we input only one dataset named <code>train</code> containing a colomn <code>fold_id</code> indicating the fold index for each sample. You need to set <code>cv_num_folds</code>, <code>cv_test_fold_id</code>, <code>cv_enable_val_fold</code>, <code>cv_fold_id_col</code> according to your experiment setting. </p> <pre><code>data:\n  class_path: modelgenerator.data.TranslationEfficiency\n  init_args:\n    path: genbio-ai/rna-downstream-tasks\n    config_name: translation_efficiency_Muscle\n    normalize: true\n    train_split_name: train\n    random_seed: 42\n    batch_size: 8\n    shuffle: true\n    cv_num_folds: 10\n    cv_test_fold_id: 0\n    cv_enable_val_fold: true\n    cv_fold_id_col: fold_id\n</code></pre> <p>See <code>experiments/AIDO.RNA/configs/translation_efficiency.yaml</code> for full hyperparameter settings.</p>"},{"location":"tutorials/kfold_cross_validation/#finetuning-script","title":"Finetuning script","text":"<pre><code>for FOLD in {0..9}\n  do\n    RUN_NAME=te_Muscle_aido_rna_1b600m_fold${FOLD}\n    CKPT_SAVE_DIR=logs/rna_tasks/${RUN_NAME}\n    CUDA_VISIBLE_DEVICES=0 mgen fit --config experiments/AIDO.RNA/configs/translation_efficiency.yaml \\\n      --data.config_name translation_efficiency_Muscle \\\n      --data.cv_test_fold_id $FOLD \\\n      --trainer.logger.name $RUN_NAME \\\n      --trainer.callbacks.dirpath $CKPT_SAVE_DIR\n  done\n</code></pre>"},{"location":"tutorials/kfold_cross_validation/#evaluation-script","title":"Evaluation script","text":"<pre><code>for FOLD in {0..9}\ndo\n  CKPT_PATH=logs/rna_tasks/te_Muscle_aido_rna_1b600m_fold${FOLD}/best_val*\n  echo \"&gt;&gt;&gt; Fold ${FOLD}\"\n  mgen test --config experiments/AIDO.RNA/configs/translation_efficiency.yaml \\\n    --data.config_name translation_efficiency_Muscle \\\n    --data.cv_test_fold_id $FOLD \\\n    --model.strict_loading True \\\n    --model.reset_optimizer_states True \\\n    --trainer.logger null \\\n    --ckpt_path $CKPT_PATH\ndone\n</code></pre>"},{"location":"tutorials/protein_inverse_folding/","title":"Protein Inverse Folding","text":"<p>Protein inverse folding represents a computational technique aimed at generating protein sequences that will fold into specific three-dimensional structures. The central challenge in protein inverse folding involves identifying sequences capable of reliably adopting the intended structure. In our research, we concentrate on designing sequences based on the known backbone structure of a protein, represented with 3D coordinates of the atoms of the backbone (without any information about what the individual amino-acids are). Specifically. we finetune the AIDO.Protein-16B model with LoRA on the CATH 4.2 benchmark dataset. We use the same train, validation, and test splits used by the previous studies, such as LM-Design, and DPLM. Current version of ModelGenerator contains the inference pipeline for protein inverse folding. Experimental pipeline on other datasets (both training and testing) will be included in the future.</p>"},{"location":"tutorials/protein_inverse_folding/#experimental-results","title":"Experimental Results","text":"<p>We present an evaluation of various inverse folding techniques using the CATH-4.2 dataset.  AIDO.Protein is compared against current state-of-the-art models across three experiment settings inspired by LM-Design:  (a) sequences with fewer than 100 residues (short chains),  (b) single-chain proteins (those represented by a single entry in CATH 4.2), and  (c) the full CATH 4.2 dataset.  We take DPLM's scores from their paper and the other scores (except AIDO.ProteinIF) were taken from LM-Design's paper. The highest and second-highest scores are highlighted in bold and italic, respectively.  A dash (-) indicates that the corresponding score was not available in the original source.</p> Models Short-chains - PPL \u2193 Short-chains - MSRR \u2193 Single-chains - PPL \u2193 Single-chains - MSRR \u2193 Full - PPL \u2193 Full - MSRR \u2193 GVP 7.23 30.60 7.84 28.95 5.36 39.47 ProteinMPNN 6.21 36.35 6.68 34.43 4.61 45.96 ProteinMPNN-CMLM 7.16 35.42 7.25 35.71 5.03 48.62 PiFold 6.04 39.84 6.31 38.53 4.55 51.66 LM-Design 7.01 35.19 6.58 40.00 4.41 54.41 DPLM - - - - - 54.54 AIDO.ProteinIF 4.29 38.46 3.18 58.87 3.20 58.60"},{"location":"tutorials/protein_inverse_folding/#_1","title":"Protein Inverse Folding","text":"<p>In the following sections, we discuss how to use AIDO.ProteinIF for inference and testing on CATH 4.2 using ModelGenerator.</p>"},{"location":"tutorials/protein_inverse_folding/#setup","title":"Setup","text":"<p>Install ModelGenerator.  - It is required to use docker to run our inverse folding pipeline. - Please set up a docker image using our provided Dockerfile and run the inverse folding inference from within the docker container. </p> <p>Here is an example bash script to set up and access a docker container:</p> <pre><code># clone the ModelGenerator repository\ngit clone https://github.com/genbio-ai/ModelGenerator.git\n# cd to \"ModelGenerator\" folder where you should find the \"Dockerfile\"\ncd ModelGenerator\n# create a docker image\ndocker build -t aido .\n# create a local folder as ModelGenerator's data directory\nmkdir -p $HOME/mgen_data\n# run a container\ndocker run -d --runtime=nvidia -it -v \"$(pwd):/workspace\" -v \"$HOME/mgen_data:/mgen_data\" aido /bin/bash\n# find the container ID\ndocker ps # this will print the running containers and their IDs\n# execute the container with ID=&lt;container_id&gt;\ndocker exec -it &lt;container_id&gt; /bin/bash  # now you should be inside the docker container\n# test if you can access the nvidia GPUs\nnvidia-smi # this should print the GPUs' details\n</code></pre> <ul> <li>Execute the following steps from within the docker container you just created.</li> <li>Note: Multi-GPU inference for inverse folding is not currently supported and will be included in the future.</li> </ul>"},{"location":"tutorials/protein_inverse_folding/#download-and-merge-model-checkpoint-chunks","title":"Download and merge model checkpoint chunks","text":"<p>Download all the 15 model checkpoint chunks (named as <code>chunk_&lt;chunk_ID&gt;.bin</code>) from here. Place them inside the directory <code>${MGEN_DATA_DIR}/modelgenerator/huggingface_models/protein_inv_fold/AIDO.ProteinIF-16B/model_chunks</code> and merge them. </p> <p>You can do this by simply running the following script:</p> <pre><code>mkdir -p ${MGEN_DATA_DIR}/modelgenerator/huggingface_models/protein_inv_fold/AIDO.ProteinIF-16B/\nhuggingface-cli download genbio-ai/AIDO.ProteinIF-16B \\\n  --repo-type model \\\n  --local-dir ${MGEN_DATA_DIR}/modelgenerator/huggingface_models/protein_inv_fold/AIDO.ProteinIF-16B/\n# change directory to the folder: /workspace/experiments/AIDO.Protein/protein_inverse_folding/\ncd /workspace/experiments/AIDO.Protein/protein_inverse_folding/\n# Merge chunks\npython merge_ckpt.py ${MGEN_DATA_DIR}/modelgenerator/huggingface_models/protein_inv_fold/AIDO.ProteinIF-16B/model_chunks ${MGEN_DATA_DIR}/modelgenerator/huggingface_models/protein_inv_fold/AIDO.ProteinIF-16B/model.ckpt\n</code></pre>"},{"location":"tutorials/protein_inverse_folding/#download-data","title":"Download data","text":""},{"location":"tutorials/protein_inverse_folding/#for-inference-on-cath-42","title":"For inference on CATH 4.2:","text":"<p>Download the preprocessed CATH 4.2 dataset from here.  You should find two files named chain_set_map.pkl and chain_set_splits.json. Place them inside the directory <code>${MGEN_DATA_DIR}/modelgenerator/datasets/protein_inv_fold/cath_4.2/</code>. (Note that it was originally preprocessed by Generative Models for Graph-Based Protein Design (Ingraham et al, NeurIPS'19), and we further preprocessed it to suit our pipeline.)</p> <p>Alternatively, you can do it by simply running the following script:</p> <pre><code>DATA_DIR=${MGEN_DATA_DIR}/modelgenerator/datasets/protein_inv_fold/cath_4.2\nmkdir -p ${DATA_DIR}/\nhuggingface-cli download genbio-ai/protein-inverse-folding \\\n  --repo-type dataset \\\n  --local-dir ${MGEN_DATA_DIR}/modelgenerator/datasets/protein_inv_fold\n</code></pre>"},{"location":"tutorials/protein_inverse_folding/#for-inference-on-a-protein-from-pdb","title":"For inference on a protein from PDB:","text":"<p>First download a single 3D structure (PDB/CIF file) from PDB:</p> <pre><code>DATA_DIR=${MGEN_DATA_DIR}/modelgenerator/datasets/protein_inv_fold/custom_data ## The directory where you want to download the PDB/CIF file. Feel free to change.\nPDB_ID=5YH2   ## example protein's PDB ID\nCHAIN_ID=A    ## example protein's CHAIN ID\nmkdir -p ${DATA_DIR}/\nwget https://files.rcsb.org/download/${PDB_ID}.cif -P ${DATA_DIR}/\n</code></pre> <p>Then put it into our format:</p> <pre><code>python preprocess_PDB.py ${DATA_DIR}/${PDB_ID}.cif ${CHAIN_ID} ${DATA_DIR}/\n</code></pre>"},{"location":"tutorials/protein_inverse_folding/#run-inference","title":"Run inference","text":"<p>From your terminal, change directory to <code>/workspace/experiments/AIDO.Protein/protein_inverse_folding</code> folder and run the following script:</p> <pre><code>cd /workspace/experiments/AIDO.Protein/protein_inverse_folding/\n# Run inference\nmgen test --config protein_inv_fold_test.yaml \\\n  --trainer.default_root_dir ${MGEN_DATA_DIR}/modelgenerator/logs/protein_inv_fold/ \\\n  --ckpt_path ${MGEN_DATA_DIR}/modelgenerator/huggingface_models/protein_inv_fold/AIDO.ProteinIF-16B/model.ckpt \\\n  --trainer.devices 0, \\\n  --data.path ${DATA_DIR}/\n</code></pre>"},{"location":"tutorials/protein_inverse_folding/#outputs","title":"Outputs","text":"<ul> <li>The evaluation score will be printed on the console. </li> <li>The generated sequences will be stored in the folder <code>./proteinIF_outputs/</code>. There will be two output files: </li> <li><code>designed_sequences.pkl</code>, </li> <li><code>results_acc_&lt;median_accuracy&gt;.txt</code> (where median accuracy is the median accuracy calculated over all the test samples)</li> </ul> <p>The contents of the files are described below:</p>"},{"location":"tutorials/protein_inverse_folding/#output-file-1-designed_sequencespkl","title":"Output file 1: <code>designed_sequences.pkl</code>","text":"<p>This file will contain the raw token (amino-acid) IDs of the ground truth sequences (<code>\"true_seq\"</code>) and predicted sequences by our method (<code>\"pred_seq\"</code>), stored as numpy arrays. An example:</p> <pre><code>{\n  'true_seq': [\n      array([[ 4,  8,  4,  3, 12,  5,  2, 11, 16, 15,  5,  1, 11, ...]]), ...\n  ],\n  'pred_seq': [\n      array([[ 8,  2,  4,  3, 10,  6,  2, 11, 16, 15,  6,  1, 11, ...]]), ...\n  ]\n}\n</code></pre>"},{"location":"tutorials/protein_inverse_folding/#output-file-2-results_acc_median_accuracytxt","title":"Output file 2: <code>results_acc_&lt;median_accuracy&gt;.txt</code>","text":"<p>Here, for each protein in the test set, we have three lines of information:   - Line1: Identity of the protein (as '<code>name=&lt;PDB_ID&gt;.&lt;CHAIN_ID&gt;</code>'), length of the squence (as '<code>L=&lt;length_of_sequence&gt;</code>'), and the recovery rate/accuracy for that protein sequence (as '<code>Recovery=&lt;recovery_rate_of_sequence&gt;</code>')   - Line2: Single-letter representation of amino-acids of the ground truth sequences (as <code>true:&lt;sequence_of_amino_acids&gt;</code>)   - Line3: Single-letter representation of amino-acids of the predicted sequences by our method (as <code>pred:&lt;sequence_of_amino_acids&gt;</code>)</p> <p>An example file content:</p> <pre><code>&gt;name=3fkf.A | L=141 | Recovery=0.5957446694374084\ntrue:VTVGKSAPYFSLPNEKGEKLSRSAERFRNRYLLLNFWASWCDPQPEANAELKRLNKEYKKNKNFAMLGISLDIDREAWETAIKKDTLSWDQVCDFTGLSSETAKQYAILTLPTNILLSPTGKILARDIQGEALTGKLKELL\npred:TAVGDEAPYFELPDLEGKKLSLDSEEFKNKYLLLDFWASWCLPCREEIAELKELYRRFAKNKKFAILGVSADTDKEAWLKAVKEDNLRWTQVSDFKGWDSEVFKNYNVQSLPENILLSPEGKILARGIRGEALRNKLKELL\n\n&gt;name=2d9e.A | L=121 | Recovery=0.7685950398445129\ntrue:GSSGSSGFLILLRKTLEQLQEKDTGNIFSEPVPLSEVPDYLDHIKKPMDFFTMKQNLEAYRYLNFDDFEEDFNLIVSNCLKYNAKDTIFYRAAVRLREQGGAVLRQARRQAEKMGSGPSSG\npred:GSSGSSGRLTLLRETLEQLQERDTGWVFSEPVPLSEVPDYLDVIDHPMDFSTMRRKLEAHRYLSFDEFERDFNLIVENCRKYNAKDTVFYRAAVRLQAQGGAILRKARRDVESLGSGPSSG\n</code></pre>"},{"location":"tutorials/rna_inverse_folding/","title":"RNA Inverse Folding","text":"<p>RNA inverse folding is a computational method designed to create RNA sequences that fold into predetermined three-dimensional structures. Our study focuses on generating sequences using the known backbone structure of an RNA, defined by the 3D coordinates of its backbone atoms, without any information of the individual bases. Specifically. we fully finetune the AIDO.RNA-1.6B model on the single-state split from Das et al. already processed by Joshi et al.. We use the same train, validation, and test splits used by their method gRNAde. Current version of ModelGenerator contains the inference pipeline for RNA inverse folding. Experimental pipeline on other datasets (both training and testing) will be included in the future.</p>"},{"location":"tutorials/rna_inverse_folding/#experimental-results","title":"Experimental Results","text":"<p>We evaluate our model in two settings: (1) adaptation with conditional diffusion where AIDO.RNA is fine-tuned for the inverse folding task; and (2) zero-shot generation where AIDO.RNA is frozen. | Model | Mean Sequence Recovery Rate | | ------ | ------ | |    gRNAde    |    52.78    | |    gRNAde+AIDO.RNA-Zeroshot    |    53.16    | |    gRNAde+AIDO.RNA-Finetuned.  |    54.41    |</p>"},{"location":"tutorials/rna_inverse_folding/#acknowledgement","title":"Acknowledgement","text":"<p>We thank the authors of gRNAde for providing their models' checkpoints and configuration files.</p>"},{"location":"tutorials/rna_inverse_folding/#_1","title":"RNA Inverse Folding","text":"<p>In the following sections, we discuss how to use AIDO.RNA for RNA inverse folding using ModelGenerator.</p>"},{"location":"tutorials/rna_inverse_folding/#setup","title":"Setup","text":"<p>Install ModelGenerator.  - It is required to use docker to run our inverse folding pipeline. - Please set up a docker image using our provided Dockerfile and run the inverse folding inference from within the docker container. </p> <p>Here is an example bash script to set up and access a docker container:</p> <pre><code># clone the ModelGenerator repository\ngit clone https://github.com/genbio-ai/ModelGenerator.git\n# cd to \"ModelGenerator\" folder where you should find the \"Dockerfile\"\ncd ModelGenerator\n# create a docker image\ndocker build -t aido .\n# create a local folder as ModelGenerator's data directory\nmkdir -p $HOME/mgen_data\n# run a container\ndocker run -d --runtime=nvidia -it -v \"$(pwd):/workspace\" -v \"$HOME/mgen_data:/mgen_data\" aido /bin/bash\n# find the container ID\ndocker ps # this will print the running containers and their IDs\n# execute the container with ID=&lt;container_id&gt;\ndocker exec -it &lt;container_id&gt; /bin/bash  # now you should be inside the docker container\n# test if you can access the nvidia GPUs\nnvidia-smi # this should print the GPUs' details\n</code></pre> <ul> <li>Execute the following steps from within the docker container you just created.</li> <li>Note: Multi-GPU inference for inverse folding is not currently supported and will be included in the future.</li> </ul>"},{"location":"tutorials/rna_inverse_folding/#download-model-checkpoints","title":"Download model checkpoints","text":"<ul> <li> <p>Download the <code>model.ckpt</code> checkpoint from here. Place it inside the local directory <code>${MGEN_DATA_DIR}/modelgenerator/huggingface_models/rna_inv_fold/AIDO.RNAIF-1.6B</code>.</p> </li> <li> <p>Download the gRNAde checkpoint named <code>gRNAde_ARv1_1state_das.h5</code> from the huggingface-hub or the original source. Place it inside the directory <code>${MGEN_DATA_DIR}/modelgenerator/huggingface_models/rna_inv_fold/AIDO.RNAIF-1.6B/other_models</code>. Set the environment variable <code>gRNAde_CKPT_PATH=${MGEN_DATA_DIR}/modelgenerator/huggingface_models/rna_inv_fold/AIDO.RNAIF-1.6B/other_models/gRNAde_ARv1_1state_das.h5</code></p> </li> </ul> <p>Alternatively, you can simply run the following script to do both of these steps:</p> <pre><code>mkdir -p ${MGEN_DATA_DIR}/modelgenerator/huggingface_models/rna_inv_fold/AIDO.RNAIF-1.6B\nhuggingface-cli download genbio-ai/AIDO.RNAIF-1.6B \\\n--repo-type model \\\n--local-dir ${MGEN_DATA_DIR}/modelgenerator/huggingface_models/rna_inv_fold/AIDO.RNAIF-1.6B\n# Set the environment variable gRNAde_CKPT_PATH\nexport gRNAde_CKPT_PATH=${MGEN_DATA_DIR}/modelgenerator/huggingface_models/rna_inv_fold/AIDO.RNAIF-1.6B/other_models/gRNAde_ARv1_1state_das.h5\n</code></pre>"},{"location":"tutorials/rna_inverse_folding/#download-data","title":"Download data","text":"<p>Download the data preprocessed by Joshi et al.. Mainly download these two files: processed.pt.zip and processed_df.csv. Place them inside the directory <code>${MGEN_DATA_DIR}/modelgenerator/datasets/rna_inv_fold/raw_data/</code>. Please refer to this link for details about the dataset and its preprocessing.</p> <p>Alternatively, you run the following script to do it:</p> <pre><code>mkdir -p ${MGEN_DATA_DIR}/modelgenerator/datasets/rna_inv_fold/raw_data/\nhuggingface-cli download genbio-ai/rna-inverse-folding \\\n--repo-type dataset \\\n--local-dir ${MGEN_DATA_DIR}/modelgenerator/datasets/rna_inv_fold/raw_data/\n</code></pre>"},{"location":"tutorials/rna_inverse_folding/#run-inference","title":"Run inference","text":"<p>From your terminal, change directory to <code>experiments/AIDO.RNA/rna_inverse_folding</code> folder and run the following script:</p> <pre><code>cd modelgenerator/rna_inv_fold/gRNAde_structure_encoder\necho \"Running inference..\"\npython main.py\necho \"Extracting structure encoding..\"\npython main_encoder_only.py\ncd  ../../../experiments/AIDO.RNA/rna_inverse_folding/\n# run inference\nmgen test --config rna_inv_fold_test.yaml \\\n  --trainer.default_root_dir ${MGEN_DATA_DIR}/modelgenerator/logs/rna_inv_fold/ \\\n  --ckpt_path ${MGEN_DATA_DIR}/modelgenerator/huggingface_models/rna_inv_fold/AIDO.RNAIF-1.6B/model.ckpt \\\n  --trainer.devices 0, \\\n  --data.path ${MGEN_DATA_DIR}/modelgenerator/datasets/rna_inv_fold/structure_encoding/\n</code></pre>"},{"location":"tutorials/rna_inverse_folding/#outputs","title":"Outputs","text":"<ul> <li>The evaluation score will be printed on the console.</li> <li>The generated sequences will be stored in <code>./rnaIF_outputs/designed_sequences.json</code>. In this file, we will have:</li> <li><code>\"true_seq\"</code>: the ground truth sequences,</li> <li><code>\"pred_seq\"</code>: predicted sequences by our method,</li> <li><code>\"baseline_seq\"</code>: predicted sequences by the baseline method gRNAde.</li> </ul> <p>An example file content with two test samples is shown below:</p> <pre><code>{\n  \"true_seq\": [\n    \"CCCAGUCCACCGGGUGAGAAGGGGGCAGAGAAACACACGACGUGGUGCAUUACCUGCC\",\n    \"UCCCGUCCACCGCGGUGAGAAGGGGGCAGAGAAACACACGAUCGUGGUACAUUACCUGCC\",\n  ],\n  \"pred_seq\": [\n    \"UGGGGAGCCCCCGGGGUGAACCAGCCGGUGAAAGGCACCCGGUGAUCGGUCAGCCCAC\",\n    \"GCGGAUGCCCCGCCCGGUCAACCGCAUGGUGAAAUCCACGCGCCUGGUGGGUUAGCCAUG\",\n  ],\n  \"baseline_seq\": [\n    \"UGGUGAGCCCCCGGGGUGAACCAGUAGGUGAAAGGCACCCGGUGAUCGGUCAGCCCAC\",\n    \"GCGGAUGCCGGGCCCGGUCCACCGCAUGGUGAAAUUCAGGCGCCUGGAGGGUUAGCCAUG\",\n  ]\n}\n</code></pre>"},{"location":"tutorials/rna_secondary_structure_prediction/","title":"RNA Secondary Structure Prediction","text":"<p>As with proteins, structure determines RNA function. RNA secondary structure, formed by base pairing, is more stable and accessible than its tertiary form within cells. Accurate prediction of RNA secondary structure is essential for tasks such as higher-order structure prediction and function prediction. As discussed in our paper AIDO.RNA, we finetune the AIDO.RNA-1.6B model on the training splits of the following two datasets: 1. bpRNA 2. Archive-II</p> <p>We preprocessed and split the datasets (into train, test, and validation splits) in the same way as done in a previous study RiNALMo. </p>"},{"location":"tutorials/rna_secondary_structure_prediction/#experimental-results","title":"Experimental Results","text":"<p>In the following table, we demonstrate RNA secondary structure prediction results on the bpRNA test set (namely, bpRNA-TS0). | Model           | Precision | Recall | F1-score | |---------------------|---------------|------------|--------------| | SPOT-RNA       | 0.594         | 0.693      | 0.619        | | UFold          | 0.607         | 0.741      | 0.654        | | RNA-FM         | 0.709         | 0.664      | 0.676        | | RNAErnie       | 0.575         | 0.678      | 0.622        | | RiNALMo        | 0.784         | 0.730      | 0.747        | | AIDO.RNA (ours) | 0.815     | 0.769  | 0.783    |</p> <p>We also demonstrate inter-family generalization for secondary structure prediction on filtered Archive-II in the following table. Reported is the average F1 score. Bold denotes the best performance within a family. | RNA family     | AIDO.RNA (ours) | RNAstructure | CONTRAfold | RiNALMo | RNA-FM | MXfold2 | UFold | |--------------------|---------------------|------------------|----------------|-------------|------------|-------------|-----------| | 5S rRNA            | 0.853               | 0.63             | 0.63           | 0.88    | 0.57       | 0.54        | 0.53      | | SRP RNA            | 0.739           | 0.63             | 0.55           | 0.70        | 0.25       | 0.50        | 0.26      | | tRNA               | 0.945           | 0.70             | 0.77           | 0.93        | 0.79       | 0.64        | 0.26      | | tmRNA              | 0.838           | 0.43             | 0.49           | 0.80        | 0.28       | 0.46        | 0.41      | | RNase P RNA        | 0.804           | 0.55             | 0.63           | 0.80        | 0.31       | 0.51        | 0.41      | | Group I Intron     | 0.644.              | 0.54             | 0.60           | 0.66    | 0.16       | 0.45        | 0.45      | | 16S rRNA           | 0.795           | 0.57             | 0.58           | 0.74        | 0.14       | 0.55        | 0.41      | | Telomerase RNA     | 0.085               | 0.50             | 0.54           | 0.12        | 0.07       | 0.34        | 0.80  | | 23S rRNA           | 0.896           | 0.73             | 0.71           | 0.85        | 0.19       | 0.64        | 0.45      | | Average            | 0.733           | 0.59             | 0.61           | 0.72        | 0.31       | 0.51        | 0.44      |</p>"},{"location":"tutorials/rna_secondary_structure_prediction/#_1","title":"RNA Secondary Structure Prediction","text":""},{"location":"tutorials/rna_secondary_structure_prediction/#to-finetune-aidorna-16b-on-rna-ss","title":"To finetune AIDO.RNA-1.6B on RNA SS:","text":"<ul> <li> <p>Set the environment variable for ModelGenerator's data directory: <code>export MGEN_DATA_DIR=~/mgen_data # or any other local directory of your choice</code></p> </li> <li> <p>Download the preprocessed data (provided as zip file named <code>rna_ss_data.zip</code>) from here. Unzip <code>rna_ss_data.zip</code> inside the directory <code>${MGEN_DATA_DIR}/modelgenerator/datasets/</code>. </p> </li> </ul> <p>Alternatively, you can simply run the following script to do this:</p> <pre><code>export MGEN_DATA_DIR=~/mgen_data # or any other local directory of your choice\nmkdir -p ${MGEN_DATA_DIR}/modelgenerator/datasets/\nwget -P ${MGEN_DATA_DIR}/modelgenerator/datasets/ https://huggingface.co/datasets/genbio-ai/rna-secondary-structure-prediction/resolve/main/rna_ss_data.zip\nunzip ${MGEN_DATA_DIR}/modelgenerator/datasets/rna_ss_data.zip -d ${MGEN_DATA_DIR}/modelgenerator/datasets/\n</code></pre> <p>You should find two sub-folders containing the preprocessed datasets: 1. bpRNA: <code>${MGEN_DATA_DIR}/modelgenerator/datasets/rna_ss_data/bpRNA</code> 2. Archive-II: <code>${MGEN_DATA_DIR}/modelgenerator/datasets/rna_ss_data/archiveII</code></p> <ul> <li>Then run a finetuning job on either dataset as following (Note that here we are using finetuning scheduler. See this tutorial for details):</li> <li>To train on bpRNA dataset, run the following command:</li> </ul> <pre><code>bash rna_secondary_structure_prediction.sh train bpRNA\n</code></pre> <ol> <li>Alternatively, to finetune on Archive-II datasets (for the inter-family generalization experiment discussed in the paper AIDO.RNA), run the following command:</li> </ol> <pre><code>bash rna_secondary_structure_prediction.sh train archiveII_&lt;FamilyName&gt;\n</code></pre> <p>Here, <code>&lt;FamilyName&gt;</code> is any of the following nine strings (representing different RNA families in Archive-II dataset): <code>5s, 16s, 23s, grp1, srp, telomerase, RNaseP, tmRNA, tRNA</code>. Note that, following the conventioned using by RiNALMo's code repository, when a <code>&lt;FamilyName&gt;</code> is chosen, it will only be used as the test set and the rest of the families are used for training and validation. One example finetuning run with <code>5s</code> family:</p> <pre><code>bash rna_secondary_structure_prediction.sh train archiveII_5s\n</code></pre> <p>Here, the AIDO.RNA-1.6B model will be finetuned using all other splits except archiveII_5s.</p>"},{"location":"tutorials/rna_secondary_structure_prediction/#to-test-a-finetuned-checkpoint-on-rna-ss","title":"To test a finetuned checkpoint on RNA SS:","text":"<ul> <li>Finetune AIDO.RNA-1.6B as discussed above, or download the <code>model.ckpt</code> checkpoint from here.</li> <li>Test the checkpoint on the corresponding dataset as following (replace <code>/path/to/checkpoint</code> with the actual path to the finetuned checkpoint):</li> <li>To test on bpRNA dataset, run the following command:</li> </ul> <pre><code>bash rna_secondary_structure_prediction.sh test bpRNA /path/to/checkpoint\n</code></pre> <ol> <li>Alternatively, to test on Archive-II datasets, run the following command:</li> </ol> <pre><code>bash rna_secondary_structure_prediction.sh test archiveII_&lt;FamilyName&gt; /path/to/checkpoint\n</code></pre> <p>See the previous section for details on <code>&lt;FamilyName&gt;</code>.</p>"},{"location":"tutorials/rna_secondary_structure_prediction/#outputs","title":"Outputs:","text":"<p>The evaluation scores will be printed on the console.</p>"},{"location":"tutorials/structure_tokenizer/","title":"Protein Structure Tokenizer","text":"<p>This repository provides tools for working with protein structures, including encoding protein structures into tokens, decoding tokens back into structures, and performing structure prediction tasks. Currently, we only support single-chain protein structures, but we plan to extend this to multi-chain structures in the future.</p> <p>Notes: The below contents only support inference. Training is not included. Training the structure tokenizer and the structure prediction model requires a large amount of data and computational resources (hundreds of GPU-days), which is beyond the scope of the current repository.</p>"},{"location":"tutorials/structure_tokenizer/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Models</li> <li>Structure Tokenizer</li> <li>Structure Prediction Model (16B Language Model)</li> <li>Tasks</li> <li>Quick Start</li> <li>Encoding Structures into Tokens</li> <li>Decoding Tokens into Structures</li> <li>Combining Encode and Decode</li> <li>Structure Prediction</li> </ol>"},{"location":"tutorials/structure_tokenizer/#models","title":"Models","text":"<p>This repository includes two key models used for working with protein structures:</p>"},{"location":"tutorials/structure_tokenizer/#structure-tokenizer","title":"Structure Tokenizer","text":"<p>The Structure Tokenizer model is designed to encode protein structures into discrete structure tokens and decode these tokens back into protein structures. It provides an efficient way to represent complex structural information in a compact, tokenized form. Key Features: - Encoding: Converts protein structures into discrete tokens. - Decoding: Reconstructs protein structures from tokens. - Combined Encoding &amp; Decoding: Supports running both processes in a single pipeline.</p> <p></p> <p>Relevant configuration files: - <code>encode.yaml</code> for encoding tasks. - <code>decode.yaml</code> for decoding tasks. - <code>encode_decode.yaml</code> for combined encoding and decoding.</p> <p>Relevant huggingface models: - <code>genbio-ai/AIDO.StructureEncoder</code> is the model for encoding protein structures into tokens. - <code>genbio-ai/AIDO.StructureDecoder</code> is the model for decoding tokens back into protein structures. - <code>genbio-ai/AIDO.StructureTokenizer</code> is the combined model for encoding and decoding protein structures.</p>"},{"location":"tutorials/structure_tokenizer/#structure-prediction-model-16b-language-model","title":"Structure Prediction Model (16B Language Model)","text":"<p>The Structure Prediction Model uses a pre-trained 16B language model to predict protein structure tokens directly from amino acid sequences. This allows for end-to-end prediction of protein structures.</p> <p>Workflow: 1. Input amino acid sequences. 2. Predict structure tokens using the language model. 3. Decode the predicted tokens into protein structures using the Structure Tokenizer. </p> <p>Relevant configuration files: - <code>protein2structoken_16b.yaml</code> for predicting structure tokens from amino acid sequences. - <code>decode.yaml</code> for decoding predicted tokens into structures.</p> <p>Relevant huggingface models: - <code>genbio-ai/AIDO.Protein2StructureToken-16B</code> is the model for predicting protein structure tokens from amino acid sequences. - <code>genbio-ai/AIDO.StructureDecoder</code> is the model for decoding predicted tokens back into protein structures.</p>"},{"location":"tutorials/structure_tokenizer/#tasks","title":"Tasks","text":""},{"location":"tutorials/structure_tokenizer/#quick-start","title":"Quick Start","text":"<p>If you want to have a quick try of the features without reading the detailed instructions in the following sections, you could run the following scripts that pack the commands for encoding, decoding, and structure prediction tasks. - <code>experiments/AIDO.StructureTokenizer/structure_encoding.sh</code>: Packs the commands for encoding and decoding tasks. - <code>experiments/AIDO.StructureTokenizer/structure_prediction.sh</code>: Packs the commands for structure prediction and decoding tasks. The outputs are under the <code>logs/</code> directory.</p> <p>Notes: - The above scripts are designed for the sample dataset. If you want to use your own dataset, please go through the detailed instructions in the following sections. - The 16B language model is quite large. You need to have enough storage in your HF cache directory and downloading the model could take a long time. Loading the checkpoint from the disk could take 4 minutes or more depending on your hardware.</p>"},{"location":"tutorials/structure_tokenizer/#encoding-structures-into-tokens","title":"Encoding Structures into Tokens","text":"<p>We can use the structure tokenizer to convert protein structures into structure tokens. First, we need to prepare the dataset for encoding. You could either use the sample dataset provided or prepare your own dataset.</p>"},{"location":"tutorials/structure_tokenizer/#preparing-sample-dataset","title":"Preparing Sample Dataset","text":"<p>You can download the sample dataset from the Hugging Face hub. Run the following command to download the dataset to the <code>data/protstruct_sample_data/</code> directory (run this command in the root directory of the repository):</p> <pre><code>huggingface-cli download genbio-ai/sample-structure-dataset --repo-type dataset --local-dir ./data/protstruct_sample_data/\n</code></pre> <p>This dataset is based on the CASP15 dataset, which can be referenced at: - CASP15 Prediction Center - Bhattacharya-Lab/CASP15</p> <p>The downloaded directory includes: - A <code>registries</code> folder containing a CSV file with metadata such as filenames and PDB IDs. - A <code>CASP15_merged</code> folder containing PDB files, where domains are merged in the same way as described in Bhattacharya-Lab/CASP15.</p>"},{"location":"tutorials/structure_tokenizer/#preparing-your-own-dataset","title":"Preparing Your Own Dataset","text":"<p>If you want to use your own dataset for testing the structure tokenizer model, you can prepare a dataset with the following structure: - A folder containing PDB files (supported formats: <code>cif.gz</code>, <code>cif</code>, <code>ent.gz</code>, <code>pdb</code>).</p> <p>Then, you need to prepare a registry file in CSV format using the following command:</p> <pre><code>python experiments/AIDO.StructureTokenizer/register_dataset.py \\\n    --folder_path /path/to/folder_path \\\n    --format cif.gz \\\n    --output_file /path/to/output_file.csv\n</code></pre> <p>Example (if you have a folder with PDB files in <code>data/protstruct_sample_data/CASP15_merged/</code>):</p> <pre><code>python experiments/AIDO.StructureTokenizer/register_dataset.py \\\n    --folder_path data/protstruct_sample_data/CASP15_merged/ \\\n    --format pdb \\\n    --output_file data/protstruct_sample_data/registries/casp15_merged_copy.csv\n</code></pre>"},{"location":"tutorials/structure_tokenizer/#running-encoding-task","title":"Running Encoding Task","text":"<p>If you use the sample dataset, you can run the encoding task using the following command:</p> <pre><code>CUDA_VISIBLE_DEVICES=0 mgen predict --config experiments/AIDO.StructureTokenizer/encode.yaml\n</code></pre> <p>If you use your own dataset, you need to update the <code>folder_path</code> and the <code>registry_path</code> in the <code>encode.yaml</code> configuration file to point to your dataset folder and registry file. Alternatively, you can override these parameters when running the command:</p> <pre><code>CUDA_VISIBLE_DEVICES=0 mgen predict --config experiments/AIDO.StructureTokenizer/encode.yaml \\\n    --data.init_args.config.proteins_datasets_configs.name=\"your_dataset_name\" \\\n    --data.init_args.config.proteins_datasets_configs.registry_path=\"your_dataset_folder_path\" \\\n    --data.init_args.config.proteins_datasets_configs.folder_path=\"your_dataset_registry_path\" \\\n    --trainer.callbacks.dict_kwargs.output_dir=\"your_output_dir\"\n</code></pre> <p>Input: - The PDB files in the dataset folder. - The registry file in CSV format indicating the metadata of the dataset.</p> <p>Output: - The encoded tokens will be saved in the output directory specified in the configuration file. By default it is saved in <code>logs/protstruct_encode/</code>. - The encoded tokens are saved as a <code>.pt</code> file, which can be loaded using PyTorch. Inside the file, it's a dictionary that maps the name of the protein to the encoded tokens (<code>struct_tokens</code>) and other auxiliary information (<code>aatype</code>, <code>residue_index</code>) for reconstruction.   The structure of the dictionary is as follows:   <code>python   {       'T1137s5_nan': { # the nan here is the chain id and CASP15 doesn't have chain id           'struct_tokens': tensor([449, 313, 207, 129, ...]),           'aatype': tensor([ 4,  7,  5, 17, ...]),           'residue_index': tensor([ 33,  34,  35,  36, ...]),       },       ...   }</code> - A codebook file (<code>codebook.pt</code>) that contains the embedding of each token. The shape is <code>(num_tokens, embedding_dim)</code>.</p> <p>Notes: - Currently, this function only supports single GPU inference due to the file saving mechanism. We plan to support multi-GPU inference in the future. - The auxiliary information (<code>aatype</code> and <code>residue_index</code>) can be substituted with placeholder values if not required.     - <code>aatype</code>: This parameter is used to reconstruct the protein sidechains. If sidechain reconstruction is not needed, you can assign dummy values (e.g., all zeros).     - <code>residue_index</code>: This parameter helps the model identify gaps in residue numbering, which can influence structural predictions. If gaps are present, the model may introduce holes in the structure. For structures without gaps, you can use a continuous sequence of integers (e.g., 0 to n-1). - You may need to adjust the <code>max_nb_res</code> parameter in the configuration file based on the maximum number of residues in your dataset. For those proteins with more residues than <code>max_nb_res</code>, the model will truncate the residues. The default value is set to 1024.</p>"},{"location":"tutorials/structure_tokenizer/#decoding-tokens-into-structures","title":"Decoding Tokens into Structures","text":"<p>We can use the decoder from the structure tokenizer to convert the encoded tokens back into protein structures. The decoder requires the encoded tokens and the codebook file. The decoded structures are saved as PDB files. Both backbone and sidechain atoms are reconstructed.</p>"},{"location":"tutorials/structure_tokenizer/#decoding-the-sample-dataset","title":"Decoding the Sample Dataset","text":"<p>If you have run the encoding task above on the sample dataset, the default <code>decode.yaml</code> configuration file is already set up to decode the encoded tokens. You don't need to change anything in the configuration file. You can directly run the decoding task using the following command:</p> <pre><code>CUDA_VISIBLE_DEVICES=0 mgen predict --config=experiments/AIDO.StructureTokenizer/decode.yaml\n</code></pre>"},{"location":"tutorials/structure_tokenizer/#decoding-your-structure-tokens","title":"Decoding Your Structure Tokens","text":"<p>To decode protein structures, you will need the structure tokens in <code>.pt</code> format and a corresponding codebook file (<code>codebook.pt</code>). For ease of use, we recommend preparing the structure tokens in TSV format and then converting them to <code>.pt</code> format using the provided script. </p> <p>The TSV file should include the following columns (an example file is available at <code>experiments/AIDO.StructureTokenizer/decode_example_input.tsv</code>): - <code>uid</code>: A unique identifier for the protein sequence. - <code>sequences</code>: The amino acid sequence (e.g., \"LRTPTT\"). - <code>predictions</code>: The structure tokens to be decoded, provided as a list (e.g., \"[164, 287, 119, ...]\"). The list length must match the length of the amino acid sequence.</p> <p>After preparing the TSV file, you need to convert the TSV file to the <code>.pt</code> format using the following command:</p> <pre><code>python experiments/AIDO.StructureTokenizer/struct_token_format_conversion.py your_tsv_file.tsv your_output_pt_file.pt\n</code></pre> <p>You also need to prepare a codebook file (<code>codebook.pt</code>) that contains the embedding of each token. The codebook could be extracted using this command:</p> <pre><code>python experiments/AIDO.StructureTokenizer/extract_structure_tokenizer_codebook.py --output_path your_output_codebook.pt\n</code></pre> <p>Then you need to update the <code>struct_tokens_path</code> and <code>codebook_path</code> in the <code>decode.yaml</code> configuration file to point to your structure tokens and codebook file. Alternatively, you can override these parameters when running the command:</p> <pre><code>CUDA_VISIBLE_DEVICES=0 mgen predict --config experiments/AIDO.StructureTokenizer/decode.yaml \\\n --data.init_args.config.struct_tokens_datasets_configs.name=\"your_dataset_name\" \\\n --data.init_args.config.struct_tokens_datasets_configs.struct_tokens_path=\"your_structure_tokens.pt\" \\\n --data.init_args.config.struct_tokens_datasets_configs.codebook_path=\"your_codebook.pt\" \\\n --trainer.callbacks.dict_kwargs.dirpath=\"your_output_dir\"\n</code></pre> <p>Input: - The encoded tokens saved in <code>.pt</code> format. - The codebook file (<code>codebook.pt</code>) that contains the embedding of each token.</p> <p>Output: - The decoded protein structures will be saved in the output directory specified in the configuration file. By default it is saved in <code>logs/protstruct_decode/</code>. - The decoded structures are saved as PDB files.</p> <p>Notes: - Decoding the structures could take a long time even when using a GPU. - Currently, this function only supports single GPU inference due to the file saving mechanism. We plan to support multi-GPU inference in the future. - Currently, we don't support specifying the residue index in TSV format. If you need to specify the residue index, you need to modify the  <code>struct_token_format_conversion.py</code> script to include the residue index in the TSV file (we may support this feature in the future), or you could provide the <code>.pt</code> file directly with the desired residue index.</p>"},{"location":"tutorials/structure_tokenizer/#combining-encode-and-decode","title":"Combining Encode and Decode","text":"<p>We can combine the encoding and decoding tasks into a single pipeline. This will output the original structure and the reconstructed structure in the same directory. Each reconstruction is aligned to the original structure using the Kabsch algorithm.</p>"},{"location":"tutorials/structure_tokenizer/#data-preparation","title":"Data Preparation","text":"<p>The data preparation is the same as Encoding Structures into Tokens.</p>"},{"location":"tutorials/structure_tokenizer/#running-combined-encoding-and-decoding-task","title":"Running Combined Encoding and Decoding Task","text":"<p>If you use the sample dataset, you can run the combined encoding and decoding task using the following command:</p> <pre><code>CUDA_VISIBLE_DEVICES=0 mgen predict --config=experiments/AIDO.StructureTokenizer/encode_decode.yaml\n</code></pre> <p>If you use your own dataset, you need to update the <code>folder_path</code> and the <code>registry_path</code> in the <code>encode_decode.yaml</code> configuration file or override them when running the command as described in Encoding Structures into Tokens. </p> <p>Example:</p> <pre><code>CUDA_VISIBLE_DEVICES=0 mgen predict --config experiments/AIDO.StructuctureTokenizer/encode_decode.yaml \\\n    --data.init_args.config.proteins_datasets_configs.name=\"your_dataset_name\" \\\n    --data.init_args.config.proteins_datasets_configs.registry_path=\"your_dataset_folder_path\" \\\n    --data.init_args.config.proteins_datasets_configs.folder_path=\"your_dataset_registry_path\" \\\n    --trainer.callbacks.dict_kwargs.output_dir=\"your_output_dir\"\n</code></pre> <p>Input: - The PDB files in the dataset folder. - The registry file in CSV format indicating the metadata of the dataset.</p> <p>Output: - The decoded structures and their corresponding original structures will be saved in the output directory specified in the configuration file. By default it is saved in <code>logs/protstruct_model/</code>.  - The decoded structures end with <code>output.pdb</code>. - The original structures end with <code>input.pdb</code>.</p> <p>Notes: - Decoding the structures could take a long time even when using a GPU. - Currently, this function only supports single GPU inference due to the file saving mechanism. We plan to support multi-GPU inference in the future. - The reconstructed structures are aligned to the original structures using the Kabsch algorithm. This makes it easier to visualize and compare the structures.</p>"},{"location":"tutorials/structure_tokenizer/#visualizing-the-reconstructed-structures","title":"Visualizing the Reconstructed Structures","text":"<p>We use VS Code + Protein Viewer Extension to visualize the protein structures. It's a beginner-friendly tool for VS Code users. You could also use your preferred protein structure viewer to visualize the structures (e.g., PyMOL, ChimeraX, etc.), but here we focus on this extension.</p> <p>If you have run the Combined Encoding and Decoding Task, you could find the decoded structures and their corresponding original structures in the output directory. You could visualize them as follows. - Find the desired <code>output.pdb</code> and <code>input.pdb</code> pair in the side panel. Select both files when holding the <code>Ctrl</code> key (for Mac users, hold the <code>Cmd</code> key).  - Right-click on the selected files and choose \"Launch Protein Viewer\".  - A new tab will open with the protein structures displayed. You can interact with the structures using the Protein Viewer extension. Wwe have aligned the reconstructed structures to the original structures using the Kabsch algorithm, the displayed structures should be like this, where different colors mean different files. </p>"},{"location":"tutorials/structure_tokenizer/#structure-prediction","title":"Structure Prediction","text":"<p>We can use the structure prediction model to predict protein structure tokens directly from amino acid sequences. The predicted tokens can then be decoded into protein structures using the structure tokenizer.</p>"},{"location":"tutorials/structure_tokenizer/#predicting-structure-tokens-on-casp14-casp15-and-cameo-datasets","title":"Predicting Structure Tokens On CASP14, CASP15, and CAMEO Datasets","text":"<p>We've provided a sample configuration file <code>protein2structoken_16b.yaml</code> that is set up to predict structure tokens from amino acid sequences in the CASP14, CASP15, and CAMEO datasets. You can run the structure prediction task using the following command:</p> <pre><code>mgen predict --config experiments/AIDO.StructureTokenizer/protein2structoken_16b.yaml\n</code></pre> <p>This will automatically download the preprocessed files from the Hugging Face hub and predict the structure tokens from the amino acid sequences. The predicted tokens will be saved in the <code>logs/protein2structoken_16b/predict_predictions.tsv</code> file.</p>"},{"location":"tutorials/structure_tokenizer/#predicting-structure-tokens-on-your-own-dataset","title":"Predicting Structure Tokens On Your Own Dataset","text":"<p>Alternatively, if you want to predict structure tokens from your own amino acid sequences, you can prepare a CSV file with the following structure:</p> idx aa_seq example KEFWNLDKNLQLRLGIVFLG <p>We've provided an example input file at <code>experiments/AIDO.StructureTokenizer/protein2structoken_example_input.csv</code> that you can use to test the structure prediction model. You can run the following command to predict the structure tokens from the amino acid sequences in the input file:</p> <pre><code>mgen predict --config experiments/AIDO.StructureTokenizer/protein2structoken_16b.yaml \\\n    --data.init_args.path=experiments/AIDO.StructureTokenizer/ \\\n    --data.init_args.test_split_files=[protein2structoken_example_input.csv]\n</code></pre>"},{"location":"tutorials/structure_tokenizer/#decoding-predicted-structure-tokens","title":"Decoding Predicted Structure Tokens","text":"<p>We need to decode the predicted structure tokens into protein structures using the decoder. If you run the above configurations without changing the output path, you could follow the below commands to decode the predicted structure tokens.</p> <pre><code># 2. convert the predicted structures in tsv into one pt file\npython experiments/AIDO.StructureTokenizer/struct_token_format_conversion.py logs/protein2structoken_16b/predict_predictions.tsv logs/protein2structoken_16b/predict_predictions.pt\n\n# 3. extract the codebook of the structure tokenizer\n# the output is logs/protein2structoken_16b/codebook.pt\npython experiments/AIDO.StructureTokenizer/extract_structure_tokenizer_codebook.py --output_path logs/protein2structoken_16b/codebook.pt\n\n# 4. run the decode model to convert the structure tokens into pdb files\n# currently this script doesn't support multi-gpu, so only use one gpu\n# the command line overrides the name, input structure tokens, and codebook path\n# the output is logs/protein2structoken_16b/predict_predictions.pdb\nCUDA_VISIBLE_DEVICES=0 mgen predict --config experiments/AIDO.StructureTokenizer/decode.yaml \\\n --data.init_args.config.struct_tokens_datasets_configs.name=protein2structoken_16b \\\n --data.init_args.config.struct_tokens_datasets_configs.struct_tokens_path=logs/protein2structoken_16b/predict_predictions.pt \\\n --data.init_args.config.struct_tokens_datasets_configs.codebook_path=logs/protein2structoken_16b/codebook.pt\n</code></pre> <p>Here is an example visualization of <code>logs/protstruct_decode/protein2structoken_16b_pdb_files/cameo:0__output.pdb</code>. </p> <p>Notes: - While the language model part (amino acid sequence to structure tokens) is relatively fast and support multi-GPU, the decoding part (structure tokens to protein structures) is slower and currently only supports single GPU inference due to the file saving mechanism. We plan to support multi-GPU inference in the future. - For CASP14, CASP15, and CAMEO datasets, you could find the corresponding ground truth structures in <code>genbio-ai/casp14-casp15-cameo-test-proteins</code>. You could use <code>huggingface-cli download genbio-ai/casp14-casp15-cameo-test-proteins --repo-type dataset --local-dir ./data/casp14-casp15-cameo-test-proteins/</code> to download the dataset. - The TSV format doesn't support specifying the residue index in TSV format. Specifying the residue index will make the prediction more accurate.</p>"},{"location":"tutorials/zeroshot/","title":"Zeroshot Variant Effect Prediction","text":"<p>Zeroshot variant effect prediction refers to the task of predicting the functional impact of genetic variants, especially single nucleotide polymorphisms (SNPs), without requiring additional task-specific fine-tuning of the model. AIDO.ModelGenerator implements the procedure proposed by Nucleotide Transformer We use this to predict the effects of single nucleotide polymorphisms (SNPs) in the AIDO.DNA-300M model. This task uses the pre-trained models directly, and does not require finetuning.</p> <ol> <li><code>ClinvarRetrieve</code> Module automatically download human reference genome, processed clinvar data and demo clinvar data to <code>GENBIO_DATA_DIR/genbio_finetune/dna_datasets</code> from hugginface https://huggingface.co/datasets/genbio-ai/Clinvar.</li> </ol> <p>Note: check if you have already set <code>GENBIO_DATA_DIR</code> as an environment variable on terminal</p> <pre><code>echo $GENBIO_DATA_DIR\n</code></pre> <p>If not, set your own data path</p> <pre><code>echo 'export GENBIO_DATA_DIR=/your/full/path/' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> <p>Otherwise, <code>ClinvarRetrieve</code> will automatically set the <code>GENBIO_DATA_DIR</code> environment variable to <code>&lt;root_path&gt;/ModelGenerator/genbio_data</code>.</p> <ol> <li>You can also choose to download raw data and preprocess clinvar data by yourself:</li> </ol> <pre><code># download hg38 reference genome\nwget -P $GENBIO_DATA_DIR/genbio_finetune/dna_datasets https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz\ngunzip $GENBIO_DATA_DIR/genbio_finetune/dna_datasets/hg38.fa.gz\n\n# download raw clinvar dataset\nwget -P $GENBIO_DATA_DIR/genbio_finetune/dna_datasets https://hgdownload.soe.ucsc.edu/gbdb/hg38/bbi/clinvar/clinvarMain.bb\n\n# download package to convert .bb file to .bed file\nwget http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/bigBedToBed\nchmod +x bigBedToBed\n./bigBedToBed $GENBIO_DATA_DIR/genbio_finetune/dna_datasets/clinvarMain.bb $GENBIO_DATA_DIR/genbio_finetune/dna_datasets/clinvarMain.bed\n\n# Preprocess the raw ClinVar dataset to retain only the following fields: 'chrom', 'start', 'end', 'name', '_clinSignCode', 'ref', 'mutate', and 'effect'.\n# The processed file is saved as Clinvar_Processed.tsv.\ncd ModelGenerator\npython experiments/AIDO.DNA/zeroshot_variant_effect_prediction/preprocess_clinvar.py $GENBIO_DATA_DIR/genbio_finetune/dna_datasets/clinvarMain.bed\n</code></pre> <ol> <li>Run <code>mgen test --config config.yaml</code>. If you want to take the norm distance between reference and variant sequence embeddings as prediction, the config should be</li> </ol> <pre><code>model:\n  class_path: modelgenerator.tasks.ZeroshotPredictionDistance\n  init_args:\n    backbone: &lt;you-choose&gt;\ndata:\n  class_path: ClinvarRetrieve\n  init_args:\n    method: Distance\n    window: &lt;window size centered around the SNPs&gt;\n    test_split_files:\n      - &lt;my_sequences.tsv&gt;\n    reference_file: &lt;human_reference_genome.ml.fa&gt; # Example: hg38.ml.fa\ntrainer:\n  callbacks:\n  - class_path: modelgenerator.callbacks.PredictionWriter\n    dict_kwargs:\n      output_dir: predictions\n      filetype: tsv\n      write_cols: ['score','norm_type','labels','num_layer']\n</code></pre> <p>If you take the loglikelihood ratio between reference and variant sequence embeddings at the mutation position as prediction, then the config should be</p> <pre><code>model:\n  class_path: modelgenerator.tasks.ZeroshotPredictionDiff\n  init_args:\n    backbone: &lt;you-choose&gt;\ndata:\n  class_path: ClinvarRetrieve\n  init_args:\n    method: Diff\n    window: &lt;window size centered around the SNPs&gt;\n    test_split_files:\n      - &lt;my_sequences.tsv&gt;\n    reference_file: &lt;human_reference_genome.ml.fa&gt; # Example: hg38.ml.fa\ntrainer:\n  callbacks:\n  - class_path: modelgenerator.callbacks.PredictionWriter\n    dict_kwargs:\n      output_dir: predictions\n      filetype: tsv\n      write_cols: ['score','label']\n</code></pre> <p>The labels and scores are also saved in <code>test_predictions.tsv</code> under the dir specified by <code>--trainer.callbacks.output_dir</code>.</p> <p>Here are two examples of how to load HF model for inference For norm distance mode</p> <pre><code>mgen test --config experiments/AIDO.DNA/zeroshot_variant_effect_prediction/Clinvar_300M_zeroshot_Distance.yaml\n</code></pre> <p>For loglikelihood ratio mode</p> <pre><code>mgen test --config experiments/AIDO.DNA/zeroshot_variant_effect_prediction/Clinvar_300M_zeroshot_Diff.yaml\n</code></pre>"},{"location":"usage/","title":"Basic Usage","text":"<p>AIDO.ModelGenerator orchestrates experiments with LightningCLI to make runs modular, composable, and reproducible.</p> <p>Training, validation, testing, and prediction are separated into independent CLI calls to <code>mgen fit/validate/test/predict</code>.  For researchers developing new backbones, heads, or tasks, a typical workflow might be </p> <pre><code># My model development workflow\n1. mgen fit --config config.yaml\n2. mgen validate --config config.yaml --ckpt_path path/to/model.ckpt\n# Make any config or code changes, repeat 1,2 until satisfied\n3. mgen test --config config.yaml --ckpt_path path/to/model.ckpt\n# Run inference for plotting, analysis, or deployment\n4. mgen predict --config config.yaml --ckpt_path path/to/model.ckpt\n</code></pre> <p>For researchers or practicioners using pre-trained models, a typical workflow might be</p> <pre><code># My inference and evaluation workflow\n1. git clone https://huggingface.co/genbio-ai/&lt;finetuned_model&gt;\n  # finetuned_model/\n  # \u251c\u2500\u2500 config.yaml\n  # \u251c\u2500\u2500 model.ckpt\n2. Compile your data to predict or test on\n3. Copy and override the `config.yaml` with your own data paths and splits\n4. mgen test --config my_config.yaml --ckpt_path finetuned_model/model.ckpt\n4. mgen predict --config my_config.yaml --ckpt_path finetuned_model/model.ckpt \\\n    --config configs/examples/save_predictions.yaml\n</code></pre> <p>For more details on designing and running experiments, see the Experiment Design pocket guide.</p>"},{"location":"usage/#options","title":"Options","text":"<p>To explore what options are configurable, <code>mgen</code> provides a <code>--help</code> flag at all levels.</p> <pre><code>mgen --help\nmgen &lt;fit/validate/test/predict&gt; --help\nmgen fit --model.help &lt;Task&gt;\n# e.g. mgen fit --model.help ConditionalDiffusion\nmgen fit --data.help &lt;Dataset&gt;\n# e.g. mgen fit --data.help PromoterExpressionRegression\nmgen fit --model.help &lt;Task&gt; --model.&lt;arg&gt;.help &lt;arg_object&gt;\n# e.g. mgen fit --model.help ConditionalDiffusion --model.backbone.help aido_dna_dummy\n</code></pre>"},{"location":"usage/#using-configs","title":"Using Configs","text":""},{"location":"usage/#config-cli-duality","title":"Config-CLI duality","text":"<p>For reproducibility and fine-grained control, all CLI calls can be organized into a <code>config.yaml</code> file. The command</p> <pre><code>mgen fit --model ConditionalDiffusion --model.backbone aido_dna_dummy \\\n  --data ConditionalDiffusion --data.path \"genbio-ai/100m-random-promoters\"\n</code></pre> <p>is equivalent to <code>mgen fit --config my_config.yaml</code> with</p> <pre><code># my_config.yaml\nmodel:\n  class_path: ConditionalDiffusion\n  backbone: aido_dna_dummy\ndata:\n  class_path: ConditionalDiffusion\n  init_args:\n    path: \"genbio-ai/100m-random-promoters\"\n</code></pre>"},{"location":"usage/#composability","title":"Composability","text":"<p>Configs are also composable and allow multiple <code>--config</code> flags to be passed. Runs always use only the LAST value for each argument. Combining these two configs </p> <pre><code># my_config.yaml\nmodel:\n  class_path: ConditionalDiffusion\n  backbone: aido_dna_dummy\n  adapter:\n    class_path: modelgenerator.adapters.ConditionalLMAdapter\n    init_args:\n        n_head: 6\n        dim_feedforward: int = 1024,\n        num_layers: int = 2,\n\n# my_new_config.yaml\nmodel:\n  class_path: ConditionalDiffusion\n  backbone: aido_dna_7b\n  adapter:\n    class_path: modelgenerator.adapters.ConditionalLMAdapter\n    init_args:\n        num_layers: int = 8,\n</code></pre> <p>as <code>mgen fit --config my_config.yaml --config my_new_config.yaml</code> results in</p> <pre><code>model:\n  class_path: ConditionalDiffusion\n  backbone: aido_dna_7b\n  adapter:\n    class_path: modelgenerator.adapters.ConditionalLMAdapter\n    init_args:\n        n_head: 6\n        dim_feedforward: int = 1024,\n        num_layers: int = 8,\n</code></pre> <p>We provide some useful tools in <code>configs/examples</code> for logging, development, LoRA finetuning, and prediction writing that can be composed with your own configs.</p>"},{"location":"usage/#reproducibility","title":"Reproducibility","text":"<p>The full configuration including all defaults and user-specified arguments will always be saved for each run. This file changes location depending on logger, but will be in <code>logs/lightning_logs/your-experiment/config.yaml</code> by default, or if using wandb <code>logs/config.yaml</code>. Even if AIDO.ModelGenerator defaults change, simply using <code>mgen fit --config your/logged/config.yaml</code> will always reproduce the experiment.</p>"},{"location":"usage/#example","title":"Example","text":"<p>Below is a full example <code>config.yaml</code> saved from the simple command </p> <pre><code>mgen fit --model SequenceRegression --data PromoterExpressionRegression\n</code></pre> <p>See the pre-packaged experiments in the <code>experiments</code> directory for more examples.</p> <pre><code># lightning.pytorch==2.4.0\nseed_everything: 0\nmodel:\n  class_path: modelgenerator.tasks.SequenceRegression\n  init_args:\n    backbone:\n      class_path: modelgenerator.backbones.aido_dna_dummy\n      init_args:\n        from_scratch: false\n        max_length: null\n        use_peft: false\n        frozen: false\n        save_peft_only: true\n        lora_r: 16\n        lora_alpha: 32\n        lora_dropout: 0.1\n        lora_target_modules:\n        - query\n        - value\n        config_overwrites: null\n        model_init_args: null\n    adapter:\n      class_path: modelgenerator.adapters.LinearCLSAdapter\n    num_outputs: 1\n    optimizer:\n      class_path: torch.optim.AdamW\n      init_args:\n        lr: 0.001\n        betas:\n        - 0.9\n        - 0.999\n        eps: 1.0e-08\n        weight_decay: 0.01\n        amsgrad: false\n        maximize: false\n        foreach: null\n        capturable: false\n        differentiable: false\n        fused: null\n    lr_scheduler: null\n    use_legacy_adapter: false\n    strict_loading: true\n    reset_optimizer_states: false\ndata:\n  class_path: modelgenerator.data.PromoterExpressionRegression\n  init_args:\n    path: genbio-ai/100m-random-promoters\n    normalize: true\n    config_name: null\n    train_split_name: train\n    test_split_name: test\n    valid_split_name: null\n    train_split_files: null\n    test_split_files: null\n    valid_split_files: null\n    test_split_size: 0.2\n    batch_size: 128\n    shuffle: true\n    sampler: null\n    num_workers: 0\n    pin_memory: true\n    persistent_workers: false\n    cv_num_folds: 1\n    cv_test_fold_id: 0\n    cv_enable_val_fold: true\n    cv_fold_id_col: null\n    cv_val_offset: 1\ntrainer:\n  accelerator: auto\n  strategy:\n    class_path: lightning.pytorch.strategies.DDPStrategy\n    init_args:\n      accelerator: null\n      parallel_devices: null\n      cluster_environment: null\n      checkpoint_io: null\n      precision_plugin: null\n      ddp_comm_state: null\n      ddp_comm_hook: null\n      ddp_comm_wrapper: null\n      model_averaging_period: null\n      process_group_backend: null\n      timeout: 0:30:00\n      start_method: popen\n      output_device: null\n      dim: 0\n      broadcast_buffers: true\n      process_group: null\n      bucket_cap_mb: null\n      find_unused_parameters: false\n      check_reduction: false\n      gradient_as_bucket_view: false\n      static_graph: false\n      delay_all_reduce_named_params: null\n      param_to_hook_all_reduce: null\n      mixed_precision: null\n      device_mesh: null\n  devices: auto\n  num_nodes: 1\n  precision: 32\n  logger: null\n  callbacks:\n  - class_path: lightning.pytorch.callbacks.LearningRateMonitor\n    init_args:\n      logging_interval: step\n      log_momentum: false\n      log_weight_decay: false\n  - class_path: lightning.pytorch.callbacks.ModelCheckpoint\n    init_args:\n      dirpath: null\n      filename: best_val:{step}-{val_loss:.3f}-{train_loss:.3f}\n      monitor: val_loss\n      verbose: false\n      save_last: null\n      save_top_k: 1\n      save_weights_only: false\n      mode: min\n      auto_insert_metric_name: true\n      every_n_train_steps: null\n      train_time_interval: null\n      every_n_epochs: null\n      save_on_train_epoch_end: null\n      enable_version_counter: true\n  - class_path: lightning.pytorch.callbacks.ModelCheckpoint\n    init_args:\n      dirpath: null\n      filename: best_train:{step}-{val_loss:.3f}-train:{train_loss:.3f}\n      monitor: train_loss\n      verbose: false\n      save_last: null\n      save_top_k: 1\n      save_weights_only: false\n      mode: min\n      auto_insert_metric_name: true\n      every_n_train_steps: null\n      train_time_interval: null\n      every_n_epochs: null\n      save_on_train_epoch_end: null\n      enable_version_counter: true\n  - class_path: lightning.pytorch.callbacks.ModelCheckpoint\n    init_args:\n      dirpath: null\n      filename: last:{step}-{val_loss:.3f}-{train_loss:.3f}\n      monitor: null\n      verbose: false\n      save_last: null\n      save_top_k: 1\n      save_weights_only: false\n      mode: min\n      auto_insert_metric_name: true\n      every_n_train_steps: null\n      train_time_interval: null\n      every_n_epochs: null\n      save_on_train_epoch_end: null\n      enable_version_counter: true\n  - class_path: lightning.pytorch.callbacks.ModelCheckpoint\n    init_args:\n      dirpath: null\n      filename: step:{step}-{val_loss:.3f}-{train_loss:.3f}\n      monitor: null\n      verbose: false\n      save_last: null\n      save_top_k: -1\n      save_weights_only: false\n      mode: min\n      auto_insert_metric_name: true\n      every_n_train_steps: 1000\n      train_time_interval: null\n      every_n_epochs: null\n      save_on_train_epoch_end: null\n      enable_version_counter: true\n  fast_dev_run: false\n  max_epochs: -1\n  min_epochs: null\n  max_steps: -1\n  min_steps: null\n  max_time: null\n  limit_train_batches: null\n  limit_val_batches: null\n  limit_test_batches: null\n  limit_predict_batches: null\n  overfit_batches: 0.0\n  val_check_interval: null\n  check_val_every_n_epoch: 1\n  num_sanity_val_steps: null\n  log_every_n_steps: 50\n  enable_checkpointing: null\n  enable_progress_bar: null\n  enable_model_summary: null\n  accumulate_grad_batches: 1\n  gradient_clip_val: 1\n  gradient_clip_algorithm: null\n  deterministic: null\n  benchmark: null\n  inference_mode: true\n  use_distributed_sampler: true\n  profiler:\n    class_path: lightning.pytorch.profilers.PyTorchProfiler\n    init_args:\n      dirpath: null\n      filename: null\n      group_by_input_shapes: false\n      emit_nvtx: false\n      export_to_chrome: true\n      row_limit: 20\n      sort_by_key: null\n      record_module_names: true\n      table_kwargs: null\n      record_shapes: false\n    dict_kwargs:\n      profile_memory: true\n  detect_anomaly: false\n  barebones: false\n  plugins: null\n  sync_batchnorm: false\n  reload_dataloaders_every_n_epochs: 0\n  default_root_dir: logs\nckpt_path: null\n</code></pre>"},{"location":"usage/exporting_models/","title":"Exporting Models","text":"<p>While AIDO.ModelGenerator is CLI-driven, models created with AIDO.ModelGenerator can also be loaded in Python scripts and exported to HuggingFace.</p>"},{"location":"usage/exporting_models/#exporting-and-loading-with-cli","title":"Exporting and Loading with CLI","text":"<p>As an example of a finetuned model export, see some of the many checkpoints available on Huggingface for immediate inference. The only requirements are the <code>config.yaml</code> and <code>&lt;model&gt;.ckpt</code> files to be run again from AIDO.ModelGenerator.</p> <pre><code># Download from HF\ngit clone https://huggingface.co/genbio-ai/dummy-ckpt\n\n# Evaluate\nmgen test --config dummy-ckpt/config.yaml \\\n  --ckpt_path dummy-ckpt/best_val:step=742-val_loss=0.404-train_loss=0.464.ckpt \n\n# Predict\nmgen predict --config dummy-ckpt/config.yaml \\\n  --ckpt_path dummy-ckpt/best_val:step=742-val_loss=0.404-train_loss=0.464.ckpt \\\n  --config configs/examples/save_predictions.yaml\n</code></pre>"},{"location":"usage/exporting_models/#exporting-and-loading-in-python","title":"Exporting and Loading in Python","text":"<p>Model checkpoints can also be used in Python scripts, notebooks, or other codebases with PyTorch Lightning.</p> <pre><code># my_notebook.ipynb\n\n########################################################################################\n# Download the data\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\n\nmy_models_path = Path.home().joinpath('my_models', 'dummy-ckpt')\nmy_models_path.mkdir(parents=True, exist_ok=True)\nsnapshot_download(repo_id=\"genbio-ai/dummy-ckpt\", local_dir=my_models_path)\n\n########################################################################################\n# Run the model\nimport torch\n\n# Check the config.yaml for the model class\nfrom modelgenerator.tasks import SequenceClassification\n\nckpt_path = my_models_path.joinpath('best_val:step=742-val_loss=0.404-train_loss=0.464.ckpt')\nmodel = SequenceClassification.load_from_checkpoint(ckpt_path)\n\ncollated_batch = model.transform({\"sequences\": [\"ACGT\", \"ACGT\"]})\nlogits = model(collated_batch)\nprint(logits)\nprint(torch.argmax(logits, dim=-1))\n########################################################################################\n</code></pre>"},{"location":"usage/exporting_models/#exporting-and-loading-in-hugging-face","title":"Exporting and Loading in Hugging Face","text":"<p>Checkpoints can also be converted to HF format.</p> <pre><code># Download from HF\ngit clone https://huggingface.co/genbio-ai/dummy-ckpt\n\n# Convert to HF\nmgen-hf convert --config conversion_config.yaml\n\n# conversion_config.yaml:\ntask_class: modelgenerator.tasks.SequenceClassification\nckpt_path: dummy-ckpt/best_val:step=742-val_loss=0.404-train_loss=0.464.ckpt\ndest_dir: dummy-ckpt-hf\npush_to_hub: false\nrepo_id: genbio-ai/dummy-ckpt-hf\n</code></pre> <p>Then load the model in Hugging Face format</p> <pre><code>import torch\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"./dummy-ckpt-hf\")\n# Or if pushed to hub\n# model = AutoModel.from_pretrained(\"genbio-ai/dummy-ckpt-hf\", trust_remote_code=True)\n\ncollated_batch = model.genbio_model.transform({\"sequences\": [\"ACGT\", \"ACGT\"]})\nlogits = model(collated_batch)\nprint(logits)\nprint(torch.argmax(logits, dim=-1))\n</code></pre>"},{"location":"usage/reproducing_experiments/","title":"Reproducing Experiments","text":"<p>Experiments run through AIDO.ModelGenerator generate <code>config.yaml</code> and <code>&lt;model&gt;.ckpt</code> artifacts which can be used to reproduce training and evaluation, or extend experiments and improve models.</p> <p>Many pre-packaged experiments are available in the experiments directory with instructions to run inference, evaluate, and reproduce training runs.</p>"},{"location":"usage/reproducing_experiments/#experiment-configs","title":"Experiment Configs","text":"<p>When you start a run, AIDO.ModelGenerator saves a full <code>config.yaml</code> file for every experiment run which includes all user-specified and default args. To reproduce a training run </p> <pre><code>mgen fit --config config.yaml\n</code></pre> <p>The location of the saved <code>config.yaml</code> varies by the logger used, but by default this will be under <code>logs/lightning_logs/version_X/config.yaml</code>.</p> <p>Checkpoints can be used immediately to reproduce test metrics or run inference on new data.</p>"},{"location":"usage/reproducing_experiments/#experiment-checkpoints","title":"Experiment Checkpoints","text":"<p>If you use checkpointing callbacks (enabled by default), AIDO.ModelGenerator will save <code>&lt;model&gt;.ckpt</code> files within your logging directory. These can be used with the saved <code>config.yaml</code> to run evaluation or prediction.</p> <pre><code># Reproduce test metrics\nmgen test --config config.yaml \\\n  --ckpt_path logs/lightning_logs/version_X/checkpoints/&lt;my_best_val&gt;.ckpt\n\n# Reproduce predictions\nmgen predict --config config.yaml \\\n  --ckpt_path logs/lightning_logs/version_X/checkpoints/&lt;my_best_val&gt;.ckpt \\\n  --config configs/examples/save_predictions.yaml\n</code></pre>"},{"location":"usage/reproducing_experiments/#pre-packaged-experiments","title":"Pre-packaged Experiments","text":"<p>Experiment configs and scripts from studies using AIDO.ModelGenerator are available in the experiments directory with instructions to run inference, evaluate, and reproduce training runs.</p> <p>Below are some general guidelines for reproducing experiments, or adding new experiments to the repository. For more details on each study, see the Studies page.</p>"},{"location":"usage/reproducing_experiments/#finetuning-experiments","title":"Finetuning Experiments","text":"<p>To reproduce a training run with a saved config, simply run</p> <pre><code>mgen fit --config config.yaml\n</code></pre> <p>Note: If you are using a full frozen config, some machine-specific parameters like <code>trainer.num_nodes</code> may need to be adjusted.</p> <p>To evaluate the results, take the <code>best_val</code> checkpoint and run</p> <pre><code>mgen test --config config.yaml --ckpt_path logs/lightning_logs/version_X/checkpoints/&lt;my_best_val&gt;.ckpt\n</code></pre> <p>In some cases, experiments will be more complicated than a single config (e.g. for cross-validation) In these cases, look for a README, a bash script, or contact the study authors.</p>"},{"location":"usage/reproducing_experiments/#zero-shot-experiments","title":"Zero-shot Experiments","text":"<p>If an experiment is zero-shot and only requires inference, you can test the model without any training</p> <pre><code>mgen test --config config.yaml\n</code></pre>"},{"location":"usage/reproducing_experiments/#inference-experiments","title":"Inference Experiments","text":"<p>If an experiment uses predictions for a downstream task, you can run inference on new data</p> <pre><code>mgen predict --config config.yaml --config configs/examples/save_predictions.yaml\n</code></pre> <p>and use any associated post-processing scripts with the experiment to compile and plot the results.</p>"},{"location":"usage/saving_outputs/","title":"Saving Outputs","text":"<p>AIDO.ModelGenerator provides a unified and hardware-adaptive interface for inference, embedding, and prediction with pre-trained models.</p> <p>This page covers how to use AIDO.ModelGenerator to get embeddings and predictions from pre-trained backbones as well as finetuned models, and how to save and manage outputs for downstream analysis.</p>"},{"location":"usage/saving_outputs/#pre-trained-backbones","title":"Pre-trained Backbones","text":"<p>Backbones in AIDO.ModelGenerator are pre-trained foundation models.</p> <p>A full list of available backbones is in the Backbone API reference. For each data modality, we suggest using</p> <ul> <li><code>aido_dna_7b</code> for DNA sequences</li> <li><code>aido_protein_16b</code> for protein sequences</li> <li><code>aido_rna_1b600m</code> for RNA sequences</li> <li><code>aido_cell_650m</code> for gene expression</li> <li><code>aido_protein2structoken_16b</code> for translating protein sequence to structure tokens</li> <li><code>aido_dna_dummy</code> and <code>aido_protein_dummy</code> for debugging</li> <li><code>dna_onehot</code> and <code>protein_onehot</code> for non-FM baselines</li> </ul>"},{"location":"usage/saving_outputs/#backbone-embedding-and-inference","title":"Backbone Embedding and Inference","text":"<p>To get embeddings, use <code>mgen predict</code> with the <code>Embed</code> task.</p> <p>Note: Predictions will always be taken from the test set. To get predictions from another dataset, set it as the test set using the <code>--data.test_split_files</code></p> <p>Note: Distributed inference with DDP is enabled by default. Predictions need post-processing to be compiled into a single output. See below for details on distributed inference.</p> <p>For example, to get embeddings from the <code>dummy</code> model on a small number of sequences in the <code>genbio-ai/100m-random-promoters</code> dataset and save to a <code>predictions</code> directory, use the following command:</p> <pre><code># mgen predict --config config.yaml\n# config.yaml:\nmodel:\n  class_path: Embed\n  init_args:\n    backbone: aido_dna_dummy\ndata:\n  class_path: SequencesDataModule\n  init_args:\n    path: genbio-ai/100m-random-promoters\n    x_col: sequence\n    id_col: sequence  # No real ID in this dataset, so just use input sequence\n    test_split_size: 0.0001\ntrainer:\n  callbacks:\n  - class_path: modelgenerator.callbacks.PredictionWriter\n    dict_kwargs:\n      output_dir: predictions\n      filetype: pt\n</code></pre> <p>To get token probabilities, use <code>mgen predict</code> with the <code>Inference</code> task.</p> <pre><code># mgen predict --config config.yaml\n# config.yaml:\nmodel:\n  class_path: Inference\n  init_args:\n    backbone: aido_dna_dummy\ndata:\n  class_path: SequencesDataModule\n  init_args:\n    path: genbio-ai/100m-random-promoters\n    x_col: sequence\n    id_col: sequence  # No real ID in this dataset, so just use input sequence\n    test_split_size: 0.0001\ntrainer:\n  callbacks:\n  - class_path: modelgenerator.callbacks.PredictionWriter\n    dict_kwargs:\n      output_dir: predictions\n      filetype: pt\n</code></pre>"},{"location":"usage/saving_outputs/#finetuned-models","title":"Finetuned Models","text":"<p>Finetuned model weights and configs from studies using AIDO.ModelGenerator are available for download on Hugging Face.</p> <p>To get predictions from a finetuned model, use <code>mgen predict</code> with the model's config file and checkpoint.</p> <pre><code># Download the model and config from Hugging Face\n# or use a local config.yaml and model.ckpt\nmgen predict --config config.yaml --ckpt_path model.ckpt \\\n    --config configs/examples/save_predictions.yaml\n</code></pre> <p>Predicting, testing, or training on new data is also straightforward, and in most cases only requires matching the format of the original dataset and overriding filepaths. See Data Experiment Design for more details.</p>"},{"location":"usage/saving_outputs/#distributed-inference","title":"Distributed Inference","text":"<p>Models and datasets are often too large to fit in memory on a single device.</p> <p>AIDO.ModelGenerator supports distributed training and inference on multiple devices by sharding models and data with FSDP. For example, to split <code>aido_protein_16b</code> across multiple nodes and multiple GPUs, add the following to your config:</p> <pre><code>trainer:\n  num_nodes: X  # 1 by default, but not automatic. Must be set correctly for multi-node.\n  devices: auto\n  strategy:\n    class_path: lightning.pytorch.strategies.FSDPStrategy\n    init_args:\n      sharding_strategy: FULL_SHARD\n      auto_wrap_policy: modelgenerator.distributed.fsdp.wrap.AutoWrapPolicy\n</code></pre> <p>The <code>auto_wrap_policy</code> is necessary to shard the model in FSDP.</p> <p>By default, PredictionWriter will save separate files for each batch and each device. For customization options, see the Callbacks API reference.</p> <p>We recommend using batch-level writing in most cases to avoid out-of-memory issues, and compiling and filtering predictions using a simple post-processing script.</p> <pre><code>import torch\nimport os\n\n# Load all predictions\npredictions = []\nfor file in os.listdir(\"predictions\"):\n    if file.endswith(\".pt\"):\n        prediction_device_batch = torch.load(os.path.join(\"predictions\", file))\n        prediction_clean = # Do any key filtering or transformations necessary here\n        predictions.append(prediction_clean)\n\n# Combine, convert, make a DataFrame, etc, and save for the next pipeline step.\n</code></pre>"}]}